{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX6YJ8nWc4qW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yutWsVAgdCUB",
        "outputId": "7b9cc39b-02c5-4684-ee3c-49fb4dd6dfe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import List, Dict, Tuple\n",
        "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import wordnet\n",
        "from scipy.spatial.distance import cosine\n",
        "import random\n",
        "from termcolor import colored\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class AdvancedTextHumanizer:\n",
        "    def __init__(self):\n",
        "        # Load models\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "        self.roberta_model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.target_pos = ['ADJ', 'ADV', 'NOUN', 'VERB', 'CONJ', 'PREP', 'PRON', 'INTJ']\n",
        "        self.cache = {}\n",
        "\n",
        "        # Expanded formal/AI vocabulary\n",
        "        self.formal_replacements = {\n",
        "            'utilize': ['use', 'apply'],\n",
        "            'implement': ['use', 'add', 'put in'],\n",
        "            'demonstrate': ['show', 'prove'],\n",
        "            'comprehensive': ['complete', 'full'],\n",
        "            'subsequently': ['then', 'after'],\n",
        "            'methodology': ['method', 'approach'],\n",
        "            'leverage': ['use', 'apply'],\n",
        "            'furthermore': ['also', 'plus'],\n",
        "            'therefore': ['so', 'thus'],\n",
        "            'approximately': ['about', 'around'],\n",
        "            'facilitate': ['help', 'aid'],\n",
        "            'optimal': ['best', 'ideal'],\n",
        "            'initiated': ['started', 'began'],\n",
        "            'sufficient': ['enough', 'adequate'],\n",
        "            'preliminary': ['initial', 'first'],\n",
        "            'endeavor': ['try', 'attempt'],\n",
        "            'ascertain': ['find out', 'check'],\n",
        "            'pursuant': ['following', 'under'],\n",
        "            'expedite': ['speed up', 'hurry'],\n",
        "            'commence': ['start', 'begin']\n",
        "        }\n",
        "\n",
        "        print(colored(\"Initializing AdvancedTextHumanizer...\", \"cyan\"))\n",
        "        print(colored(\"✓ Loading language models\", \"green\"))\n",
        "\n",
        "        # Add common words list (top 1000 most frequent English words)\n",
        "        self.common_words = set()\n",
        "        try:\n",
        "            with open('common_words.txt', 'r') as f:\n",
        "                self.common_words = set(f.read().splitlines())\n",
        "        except:\n",
        "            # Fallback to a small set of very common words\n",
        "            self.common_words = {'good', 'great', 'nice', 'better', 'best', 'easy', 'simple',\n",
        "                               'clear', 'fast', 'quick', 'well', 'sure', 'right', 'fine'}\n",
        "\n",
        "        # Add negative prefixes to check\n",
        "        self.negative_prefixes = {'un', 'in', 'im', 'ir', 'dis', 'non'}\n",
        "\n",
        "    def char_similarity_ratio(self, str1: str, str2: str) -> float:\n",
        "        \"\"\"Calculate character-level similarity between two strings.\"\"\"\n",
        "        if len(str1) == 0 or len(str2) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common_chars = sum(1 for c in str1 if c in str2)\n",
        "        return common_chars / max(len(str1), len(str2))\n",
        "\n",
        "    def has_opposite_meaning(self, word: str, candidate: str) -> bool:\n",
        "        \"\"\"Check if the candidate is just the negative form of the word.\"\"\"\n",
        "        for prefix in self.negative_prefixes:\n",
        "            if (candidate.startswith(prefix) and word in candidate) or \\\n",
        "               (word.startswith(prefix) and candidate in word):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def compute_similarity_score(self, word1: str, word2: str) -> float:\n",
        "        # Combine multiple similarity metrics\n",
        "        scores = []\n",
        "\n",
        "\n",
        "        # Sentence-BERT similarity\n",
        "        sbert_sim = self.sentence_model.encode([word1, word2])\n",
        "        sbert_sim = 1 - cosine(sbert_sim[0], sbert_sim[1])\n",
        "        scores.append(sbert_sim)\n",
        "\n",
        "        # WordNet similarity\n",
        "        wn_sim = self.get_wordnet_similarity(word1, word2)\n",
        "        if wn_sim:\n",
        "            scores.append(wn_sim)\n",
        "\n",
        "        return np.mean(scores)\n",
        "\n",
        "    def get_wordnet_similarity(self, word1: str, word2: str) -> float:\n",
        "        synsets1 = wordnet.synsets(word1)\n",
        "        synsets2 = wordnet.synsets(word2)\n",
        "\n",
        "        if not synsets1 or not synsets2:\n",
        "            return None\n",
        "\n",
        "        max_sim = 0\n",
        "        for s1 in synsets1:\n",
        "            for s2 in synsets2:\n",
        "                sim = s1.path_similarity(s2)\n",
        "                if sim and sim > max_sim:\n",
        "                    max_sim = sim\n",
        "        return max_sim\n",
        "\n",
        "    def get_replacement_candidates(self, sentence: str, word: str, pos: str) -> List[str]:\n",
        "        if word.lower() in self.formal_replacements:\n",
        "            candidates = self.formal_replacements[word.lower()]\n",
        "            # Prefer common words from predefined replacements\n",
        "            common_candidates = [c for c in candidates if c in self.common_words]\n",
        "            return common_candidates if common_candidates else candidates\n",
        "\n",
        "        if (sentence, word) in self.cache:\n",
        "            return self.cache[(sentence, word)]\n",
        "\n",
        "        candidates = []\n",
        "\n",
        "        # RoBERTa predictions\n",
        "        masked_text = sentence.replace(word, self.roberta_tokenizer.mask_token)\n",
        "        inputs = self.roberta_tokenizer(masked_text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.roberta_model(**inputs)\n",
        "            predictions = outputs.logits\n",
        "\n",
        "        mask_idx = torch.where(inputs[\"input_ids\"][0] == self.roberta_tokenizer.mask_token_id)[0]\n",
        "        top_tokens = torch.topk(predictions[0, mask_idx], 20, dim=1)\n",
        "\n",
        "        # Combine candidates\n",
        "        for token_id in top_tokens.indices[0]:\n",
        "            candidate = self.roberta_tokenizer.decode(token_id).strip()\n",
        "            if self.validate_candidate(candidate, word, pos):\n",
        "                candidates.append(candidate)\n",
        "\n",
        "        # Filter and rank candidates\n",
        "        ranked_candidates = []\n",
        "        for candidate in set(candidates):\n",
        "            similarity = self.compute_similarity_score(word, candidate)\n",
        "            if similarity > 0.5:  # Adjusted threshold\n",
        "                ranked_candidates.append((candidate, similarity))\n",
        "\n",
        "        ranked_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Additional filtering for final candidates\n",
        "        final_candidates = []\n",
        "        for candidate, similarity in ranked_candidates:\n",
        "            # Prefer common words\n",
        "            score_boost = 0.1 if candidate.lower() in self.common_words else 0\n",
        "            adjusted_similarity = similarity + score_boost\n",
        "\n",
        "            if adjusted_similarity > 0.5:\n",
        "                final_candidates.append(candidate)\n",
        "\n",
        "        # Limit to top 5 candidates, preferring common words\n",
        "        final_candidates.sort(key=lambda x: x in self.common_words, reverse=True)\n",
        "        final_candidates = final_candidates[:5]\n",
        "\n",
        "        self.cache[(sentence, word)] = final_candidates\n",
        "        return final_candidates\n",
        "\n",
        "    def validate_candidate(self, candidate: str, original: str, pos: str) -> bool:\n",
        "        if not candidate.isalpha() or len(candidate) < 3:\n",
        "            return False\n",
        "\n",
        "        if candidate.lower() == original.lower():\n",
        "            return False\n",
        "\n",
        "        # Check for character similarity to avoid misspellings\n",
        "        char_sim = self.char_similarity_ratio(candidate.lower(), original.lower())\n",
        "        if char_sim > 0.8:  # If words are too similar, likely a misspelling\n",
        "            return False\n",
        "\n",
        "        # Check for negative forms\n",
        "        if self.has_opposite_meaning(original.lower(), candidate.lower()):\n",
        "            return False\n",
        "\n",
        "        doc = self.nlp(candidate)\n",
        "        if doc[0].pos_ != pos:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def humanize_text(self, text: str, replacement_prob: float = 0.7) -> Tuple[str, Dict]:\n",
        "        doc = self.nlp(text)\n",
        "        new_text = []\n",
        "        replacements = {}\n",
        "\n",
        "        print(colored(\"\\nProcessing text...\", \"cyan\"))\n",
        "        for sent in tqdm(list(doc.sents), desc=\"Analyzing sentences\"):\n",
        "            sent_text = sent.text\n",
        "            sent_embedding = self.sentence_model.encode([sent_text])[0]\n",
        "\n",
        "            for token in sent:\n",
        "                local_prob = replacement_prob\n",
        "                if len(token.text) > 8:\n",
        "                    local_prob += 0.2\n",
        "\n",
        "                if (token.pos_ in self.target_pos and\n",
        "                    len(token.text) > 3 and\n",
        "                    random.random() < local_prob):\n",
        "\n",
        "                    candidates = self.get_replacement_candidates(sent_text, token.text, token.pos_)\n",
        "                    if candidates:\n",
        "                        replacement = random.choice(candidates)\n",
        "                        replacements[token.text] = replacement\n",
        "                        new_text.append(replacement)\n",
        "                    else:\n",
        "                        new_text.append(token.text)\n",
        "                else:\n",
        "                    new_text.append(token.text)\n",
        "                new_text.append(token.whitespace_)\n",
        "\n",
        "        result_text = \"\".join(new_text)\n",
        "        print(colored(\"\\nHumanization complete!\", \"green\"))\n",
        "        return result_text, replacements\n",
        "\n",
        "    def format_output(self, original_text: str, humanized_text: str, replacements: Dict) -> str:\n",
        "        output = \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        output += colored(\"TEXT HUMANIZATION RESULTS\", \"cyan\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += \"=\"*80 + \"\\n\\n\"\n",
        "\n",
        "        # Original text section\n",
        "        output += colored(\"ORIGINAL TEXT:\", \"yellow\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += f\"{original_text}\\n\\n\"\n",
        "\n",
        "        # Humanized text section\n",
        "        output += colored(\"HUMANIZED TEXT:\", \"green\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += f\"{humanized_text}\\n\\n\"\n",
        "\n",
        "        # Statistics section\n",
        "        output += colored(\"STATISTICS:\", \"magenta\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += f\"Total words replaced: {len(replacements)}\\n\"\n",
        "        output += f\"Unique replacements: {len(set(replacements.values()))}\\n\\n\"\n",
        "\n",
        "        # Replacements section\n",
        "        output += colored(\"WORD REPLACEMENTS:\", \"blue\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        for original, replacement in replacements.items():\n",
        "            output += f\"• {colored(original, 'red')} → {colored(replacement, 'green')}\\n\"\n",
        "\n",
        "        output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        return output\n",
        "\n",
        "    def __del__(self):\n",
        "        self.cache.clear()\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    humanizer = AdvancedTextHumanizer()\n",
        "\n",
        "    text = \"\"\"\n",
        "\n",
        "As a longstanding Apple enthusiast, I approached the evaluation of the new iPhone 15 with a critical eye, mindful of the considerable anticipation surrounding its launch. The iPhone 15 presents a refined, contemporary design that harmoniously blends familiar elements with innovative touches, introducing new color variants that are aesthetically pleasing and maintaining the robust build quality for which Apple is renowned. The device exudes a premium feel, striking an optimal balance between weight and ergonomics. The display is a particular highlight; the Super Retina XDR technology is markedly brighter and more vibrant, elevating the visual experience across a range of activities, from viewing multimedia content to navigating social media platforms. The integration of 120Hz ProMotion technology ensures an exceptionally smooth and responsive user interface. Equipped with the new A17 Bionic chip, the iPhone 15 delivers unparalleled performance—applications open instantaneously, multitasking is seamless, and even the most resource-intensive games operate without lag. The battery life is noteworthy, comfortably supporting a full day of moderate usage. The camera system on the iPhone 15 is especially impressive, featuring enhanced low-light capabilities and advanced computational photography techniques that yield professional-grade images. In conclusion, the iPhone 15 constitutes a substantial upgrade, offering meaningful advancements in performance, display technology, and photographic capabilities, thereby representing a sound investment for the discerning technology aficionado.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    humanized_text, replacements = humanizer.humanize_text(text)\n",
        "    formatted_output = humanizer.format_output(text, humanized_text, replacements)\n",
        "    print(formatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzvjcHa6dxjy",
        "outputId": "d3b2a24d-05f1-4dce-8a3e-b3c386ce5457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing AdvancedTextHumanizer...\n",
            "✓ Loading language models\n",
            "\n",
            "Processing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing sentences: 100%|██████████| 9/9 [01:16<00:00,  8.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Humanization complete!\n",
            "\n",
            "================================================================================\n",
            "TEXT HUMANIZATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "ORIGINAL TEXT:\n",
            "\n",
            "\n",
            "As a longstanding Apple enthusiast, I approached the evaluation of the new iPhone 15 with a critical eye, mindful of the considerable anticipation surrounding its launch. The iPhone 15 presents a refined, contemporary design that harmoniously blends familiar elements with innovative touches, introducing new color variants that are aesthetically pleasing and maintaining the robust build quality for which Apple is renowned. The device exudes a premium feel, striking an optimal balance between weight and ergonomics. The display is a particular highlight; the Super Retina XDR technology is markedly brighter and more vibrant, elevating the visual experience across a range of activities, from viewing multimedia content to navigating social media platforms. The integration of 120Hz ProMotion technology ensures an exceptionally smooth and responsive user interface. Equipped with the new A17 Bionic chip, the iPhone 15 delivers unparalleled performance—applications open instantaneously, multitasking is seamless, and even the most resource-intensive games operate without lag. The battery life is noteworthy, comfortably supporting a full day of moderate usage. The camera system on the iPhone 15 is especially impressive, featuring enhanced low-light capabilities and advanced computational photography techniques that yield professional-grade images. In conclusion, the iPhone 15 constitutes a substantial upgrade, offering meaningful advancements in performance, display technology, and photographic capabilities, thereby representing a sound investment for the discerning technology aficionado.\n",
            "\n",
            "\n",
            "\n",
            "HUMANIZED TEXT:\n",
            "\n",
            "\n",
            "As a longstanding Apple enthusiast, I approaching the evaluation of the new iPhone 15 with a critical eye, aware of the considerable suspense surrounding its launches. The iPhone 15 presents a refined, contemporary design which seamlessly combines familiar components with modern touches, presenting new color variants which are aesthetically pleasing and maintains the robust build quality for which Apple is notable. The device exudes a premium feel, hitting an optimal equilibrium between weight and ergonomics. The screen is a particular highlight; the Super Retina XDR technology is noticeably brighter and more colourful, elevating the visual experience across a range of activities, from viewing multimedia content to navigating social media platforms. The integration of 120Hz ProMotion technology ensures an extremely slick and responsive user interface. Equipped with the new A17 Bionic chips, the iPhone 15 has unparalleled performance—Apps open instantaneously, multitasking is seamless, and yet the most resource-intensive activities run without lag. The battery life is noteworthy, comfortably enduring a full day of moderate usage. The camera system on the iPhone 15 is particularly remarkable, showcasing enhanced low-light capability and advanced computational photography methods which give pro-grade pictures. In conclusion, the iPhone 15 represents a considerable upgrade, providing meaningful advancements in performance, screen technology, and photographic capability, consequently indicating a good investment for the discerning technology aficionado.\n",
            "\n",
            "\n",
            "\n",
            "STATISTICS:\n",
            "Total words replaced: 40\n",
            "Unique replacements: 40\n",
            "\n",
            "WORD REPLACEMENTS:\n",
            "• approached → approaching\n",
            "• mindful → aware\n",
            "• anticipation → suspense\n",
            "• launch → launches\n",
            "• that → which\n",
            "• harmoniously → seamlessly\n",
            "• blends → combines\n",
            "• elements → components\n",
            "• innovative → modern\n",
            "• introducing → presenting\n",
            "• maintaining → maintains\n",
            "• renowned → notable\n",
            "• striking → hitting\n",
            "• balance → equilibrium\n",
            "• display → screen\n",
            "• markedly → noticeably\n",
            "• vibrant → colourful\n",
            "• exceptionally → extremely\n",
            "• smooth → slick\n",
            "• chip → chips\n",
            "• delivers → has\n",
            "• applications → Apps\n",
            "• even → yet\n",
            "• games → activities\n",
            "• operate → run\n",
            "• supporting → enduring\n",
            "• especially → particularly\n",
            "• impressive → remarkable\n",
            "• featuring → showcasing\n",
            "• capabilities → capability\n",
            "• techniques → methods\n",
            "• yield → give\n",
            "• professional → pro\n",
            "• images → pictures\n",
            "• constitutes → represents\n",
            "• substantial → considerable\n",
            "• offering → providing\n",
            "• thereby → consequently\n",
            "• representing → indicating\n",
            "• sound → good\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "!pip install fasttext\n",
        "!pip install nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import List, Dict, Tuple\n",
        "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from nltk.corpus import wordnet\n",
        "from scipy.spatial.distance import cosine\n",
        "import random\n",
        "from termcolor import colored\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class AdvancedTextHumanizer:\n",
        "    def __init__(self):\n",
        "        # Load models\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "        self.roberta_model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        fasttext.util.download_model('en', if_exists='ignore')\n",
        "        self.ft_model = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "        self.target_pos = ['ADJ', 'ADV']\n",
        "        self.target_pos = ['ADJ', 'ADV', 'NOUN', 'VERB', 'CONJ', 'PREP', 'PRON', 'INTJ']\n",
        "        self.cache = {}\n",
        "\n",
        "        # Expanded formal/AI vocabulary\n",
        "        self.formal_replacements = {\n",
        "            'utilize': ['use', 'apply'],\n",
        "            'implement': ['use', 'add', 'put in'],\n",
        "            'demonstrate': ['show', 'prove'],\n",
        "            'comprehensive': ['complete', 'full'],\n",
        "            'subsequently': ['then', 'after'],\n",
        "            'methodology': ['method', 'approach'],\n",
        "            'leverage': ['use', 'apply'],\n",
        "            'furthermore': ['also', 'plus'],\n",
        "            'therefore': ['so', 'thus'],\n",
        "            'approximately': ['about', 'around'],\n",
        "            'facilitate': ['help', 'aid'],\n",
        "            'optimal': ['best', 'ideal'],\n",
        "            'initiated': ['started', 'began'],\n",
        "            'sufficient': ['enough', 'adequate'],\n",
        "            'preliminary': ['initial', 'first'],\n",
        "            'endeavor': ['try', 'attempt'],\n",
        "            'ascertain': ['find out', 'check'],\n",
        "            'pursuant': ['following', 'under'],\n",
        "            'expedite': ['speed up', 'hurry'],\n",
        "            'commence': ['start', 'begin']\n",
        "        }\n",
        "\n",
        "        print(colored(\"Initializing AdvancedTextHumanizer...\", \"cyan\"))\n",
        "        print(colored(\"✓ Loading language models\", \"green\"))\n",
        "\n",
        "        # Add common words list (top 1000 most frequent English words)\n",
        "        self.common_words = set()\n",
        "        try:\n",
        "            with open('common_words.txt', 'r') as f:\n",
        "                self.common_words = set(f.read().splitlines())\n",
        "        except:\n",
        "            # Fallback to a small set of very common words\n",
        "            self.common_words = {'good', 'great', 'nice', 'better', 'best', 'easy', 'simple',\n",
        "                               'clear', 'fast', 'quick', 'well', 'sure', 'right', 'fine'}\n",
        "\n",
        "        # Add negative prefixes to check\n",
        "        self.negative_prefixes = {'un', 'in', 'im', 'ir', 'dis', 'non'}\n",
        "\n",
        "    def char_similarity_ratio(self, str1: str, str2: str) -> float:\n",
        "        \"\"\"Calculate character-level similarity between two strings.\"\"\"\n",
        "        if len(str1) == 0 or len(str2) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common_chars = sum(1 for c in str1 if c in str2)\n",
        "        return common_chars / max(len(str1), len(str2))\n",
        "\n",
        "    def has_opposite_meaning(self, word: str, candidate: str) -> bool:\n",
        "        \"\"\"Check if the candidate is just the negative form of the word.\"\"\"\n",
        "        for prefix in self.negative_prefixes:\n",
        "            if (candidate.startswith(prefix) and word in candidate) or \\\n",
        "               (word.startswith(prefix) and candidate in word):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def compute_similarity_score(self, word1: str, word2: str) -> float:\n",
        "        # Combine multiple similarity metrics\n",
        "        scores = []\n",
        "\n",
        "        # FastText similarity\n",
        "        vec1 = self.ft_model.get_word_vector(word1)\n",
        "        vec2 = self.ft_model.get_word_vector(word2)\n",
        "        ft_sim = 1 - cosine(vec1, vec2)\n",
        "        scores.append(ft_sim)\n",
        "\n",
        "        # Sentence-BERT similarity\n",
        "        sbert_sim = self.sentence_model.encode([word1, word2])\n",
        "        sbert_sim = 1 - cosine(sbert_sim[0], sbert_sim[1])\n",
        "        scores.append(sbert_sim)\n",
        "\n",
        "        # WordNet similarity\n",
        "        wn_sim = self.get_wordnet_similarity(word1, word2)\n",
        "        if wn_sim:\n",
        "            scores.append(wn_sim)\n",
        "\n",
        "        return np.mean(scores)\n",
        "\n",
        "    def get_wordnet_similarity(self, word1: str, word2: str) -> float:\n",
        "        synsets1 = wordnet.synsets(word1)\n",
        "        synsets2 = wordnet.synsets(word2)\n",
        "\n",
        "        if not synsets1 or not synsets2:\n",
        "            return None\n",
        "\n",
        "        max_sim = 0\n",
        "        for s1 in synsets1:\n",
        "            for s2 in synsets2:\n",
        "                sim = s1.path_similarity(s2)\n",
        "                if sim and sim > max_sim:\n",
        "                    max_sim = sim\n",
        "        return max_sim\n",
        "\n",
        "    def get_replacement_candidates(self, sentence: str, word: str, pos: str) -> List[str]:\n",
        "        if word.lower() in self.formal_replacements:\n",
        "            candidates = self.formal_replacements[word.lower()]\n",
        "            # Prefer common words from predefined replacements\n",
        "            common_candidates = [c for c in candidates if c in self.common_words]\n",
        "            return common_candidates if common_candidates else candidates\n",
        "\n",
        "        if (sentence, word) in self.cache:\n",
        "            return self.cache[(sentence, word)]\n",
        "\n",
        "        candidates = []\n",
        "\n",
        "        # RoBERTa predictions\n",
        "        masked_text = sentence.replace(word, self.roberta_tokenizer.mask_token)\n",
        "        inputs = self.roberta_tokenizer(masked_text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.roberta_model(**inputs)\n",
        "            predictions = outputs.logits\n",
        "\n",
        "        mask_idx = torch.where(inputs[\"input_ids\"][0] == self.roberta_tokenizer.mask_token_id)[0]\n",
        "        top_tokens = torch.topk(predictions[0, mask_idx], 20, dim=1)\n",
        "\n",
        "        # FastText similar words\n",
        "        ft_similar = self.ft_model.get_nearest_neighbors(word, k=10)\n",
        "\n",
        "        # Combine candidates\n",
        "        for token_id in top_tokens.indices[0]:\n",
        "            candidate = self.roberta_tokenizer.decode(token_id).strip()\n",
        "            if self.validate_candidate(candidate, word, pos):\n",
        "                candidates.append(candidate)\n",
        "\n",
        "        for _, ft_word in ft_similar:\n",
        "            if self.validate_candidate(ft_word, word, pos):\n",
        "                candidates.append(ft_word)\n",
        "\n",
        "        # Filter and rank candidates\n",
        "        ranked_candidates = []\n",
        "        for candidate in set(candidates):\n",
        "            similarity = self.compute_similarity_score(word, candidate)\n",
        "            if similarity > 0.5:  # Adjusted threshold\n",
        "                ranked_candidates.append((candidate, similarity))\n",
        "\n",
        "        ranked_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Additional filtering for final candidates\n",
        "        final_candidates = []\n",
        "        for candidate, similarity in ranked_candidates:\n",
        "            # Prefer common words\n",
        "            score_boost = 0.1 if candidate.lower() in self.common_words else 0\n",
        "            adjusted_similarity = similarity + score_boost\n",
        "\n",
        "            if adjusted_similarity > 0.5:\n",
        "                final_candidates.append(candidate)\n",
        "\n",
        "        # Limit to top 5 candidates, preferring common words\n",
        "        final_candidates.sort(key=lambda x: x in self.common_words, reverse=True)\n",
        "        final_candidates = final_candidates[:5]\n",
        "\n",
        "        self.cache[(sentence, word)] = final_candidates\n",
        "        return final_candidates\n",
        "\n",
        "    def validate_candidate(self, candidate: str, original: str, pos: str) -> bool:\n",
        "        if not candidate.isalpha() or len(candidate) < 3:\n",
        "            return False\n",
        "\n",
        "        if candidate.lower() == original.lower():\n",
        "            return False\n",
        "\n",
        "        # Check for character similarity to avoid misspellings\n",
        "        char_sim = self.char_similarity_ratio(candidate.lower(), original.lower())\n",
        "        if char_sim > 0.8:  # If words are too similar, likely a misspelling\n",
        "            return False\n",
        "\n",
        "        # Check for negative forms\n",
        "        if self.has_opposite_meaning(original.lower(), candidate.lower()):\n",
        "            return False\n",
        "\n",
        "        doc = self.nlp(candidate)\n",
        "        if doc[0].pos_ != pos:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def humanize_text(self, text: str, replacement_prob: float = 0.7) -> Tuple[str, Dict]:\n",
        "        doc = self.nlp(text)\n",
        "        new_text = []\n",
        "        replacements = {}\n",
        "\n",
        "        print(colored(\"\\nProcessing text...\", \"cyan\"))\n",
        "        for sent in tqdm(list(doc.sents), desc=\"Analyzing sentences\"):\n",
        "            sent_text = sent.text\n",
        "            sent_embedding = self.sentence_model.encode([sent_text])[0]\n",
        "\n",
        "            for token in sent:\n",
        "                local_prob = replacement_prob\n",
        "                if len(token.text) > 8:\n",
        "                    local_prob += 0.2\n",
        "\n",
        "                if (token.pos_ in self.target_pos and\n",
        "                    len(token.text) > 3 and\n",
        "                    random.random() < local_prob):\n",
        "\n",
        "                    candidates = self.get_replacement_candidates(sent_text, token.text, token.pos_)\n",
        "                    if candidates:\n",
        "                        replacement = random.choice(candidates)\n",
        "                        replacements[token.text] = replacement\n",
        "                        new_text.append(replacement)\n",
        "                    else:\n",
        "                        new_text.append(token.text)\n",
        "                else:\n",
        "                    new_text.append(token.text)\n",
        "                new_text.append(token.whitespace_)\n",
        "\n",
        "        result_text = \"\".join(new_text)\n",
        "        print(colored(\"\\nHumanization complete!\", \"green\"))\n",
        "        return result_text, replacements\n",
        "\n",
        "    def format_output(self, original_text: str, humanized_text: str, replacements: Dict) -> str:\n",
        "        output = \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        output += colored(\"TEXT HUMANIZATION RESULTS\", \"cyan\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += \"=\"*80 + \"\\n\\n\"\n",
        "\n",
        "        # Original text section\n",
        "        output += colored(\"ORIGINAL TEXT:\", \"yellow\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += f\"{original_text}\\n\\n\"\n",
        "\n",
        "        # Humanized text section\n",
        "        output += colored(\"HUMANIZED TEXT:\", \"green\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += f\"{humanized_text}\\n\\n\"\n",
        "\n",
        "        # Statistics section\n",
        "        output += colored(\"STATISTICS:\", \"magenta\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += f\"Total words replaced: {len(replacements)}\\n\"\n",
        "        output += f\"Unique replacements: {len(set(replacements.values()))}\\n\\n\"\n",
        "\n",
        "        # Replacements section\n",
        "        output += colored(\"WORD REPLACEMENTS:\", \"blue\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        for original, replacement in replacements.items():\n",
        "            output += f\"• {colored(original, 'red')} → {colored(replacement, 'green')}\\n\"\n",
        "\n",
        "        output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        return output\n",
        "\n",
        "    def __del__(self):\n",
        "        self.cache.clear()\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    humanizer = AdvancedTextHumanizer()\n",
        "\n",
        "    text = \"\"\"\n",
        "\n",
        "As a longstanding Apple enthusiast, I approached the evaluation of the new iPhone 15 with a critical eye, mindful of the considerable anticipation surrounding its launch. The iPhone 15 presents a refined, contemporary design that harmoniously blends familiar elements with innovative touches, introducing new color variants that are aesthetically pleasing and maintaining the robust build quality for which Apple is renowned. The device exudes a premium feel, striking an optimal balance between weight and ergonomics. The display is a particular highlight; the Super Retina XDR technology is markedly brighter and more vibrant, elevating the visual experience across a range of activities, from viewing multimedia content to navigating social media platforms. The integration of 120Hz ProMotion technology ensures an exceptionally smooth and responsive user interface. Equipped with the new A17 Bionic chip, the iPhone 15 delivers unparalleled performance—applications open instantaneously, multitasking is seamless, and even the most resource-intensive games operate without lag. The battery life is noteworthy, comfortably supporting a full day of moderate usage. The camera system on the iPhone 15 is especially impressive, featuring enhanced low-light capabilities and advanced computational photography techniques that yield professional-grade images. In conclusion, the iPhone 15 constitutes a substantial upgrade, offering meaningful advancements in performance, display technology, and photographic capabilities, thereby representing a sound investment for the discerning technology aficionado.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    humanized_text, replacements = humanizer.humanize_text(text)\n",
        "    formatted_output = humanizer.format_output(text, humanized_text, replacements)\n",
        "    print(formatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSj4f7d3e4We",
        "outputId": "434eb731-9546-413d-e698-a893ee1c69ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing AdvancedTextHumanizer...\n",
            "✓ Loading language models\n",
            "\n",
            "Processing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing sentences: 100%|██████████| 9/9 [05:57<00:00, 39.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Humanization complete!\n",
            "\n",
            "================================================================================\n",
            "TEXT HUMANIZATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "ORIGINAL TEXT:\n",
            "\n",
            "\n",
            "As a longstanding Apple enthusiast, I approached the evaluation of the new iPhone 15 with a critical eye, mindful of the considerable anticipation surrounding its launch. The iPhone 15 presents a refined, contemporary design that harmoniously blends familiar elements with innovative touches, introducing new color variants that are aesthetically pleasing and maintaining the robust build quality for which Apple is renowned. The device exudes a premium feel, striking an optimal balance between weight and ergonomics. The display is a particular highlight; the Super Retina XDR technology is markedly brighter and more vibrant, elevating the visual experience across a range of activities, from viewing multimedia content to navigating social media platforms. The integration of 120Hz ProMotion technology ensures an exceptionally smooth and responsive user interface. Equipped with the new A17 Bionic chip, the iPhone 15 delivers unparalleled performance—applications open instantaneously, multitasking is seamless, and even the most resource-intensive games operate without lag. The battery life is noteworthy, comfortably supporting a full day of moderate usage. The camera system on the iPhone 15 is especially impressive, featuring enhanced low-light capabilities and advanced computational photography techniques that yield professional-grade images. In conclusion, the iPhone 15 constitutes a substantial upgrade, offering meaningful advancements in performance, display technology, and photographic capabilities, thereby representing a sound investment for the discerning technology aficionado.\n",
            "\n",
            "\n",
            "\n",
            "HUMANIZED TEXT:\n",
            "\n",
            "\n",
            "As a longstanding Apple enthusiast, I approached the assessment of the new iPhone 15 with a vital eye, aware of the significant expectations surrounding its launch. The iPhone 15 presenting a refined, modern design what harmoniously blends unfamilar elements with innovative touches, showcasing new color variants what are visually attractive and preserving the robust build quality for which Apple is notable. The device exudes a premium feel, striking an optimal balance between weight and ergonomics. The presentation is a specific highlight; the Super Retina XDR technology is substantially bright and more colourful, elevated the visual experience across a range of activity, from viewing multimedia content to navigate social media platforms. The integration of 120Hz ProMotion technology ensuring an outstandingly slick and responsive user interface. Equipped with the new A17 Bionic chips, the iPhone 15 has unmatched performance—apps open instantly, multitasking is seamless, and even the more resource-intense game operate without lag. The batteries life is remarkable, comfortably holding a full day of moderate usage. The camera system on the iPhone 15 is Particularly incredible, featuring enhanced low-illumination capability and advanced computational photography methods that produce pro-class photos. In conclusion, the iPhone 15 represented a solid upgrade, providing impactful breakthroughs in performance, screen technology, and videographic capability, thereby represent a good investor for the discriminating technology aficionado.\n",
            "\n",
            "\n",
            "\n",
            "STATISTICS:\n",
            "Total words replaced: 55\n",
            "Unique replacements: 55\n",
            "\n",
            "WORD REPLACEMENTS:\n",
            "• evaluation → assessment\n",
            "• critical → vital\n",
            "• mindful → aware\n",
            "• considerable → significant\n",
            "• anticipation → expectations\n",
            "• presents → presenting\n",
            "• contemporary → modern\n",
            "• that → what\n",
            "• familiar → unfamilar\n",
            "• introducing → showcasing\n",
            "• aesthetically → visually\n",
            "• pleasing → attractive\n",
            "• maintaining → preserving\n",
            "• renowned → notable\n",
            "• display → screen\n",
            "• particular → specific\n",
            "• markedly → substantially\n",
            "• brighter → bright\n",
            "• vibrant → colourful\n",
            "• elevating → elevated\n",
            "• activities → activity\n",
            "• navigating → navigate\n",
            "• ensures → ensuring\n",
            "• exceptionally → outstandingly\n",
            "• smooth → slick\n",
            "• chip → chips\n",
            "• delivers → has\n",
            "• unparalleled → unmatched\n",
            "• applications → apps\n",
            "• instantaneously → instantly\n",
            "• most → more\n",
            "• intensive → intense\n",
            "• games → game\n",
            "• battery → batteries\n",
            "• noteworthy → remarkable\n",
            "• supporting → holding\n",
            "• especially → Particularly\n",
            "• impressive → incredible\n",
            "• light → illumination\n",
            "• capabilities → capability\n",
            "• techniques → methods\n",
            "• yield → produce\n",
            "• professional → pro\n",
            "• grade → class\n",
            "• images → photos\n",
            "• constitutes → represented\n",
            "• substantial → solid\n",
            "• offering → providing\n",
            "• meaningful → impactful\n",
            "• advancements → breakthroughs\n",
            "• photographic → videographic\n",
            "• representing → represent\n",
            "• sound → good\n",
            "• investment → investor\n",
            "• discerning → discriminating\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Without Fasttext\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vFYgcOj-Xqzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "project_dir = '/content/drive/MyDrive/humanClassifier'\n",
        "\n",
        "\n",
        "outputs_dir = os.path.join(project_dir, 'outputs')\n",
        "os.makedirs(outputs_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "fTWwvi_jhKLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##without fastext\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import List, Dict, Tuple\n",
        "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import wordnet\n",
        "from scipy.spatial.distance import cosine\n",
        "import random\n",
        "from termcolor import colored\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "\n",
        "class AdvancedTextHumanizer:\n",
        "    def __init__(self):\n",
        "        # Download required NLTK data\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "\n",
        "        # Load models\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "        self.roberta_model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.target_pos = ['ADJ', 'ADV', 'NOUN', 'VERB', 'CONJ', 'PREP', 'PRON', 'INTJ']\n",
        "        self.cache = {}\n",
        "\n",
        "        # Expanded formal/AI vocabulary\n",
        "        self.formal_replacements = {\n",
        "            'utilize': ['use', 'apply'],\n",
        "            'implement': ['use', 'add', 'put in'],\n",
        "            'demonstrate': ['show', 'prove'],\n",
        "            'comprehensive': ['complete', 'full'],\n",
        "            'subsequently': ['then', 'after'],\n",
        "            'methodology': ['method', 'approach'],\n",
        "            'leverage': ['use', 'apply'],\n",
        "            'furthermore': ['also', 'plus'],\n",
        "            'therefore': ['so', 'thus'],\n",
        "            'approximately': ['about', 'around'],\n",
        "            'facilitate': ['help', 'aid'],\n",
        "            'optimal': ['best', 'ideal'],\n",
        "            'initiated': ['started', 'began'],\n",
        "            'sufficient': ['enough', 'adequate'],\n",
        "            'preliminary': ['initial', 'first'],\n",
        "            'endeavor': ['try', 'attempt'],\n",
        "            'ascertain': ['find out', 'check'],\n",
        "            'pursuant': ['following', 'under'],\n",
        "            'expedite': ['speed up', 'hurry'],\n",
        "            'commence': ['start', 'begin']\n",
        "        }\n",
        "\n",
        "        print(colored(\"Initializing AdvancedTextHumanizer...\", \"cyan\"))\n",
        "        print(colored(\"✓ Loading language models\", \"green\"))\n",
        "\n",
        "        # Add common words list (top 1000 most frequent English words)\n",
        "        self.common_words = set()\n",
        "        try:\n",
        "            with open('common_words.txt', 'r') as f:\n",
        "                self.common_words = set(f.read().splitlines())\n",
        "        except:\n",
        "            # Fallback to a small set of very common words\n",
        "            self.common_words = {'good', 'great', 'nice', 'better', 'best', 'easy', 'simple',\n",
        "                               'clear', 'fast', 'quick', 'well', 'sure', 'right', 'fine'}\n",
        "\n",
        "        # Add negative prefixes to check\n",
        "        self.negative_prefixes = {'un', 'in', 'im', 'ir', 'dis', 'non'}\n",
        "\n",
        "        # Add N-gram mappings\n",
        "        self.ngram_replacements = {\n",
        "            # Bigrams\n",
        "            ('in order', 'to'): 'to',\n",
        "            ('due to', 'the'): 'because of',\n",
        "            ('with respect', 'to'): 'about',\n",
        "            ('in addition', 'to'): 'besides',\n",
        "            ('prior to', 'the'): 'before',\n",
        "            ('subsequent to', 'the'): 'after',\n",
        "\n",
        "            # Trigrams\n",
        "            ('as a result', 'of', 'the'): 'because of',\n",
        "            ('in the event', 'that', 'the'): 'if',\n",
        "            ('on the basis', 'of', 'the'): 'based on',\n",
        "            ('in spite of', 'the', 'fact'): 'although',\n",
        "            ('with regard to', 'the', 'matter'): 'about'\n",
        "        }\n",
        "\n",
        "    def char_similarity_ratio(self, str1: str, str2: str) -> float:\n",
        "        \"\"\"Calculate character-level similarity between two strings.\"\"\"\n",
        "        if len(str1) == 0 or len(str2) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common_chars = sum(1 for c in str1 if c in str2)\n",
        "        return common_chars / max(len(str1), len(str2))\n",
        "\n",
        "    def has_opposite_meaning(self, word: str, candidate: str) -> bool:\n",
        "        \"\"\"Check if the candidate is just the negative form of the word.\"\"\"\n",
        "        for prefix in self.negative_prefixes:\n",
        "            if (candidate.startswith(prefix) and word in candidate) or \\\n",
        "               (word.startswith(prefix) and candidate in word):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def compute_similarity_score(self, word1: str, word2: str) -> float:\n",
        "        # Combine multiple similarity metrics\n",
        "        scores = []\n",
        "\n",
        "\n",
        "        # Sentence-BERT similarity\n",
        "        sbert_sim = self.sentence_model.encode([word1, word2])\n",
        "        sbert_sim = 1 - cosine(sbert_sim[0], sbert_sim[1])\n",
        "        scores.append(sbert_sim)\n",
        "\n",
        "        # WordNet similarity\n",
        "        wn_sim = self.get_wordnet_similarity(word1, word2)\n",
        "        if wn_sim:\n",
        "            scores.append(wn_sim)\n",
        "\n",
        "        return np.mean(scores)\n",
        "\n",
        "    def get_wordnet_similarity(self, word1: str, word2: str) -> float:\n",
        "        \"\"\"Calculate WordNet similarity between two words.\"\"\"\n",
        "        try:\n",
        "            synsets1 = wordnet.synsets(word1)\n",
        "            synsets2 = wordnet.synsets(word2)\n",
        "\n",
        "            if not synsets1 or not synsets2:\n",
        "                return 0.0\n",
        "\n",
        "            max_sim = 0.0\n",
        "            for s1 in synsets1:\n",
        "                for s2 in synsets2:\n",
        "                    try:\n",
        "                        sim = s1.path_similarity(s2)\n",
        "                        if sim and sim > max_sim:\n",
        "                            max_sim = sim\n",
        "                    except:\n",
        "                        continue\n",
        "            return max_sim\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def get_replacement_candidates(self, sentence: str, word: str, pos: str) -> List[str]:\n",
        "        if word.lower() in self.formal_replacements:\n",
        "            candidates = self.formal_replacements[word.lower()]\n",
        "            # Prefer common words from predefined replacements\n",
        "            common_candidates = [c for c in candidates if c in self.common_words]\n",
        "            return common_candidates if common_candidates else candidates\n",
        "\n",
        "        if (sentence, word) in self.cache:\n",
        "            return self.cache[(sentence, word)]\n",
        "\n",
        "        candidates = []\n",
        "\n",
        "        # RoBERTa predictions\n",
        "        masked_text = sentence.replace(word, self.roberta_tokenizer.mask_token)\n",
        "        inputs = self.roberta_tokenizer(masked_text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.roberta_model(**inputs)\n",
        "            predictions = outputs.logits\n",
        "\n",
        "        mask_idx = torch.where(inputs[\"input_ids\"][0] == self.roberta_tokenizer.mask_token_id)[0]\n",
        "        top_tokens = torch.topk(predictions[0, mask_idx], 20, dim=1)\n",
        "\n",
        "        # Combine candidates\n",
        "        for token_id in top_tokens.indices[0]:\n",
        "            candidate = self.roberta_tokenizer.decode(token_id).strip()\n",
        "            if self.validate_candidate(candidate, word, pos):\n",
        "                candidates.append(candidate)\n",
        "\n",
        "        # Filter and rank candidates\n",
        "        ranked_candidates = []\n",
        "        for candidate in set(candidates):\n",
        "            similarity = self.compute_similarity_score(word, candidate)\n",
        "            if similarity > 0.5:  # Adjusted threshold\n",
        "                ranked_candidates.append((candidate, similarity))\n",
        "\n",
        "        ranked_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Additional filtering for final candidates\n",
        "        final_candidates = []\n",
        "        for candidate, similarity in ranked_candidates:\n",
        "            # Prefer common words\n",
        "            score_boost = 0.1 if candidate.lower() in self.common_words else 0\n",
        "            adjusted_similarity = similarity + score_boost\n",
        "\n",
        "            if adjusted_similarity > 0.5:\n",
        "                final_candidates.append(candidate)\n",
        "\n",
        "        # Limit to top 5 candidates, preferring common words\n",
        "        final_candidates.sort(key=lambda x: x in self.common_words, reverse=True)\n",
        "        final_candidates = final_candidates[:5]\n",
        "\n",
        "        self.cache[(sentence, word)] = final_candidates\n",
        "        return final_candidates\n",
        "\n",
        "    def validate_candidate(self, candidate: str, original: str, pos: str) -> bool:\n",
        "        if not candidate.isalpha() or len(candidate) < 3:\n",
        "            return False\n",
        "\n",
        "        if candidate.lower() == original.lower():\n",
        "            return False\n",
        "\n",
        "        # Check for character similarity to avoid misspellings\n",
        "        char_sim = self.char_similarity_ratio(candidate.lower(), original.lower())\n",
        "        if char_sim > 0.8:  # If words are too similar, likely a misspelling\n",
        "            return False\n",
        "\n",
        "        # Check for negative forms\n",
        "        if self.has_opposite_meaning(original.lower(), candidate.lower()):\n",
        "            return False\n",
        "\n",
        "        doc = self.nlp(candidate)\n",
        "        if doc[0].pos_ != pos:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def extract_ngrams(self, tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
        "        \"\"\"Extract n-grams from a list of tokens.\"\"\"\n",
        "        return list(zip(*[tokens[i:] for i in range(n)]))\n",
        "\n",
        "    def find_ngram_matches(self, tokens: List[str]) -> List[Tuple[int, int, str]]:\n",
        "        \"\"\"Find matching n-grams and their positions in the text.\"\"\"\n",
        "        matches = []\n",
        "\n",
        "        # Check trigrams first\n",
        "        trigrams = self.extract_ngrams(tokens, 3)\n",
        "        for i, trigram in enumerate(trigrams):\n",
        "            if trigram in self.ngram_replacements:\n",
        "                matches.append((i, i + 3, self.ngram_replacements[trigram]))\n",
        "\n",
        "        # Then check bigrams\n",
        "        bigrams = self.extract_ngrams(tokens, 2)\n",
        "        for i, bigram in enumerate(bigrams):\n",
        "            # Skip if overlaps with a trigram match\n",
        "            if any(i >= start and i < end for start, end, _ in matches):\n",
        "                continue\n",
        "            if bigram in self.ngram_replacements:\n",
        "                matches.append((i, i + 2, self.ngram_replacements[bigram]))\n",
        "\n",
        "        return sorted(matches, key=lambda x: x[0])\n",
        "\n",
        "    def humanize_text(self, text: str, replacement_prob: float = 0.7) -> Tuple[str, Dict]:\n",
        "        doc = self.nlp(text)\n",
        "        replacements = {}\n",
        "\n",
        "        print(colored(\"\\nProcessing text...\", \"cyan\"))\n",
        "        result_text = []\n",
        "\n",
        "        for sent in tqdm(list(doc.sents), desc=\"Analyzing sentences\"):\n",
        "            tokens = [token.text for token in sent]\n",
        "            ngram_matches = self.find_ngram_matches(tokens)\n",
        "\n",
        "            # Apply n-gram replacements first\n",
        "            current_pos = 0\n",
        "            for start, end, replacement in ngram_matches:\n",
        "                # Add text before the n-gram\n",
        "                while current_pos < start:\n",
        "                    result_text.append(tokens[current_pos])\n",
        "                    current_pos += 1\n",
        "\n",
        "                # Add the replacement\n",
        "                original_phrase = \" \".join(tokens[start:end])\n",
        "                replacements[original_phrase] = replacement\n",
        "                result_text.append(replacement)\n",
        "                current_pos = end\n",
        "\n",
        "            # Process remaining tokens\n",
        "            while current_pos < len(tokens):\n",
        "                token = sent[current_pos]\n",
        "\n",
        "                if (token.pos_ in self.target_pos and\n",
        "                    len(token.text) > 3 and\n",
        "                    random.random() < replacement_prob):\n",
        "\n",
        "                    candidates = self.get_replacement_candidates(sent.text, token.text, token.pos_)\n",
        "                    if candidates:\n",
        "                        replacement = random.choice(candidates)\n",
        "                        replacements[token.text] = replacement\n",
        "                        result_text.append(replacement)\n",
        "                    else:\n",
        "                        result_text.append(token.text)\n",
        "                else:\n",
        "                    result_text.append(token.text)\n",
        "\n",
        "                result_text.append(token.whitespace_)\n",
        "                current_pos += 1\n",
        "\n",
        "        result_text = \"\".join(result_text)\n",
        "        print(colored(\"\\nHumanization complete!\", \"green\"))\n",
        "        return result_text, replacements\n",
        "\n",
        "    def format_output(self, original_text: str, humanized_text: str, replacements: Dict) -> str:\n",
        "        output = \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        output += colored(\"TEXT HUMANIZATION RESULTS\", \"cyan\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += \"=\"*80 + \"\\n\\n\"\n",
        "\n",
        "        # Original text section\n",
        "        output += colored(\"ORIGINAL TEXT:\", \"yellow\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += f\"{original_text}\\n\\n\"\n",
        "\n",
        "        # Humanized text section\n",
        "        output += colored(\"HUMANIZED TEXT:\", \"green\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += f\"{humanized_text}\\n\\n\"\n",
        "\n",
        "        # Statistics section\n",
        "        output += colored(\"STATISTICS:\", \"magenta\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        output += f\"Total words replaced: {len(replacements)}\\n\"\n",
        "        output += f\"Unique replacements: {len(set(replacements.values()))}\\n\\n\"\n",
        "\n",
        "        # Replacements section\n",
        "        output += colored(\"WORD REPLACEMENTS:\", \"blue\", attrs=[\"bold\"]) + \"\\n\"\n",
        "        for original, replacement in replacements.items():\n",
        "            output += f\"• {colored(original, 'red')} → {colored(replacement, 'green')}\\n\"\n",
        "\n",
        "        output += \"\\n\" + \"=\"*80 + \"\\n\"\n",
        "        return output\n",
        "\n",
        "    def __del__(self):\n",
        "        self.cache.clear()\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    humanizer = AdvancedTextHumanizer()\n",
        "\n",
        "    text = \"\"\"\n",
        "\n",
        "    Natural Language Processing (NLP) is essential for enabling machines to understand and interpret human language. Preprocessing is a crucial step in NLP that prepares raw text data for analysis. In our application, which focuses on distinguishing between AI-generated and human-written texts, effective preprocessing ensures accurate and meaningful results.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    humanized_text, replacements = humanizer.humanize_text(text)\n",
        "    formatted_output = humanizer.format_output(text, humanized_text, replacements)\n",
        "    print(formatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlfqXwAuStod",
        "outputId": "c9cc927a-7d2d-4005-fa8e-8ba939dddca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing AdvancedTextHumanizer...\n",
            "✓ Loading language models\n",
            "\n",
            "Processing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing sentences: 100%|██████████| 3/3 [00:25<00:00,  8.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Humanization complete!\n",
            "\n",
            "================================================================================\n",
            "TEXT HUMANIZATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "ORIGINAL TEXT:\n",
            "\n",
            "\n",
            "    Natural Language Processing (NLP) is essential for enabling machines to understand and interpret human language. Preprocessing is a crucial step in NLP that prepares raw text data for analysis. In our application, which focuses on distinguishing between AI-generated and human-written texts, effective preprocessing ensures accurate and meaningful results.\n",
            "    \n",
            "    \n",
            "\n",
            "HUMANIZED TEXT:\n",
            "\n",
            "\n",
            "      Natural   Language   Processing   (  NLP  )   is   crucial   for   enabling   computers   to   read   and   understand   human   words  .   Preprocessing   is   a   crucial   step   in   NLP   this   prepares   raw   text   information   for   analysis  .   In   our   program  ,   this   focus   on   discriminating   between   AI  -  produced   and   human  -  authored   text  ,   effective   preprocessing   guarantees   accurate   and   meaningful   responses  .  \n",
            "    \n",
            "     \n",
            "\n",
            "STATISTICS:\n",
            "Total words replaced: 16\n",
            "Unique replacements: 15\n",
            "\n",
            "WORD REPLACEMENTS:\n",
            "• essential → crucial\n",
            "• machines → computers\n",
            "• understand → read\n",
            "• interpret → understand\n",
            "• language → words\n",
            "• that → this\n",
            "• data → information\n",
            "• application → program\n",
            "• which → this\n",
            "• focuses → focus\n",
            "• distinguishing → discriminating\n",
            "• generated → produced\n",
            "• written → authored\n",
            "• texts → text\n",
            "• ensures → guarantees\n",
            "• results → responses\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6zSrlk5SweQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}