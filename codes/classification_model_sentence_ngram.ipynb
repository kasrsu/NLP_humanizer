{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DuV5a8BDBrpy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FJa1m97LBrpy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/sample_data/sentences_pairs_original.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-LQbqLD-Brpz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E86I_EYtBrpz",
        "outputId": "29e72e18-e22e-454f-cb45-3a68dab29819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   Text  Label\n",
            "0        Error: Invalid URL 'link': No scheme supplied.      0\n",
            "1                       Perhaps you meant https://link?      0\n",
            "2     The map runs to sixteen laminated foolscap pag...      0\n",
            "3     I have been given it on the condition that I d...      0\n",
            "4     It is not like any map I have ever seen, and I...      0\n",
            "...                                                 ...    ...\n",
            "2543                  The aperture drastically narrows!      1\n",
            "2544  The captain states, \"Diving daDoria is akin to...      1\n",
            "2545  \"He advises the diver to fasten the mask strap...      1\n",
            "2546   \"Once, there existed a certain female entity...\"      1\n",
            "2547  Refashion the given phrase into an AI-like syn...      1\n",
            "\n",
            "[5096 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Create a new dataframe with combined columns\n",
        "new_df = pd.DataFrame({\n",
        "    'Text': pd.concat([df['sentence'], df['ai_sentence']]),\n",
        "    'Label': [0] * len(df) + [1] * len(df)\n",
        "})\n",
        "\n",
        "print(new_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ6367IRBrpz",
        "outputId": "be78f894-1102-4818-8562-dadaa12b10d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Text  Label\n",
            "0                                 Their teeth clench.      0\n",
            "1   I found myself uncertain about the anticipated...      1\n",
            "2    Resources are to be shared, even with strangers.      0\n",
            "3   Occasionally, on a few select occasions annual...      1\n",
            "4   Following the conclusion of his occupied New Y...      1\n",
            "5   Although Steves spends nearly half his life tr...      0\n",
            "6   So when he finishes building a gyroscope out o...      0\n",
            "7   The captain states, \"Diving daDoria is akin to...      1\n",
            "8   The latest photographic subject of the magazin...      1\n",
            "9   On our final day in Paris, we regrettably lack...      1\n",
            "10  Optimal, according to their unique specificati...      1\n",
            "11  His preference leans towards casual dining est...      1\n",
            "12  Post-dinner, upon readying for lodging at a ne...      1\n",
            "13  It was an uncanny coincidence that the specifi...      1\n",
            "14  His emotional state transitioned to a combinat...      1\n",
            "15  You savor these last honest moments of being a...      0\n",
            "16  In their small diving bell of a capsule, sandw...      0\n",
            "17  After one recent speech in the Deep South, eve...      0\n",
            "18  By JON MOOALLEM DAVID BENJAMIN SHERRY The whal...      0\n",
            "19            \"It is necessary to inquire from them.\"      1\n",
            "20  He transformed travel into an essential self-d...      1\n",
            "21  At a nearby café, Boris explained that during ...      0\n",
            "22                           But, hey, you know what?      0\n",
            "23  The distinct sheen of America, as experienced ...      1\n",
            "24  The ferocity of the rain and wind had diminish...      1\n",
            "25  The upcoming journey signifies that Pettit and...      1\n",
            "26                 Perpetually, the light fades away.      1\n",
            "27  Our signal would have covered two or three mil...      0\n",
            "28  At the end of his talk, Steves offered to sign...      0\n",
            "29  It was Guillaumot who initiated the first mapp...      0\n",
            "30              \"We're Americans,\" I offered finally.      0\n",
            "31  In this instance, Expedition Six is instructed...      1\n",
            "32                      I'm going to church to pray.\"      0\n",
            "33               It was a tremendous silence to fill.      0\n",
            "34  With such swiftness did our flight bypass youn...      1\n",
            "35  In 2012, Steves campaigned hard for Washington...      0\n",
            "36  Within the specified area of the catacombs, an...      1\n",
            "37  A prolonged pause filled with an uncomfortable...      1\n",
            "38  It was improbable that one could contact indiv...      1\n",
            "39  The global expanse of urban exploration encomp...      1\n",
            "40  An adept and seasoned subaquatic explorer, unf...      1\n",
            "41  Upon discovering it, virtually every other ite...      1\n",
            "42                                         He was in?      0\n",
            "43  It slaps the dead-calm surface, spreading ripp...      0\n",
            "44  He accrued income by instructing piano lessons...      1\n",
            "45  Remarkably, within this confined area, Amtrak ...      1\n",
            "46  A select few of the cataphiles descended for a...      1\n",
            "47  They are of Second World War origin, Lina says...      0\n",
            "48            In Sweden, on an island, in the forest.      0\n",
            "49  It's evident that Baldessari requires persuasion.      1\n"
          ]
        }
      ],
      "source": [
        "shuffled_df = new_df.sample(frac=1).reset_index(drop=True)\n",
        "print(shuffled_df.head(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBHU1X30Brpz",
        "outputId": "5efbef5c-46e2-4179-9220-f9e4f511326b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Text  Label  \\\n",
            "0                                 Their teeth clench.      0   \n",
            "1   I found myself uncertain about the anticipated...      1   \n",
            "2    Resources are to be shared, even with strangers.      0   \n",
            "3   Occasionally, on a few select occasions annual...      1   \n",
            "4   Following the conclusion of his occupied New Y...      1   \n",
            "5   Although Steves spends nearly half his life tr...      0   \n",
            "6   So when he finishes building a gyroscope out o...      0   \n",
            "7   The captain states, \"Diving daDoria is akin to...      1   \n",
            "8   The latest photographic subject of the magazin...      1   \n",
            "9   On our final day in Paris, we regrettably lack...      1   \n",
            "10  Optimal, according to their unique specificati...      1   \n",
            "11  His preference leans towards casual dining est...      1   \n",
            "12  Post-dinner, upon readying for lodging at a ne...      1   \n",
            "13  It was an uncanny coincidence that the specifi...      1   \n",
            "14  His emotional state transitioned to a combinat...      1   \n",
            "15  You savor these last honest moments of being a...      0   \n",
            "16  In their small diving bell of a capsule, sandw...      0   \n",
            "17  After one recent speech in the Deep South, eve...      0   \n",
            "18  By JON MOOALLEM DAVID BENJAMIN SHERRY The whal...      0   \n",
            "19            \"It is necessary to inquire from them.\"      1   \n",
            "\n",
            "                                             POS_Tags  \n",
            "0         [(Their, PRP$), (teeth, NNS), (clench, NN)]  \n",
            "1   [(I, PRP), (found, VBD), (myself, PRP), (uncer...  \n",
            "2   [(Resources, NNS), (are, VBP), (to, TO), (be, ...  \n",
            "3   [(Occasionally, RB), (on, IN), (a, DT), (few, ...  \n",
            "4   [(Following, VBG), (the, DT), (conclusion, NN)...  \n",
            "5   [(Although, IN), (Steves, NNP), (spends, VBZ),...  \n",
            "6   [(So, RB), (when, WRB), (he, PRP), (finishes, ...  \n",
            "7   [(The, DT), (captain, NN), (states, NNS), (Div...  \n",
            "8   [(The, DT), (latest, JJS), (photographic, JJ),...  \n",
            "9   [(On, IN), (our, PRP$), (final, JJ), (day, NN)...  \n",
            "10  [(Optimal, JJ), (according, VBG), (to, TO), (t...  \n",
            "11  [(His, PRP$), (preference, NN), (leans, VBZ), ...  \n",
            "12  [(Postdinner, NNP), (upon, IN), (readying, VBG...  \n",
            "13  [(It, PRP), (was, VBD), (an, DT), (uncanny, JJ...  \n",
            "14  [(His, PRP$), (emotional, JJ), (state, NN), (t...  \n",
            "15  [(You, PRP), (savor, VBP), (these, DT), (last,...  \n",
            "16  [(In, IN), (their, PRP$), (small, JJ), (diving...  \n",
            "17  [(After, IN), (one, CD), (recent, JJ), (speech...  \n",
            "18  [(By, IN), (JON, NNP), (MOOALLEM, NNP), (DAVID...  \n",
            "19  [(It, PRP), (is, VBZ), (necessary, JJ), (to, T...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "# Download the specific English language model\n",
        "nltk.download('averaged_perceptron_tagger_eng') # This line is added\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation, special characters, and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Perform POS tagging\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "# Apply the function to the 'Text' column of shuffled_df\n",
        "shuffled_df['POS_Tags'] = shuffled_df['Text'].apply(lambda x: preprocess_text(x) if pd.notnull(x) else x)\n",
        "\n",
        "print(shuffled_df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aPqWcpWiBrp0"
      },
      "outputs": [],
      "source": [
        "shuffled_df.to_csv('/content/sample_data/shuffled_final_sentence.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7K4ZQmVBrp0",
        "outputId": "9d0de934-fca7-4ce0-d99b-2c0f969bf777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text  \\\n",
            "0                                Their teeth clench.   \n",
            "1  I found myself uncertain about the anticipated...   \n",
            "2   Resources are to be shared, even with strangers.   \n",
            "3  Occasionally, on a few select occasions annual...   \n",
            "4  Following the conclusion of his occupied New Y...   \n",
            "\n",
            "                                 Normalized_POS_Tags  \n",
            "0  {'PRP$': 0.3333333333333333, 'NNS': 0.33333333...  \n",
            "1  {'PRP': 0.25, 'VBD': 0.125, 'JJ': 0.25, 'IN': ...  \n",
            "2  {'NNS': 0.25, 'VBP': 0.125, 'TO': 0.125, 'VB':...  \n",
            "3  {'RB': 0.11428571428571428, 'IN': 0.1428571428...  \n",
            "4  {'VBG': 0.07407407407407407, 'DT': 0.148148148...  \n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def normalize_pos_tags(pos_tags):\n",
        "    # Count the frequency of each POS tag\n",
        "    tag_counts = Counter(tag for word, tag in pos_tags)\n",
        "\n",
        "    # Normalize the frequencies by the length of the sequence\n",
        "    total_tags = sum(tag_counts.values())\n",
        "    normalized_counts = {tag: count / total_tags for tag, count in tag_counts.items()}\n",
        "\n",
        "    return normalized_counts\n",
        "\n",
        "# Apply the function to the 'POS_Tags' column of shuffled_df\n",
        "shuffled_df['Normalized_POS_Tags'] = shuffled_df['POS_Tags'].apply(lambda x: normalize_pos_tags(x) if isinstance(x, list) else x)\n",
        "\n",
        "print(shuffled_df[['Text', 'Normalized_POS_Tags']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LO-se0eBrp0",
        "outputId": "6329b6cd-7036-485a-d08d-654c2ebd30a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0mCombined features saved as combined_features.csv\n",
            "TF-IDF vectorizer saved as tfidf_vectorizer.pkl\n",
            "       PRP$       NNS        NN       PRP       VBD        JJ        IN  \\\n",
            "0  0.333333  0.333333  0.333333  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.125000  0.250000  0.125000  0.250000  0.125000   \n",
            "2  0.000000  0.250000  0.000000  0.000000  0.000000  0.000000  0.125000   \n",
            "3  0.028571  0.057143  0.142857  0.057143  0.028571  0.114286  0.142857   \n",
            "4  0.037037  0.000000  0.185185  0.037037  0.037037  0.037037  0.148148   \n",
            "\n",
            "         DT    VBP        TO  ...  zone where  zones  zones and  zones from  \\\n",
            "0  0.000000  0.000  0.000000  ...         0.0    0.0        0.0         0.0   \n",
            "1  0.125000  0.000  0.000000  ...         0.0    0.0        0.0         0.0   \n",
            "2  0.000000  0.125  0.125000  ...         0.0    0.0        0.0         0.0   \n",
            "3  0.114286  0.000  0.028571  ...         0.0    0.0        0.0         0.0   \n",
            "4  0.148148  0.000  0.000000  ...         0.0    0.0        0.0         0.0   \n",
            "\n",
            "   zoom  zoom over  zubin  zubin shroff   ça  ça est  \n",
            "0   0.0        0.0    0.0           0.0  0.0     0.0  \n",
            "1   0.0        0.0    0.0           0.0  0.0     0.0  \n",
            "2   0.0        0.0    0.0           0.0  0.0     0.0  \n",
            "3   0.0        0.0    0.0           0.0  0.0     0.0  \n",
            "4   0.0        0.0    0.0           0.0  0.0     0.0  \n",
            "\n",
            "[5 rows x 78295 columns]\n"
          ]
        }
      ],
      "source": [
        "!pip install pickle\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# A. POS Tag Features\n",
        "# Convert the normalized POS tag dictionary to a DataFrame\n",
        "pos_features = pd.json_normalize(shuffled_df['Normalized_POS_Tags']).fillna(0)\n",
        "\n",
        "# B. Text Features\n",
        "# Initialize the TF-IDF Vectorizer with n-grams (unigrams and bigrams)\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "\n",
        "# Fit and transform the 'Text' column\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(shuffled_df['Text'].fillna(''))\n",
        "\n",
        "# Convert the sparse matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# C. Combine Features\n",
        "# Concatenate POS-based features with text-based features\n",
        "combined_features = pd.concat([pos_features.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
        "# Save the combined features DataFrame\n",
        "combined_features_path = 'combined_features.csv'\n",
        "combined_features.to_csv(combined_features_path, index=False)\n",
        "\n",
        "print(f'Combined features saved as {combined_features_path}')\n",
        "\n",
        "# Save the TF-IDF vectorizer\n",
        "tfidf_vectorizer_path = 'tfidf_vectorizer.pkl'\n",
        "with open(tfidf_vectorizer_path, 'wb') as file:\n",
        "    pickle.dump(tfidf_vectorizer, file)\n",
        "\n",
        "print(f'TF-IDF vectorizer saved as {tfidf_vectorizer_path}')\n",
        "print(combined_features.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SR6OErAtBrp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d902fdf-42da-4cfd-9398-571a754d7399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 4076\n",
            "Validation set size: 510\n",
            "Test set size: 510\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training (80%) and temp (20%) sets\n",
        "train_df, temp_df = train_test_split(shuffled_df, test_size=0.2, stratify=shuffled_df['Label'])\n",
        "\n",
        "# Split the temp set into validation (10%) and test (10%) sets\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['Label'])\n",
        "\n",
        "# Print the sizes of the splits\n",
        "print(f'Training set size: {len(train_df)}')\n",
        "print(f'Validation set size: {len(val_df)}')\n",
        "print(f'Test set size: {len(test_df)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8RvsgeI1Brp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbaae23-c42d-48d4-b981-c1f00c32c2d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation AUC-ROC: 0.929042675893887\n",
            "Validation Accuracy: 0.8450980392156863\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       255\n",
            "           1       0.84      0.85      0.85       255\n",
            "\n",
            "    accuracy                           0.85       510\n",
            "   macro avg       0.85      0.85      0.85       510\n",
            "weighted avg       0.85      0.85      0.85       510\n",
            "\n",
            "Model saved as lg_ai_det.pkl\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pickle\n",
        "\n",
        "X_train = combined_features.loc[train_df.index]\n",
        "y_train = train_df['Label']\n",
        "\n",
        "X_val = combined_features.loc[val_df.index]\n",
        "y_val = val_df['Label']\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization\n",
        "log_reg = LogisticRegression(penalty='l2', solver='liblinear', C=1.0)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = log_reg.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "report = classification_report(y_val, y_val_pred)\n",
        "\n",
        "auc_roc = roc_auc_score(y_val, log_reg.predict_proba(X_val)[:, 1])\n",
        "\n",
        "print(f'Validation AUC-ROC: {auc_roc}')\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print('Classification Report:')\n",
        "print(report)\n",
        "\n",
        "# Save the trained Logistic Regression model\n",
        "model_filename = 'lg_ai_det.pkl'\n",
        "with open(model_filename, 'wb') as file:\n",
        "    pickle.dump(log_reg, file)\n",
        "\n",
        "print(f'Model saved as {model_filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_KNDuvVMBrp1"
      },
      "outputs": [],
      "source": [
        "df_structure = combined_features\n",
        "# Extract column names and save them as a DataFrame\n",
        "column_names_df = pd.DataFrame(columns=df_structure.columns)\n",
        "\n",
        "# Save the column names DataFrame to a CSV file\n",
        "column_names_df.to_csv('combined_features_columns.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yloA7PgQBrp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545cc2b2-b92b-43e1-f7d5-18dfb23042dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: AI\n",
            "Probability of Not AI: 32 %\n",
            "Probability of AI: 68.4816031473547 %\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load the trained Random Forest model\n",
        "model_path = 'lg_ai_det.pkl'  # Path to your saved model\n",
        "with open(model_path, 'rb') as file:\n",
        "    rf_model = pickle.load(file)\n",
        "\n",
        "# Load the TF-IDF vectorizer\n",
        "tfidf_vectorizer_path = 'tfidf_vectorizer.pkl'  # Path to your saved TF-IDF vectorizer\n",
        "with open(tfidf_vectorizer_path, 'rb') as file:\n",
        "    tfidf_vectorizer = pickle.load(file)\n",
        "\n",
        "# Define the combined features DataFrame structure\n",
        "combined_features_columns = pd.read_csv('combined_features_columns.csv').columns\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation, special characters, and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Perform POS tagging\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "def normalize_pos_tags(pos_tags):\n",
        "    # Count the frequency of each POS tag\n",
        "    tag_counts = Counter(tag for word, tag in pos_tags)\n",
        "\n",
        "    # Normalize the frequencies by the length of the sequence\n",
        "    total_tags = sum(tag_counts.values())\n",
        "    normalized_counts = {tag: count / total_tags for tag, count in tag_counts.items()}\n",
        "\n",
        "    return normalized_counts\n",
        "\n",
        "def predict_human_or_not(text):\n",
        "    # Preprocess the input text\n",
        "    pos_tags = preprocess_text(text)\n",
        "    normalized_pos_tags = normalize_pos_tags(pos_tags)\n",
        "\n",
        "    # Convert the normalized POS tag dictionary to a DataFrame\n",
        "    pos_features = pd.json_normalize(normalized_pos_tags).fillna(0)\n",
        "\n",
        "    # Ensure the columns match those used during training\n",
        "    pos_features = pos_features.reindex(columns=combined_features_columns[:pos_features.shape[1]], fill_value=0)\n",
        "\n",
        "    # Transform the input text using the TF-IDF vectorizer\n",
        "    tfidf_features = tfidf_vectorizer.transform([text])\n",
        "    tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Ensure the columns match those used during training\n",
        "    tfidf_df = tfidf_df.reindex(columns=combined_features_columns[pos_features.shape[1]:], fill_value=0)\n",
        "\n",
        "    # Combine POS-based features with text-based features\n",
        "    combined_input_features = pd.concat([pos_features.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Predict using model\n",
        "    prediction = rf_model.predict(combined_input_features)\n",
        "    prediction_proba = rf_model.predict_proba(combined_input_features)\n",
        "\n",
        "    return \"Not AI\" if prediction[0] == 0 else \"AI\", prediction_proba[0]\n",
        "\n",
        "# Example usage\n",
        "input_text = \"\"\"\n",
        "The combination of POS tag features and TF-IDF vectorization offers a robust approach to text classification. POS tags provide insights into the grammatical structure of the text, which can differ between human and AI writing styles. For instance, AI-generated text might have more consistent and predictable grammar, while human text might be more varied.\n",
        "\n",
        "TF-IDF vectorization, on the other hand, captures the importance of specific words within the text, which can help identify unique vocabulary patterns. For example, AI-generated text might use a more limited vocabulary or repeat certain phrases more frequently.\n",
        "\n",
        "By integrating these features, the model can leverage both grammatical and lexical information to make more accurate classifications. This multi-faceted approach enhances the model's ability to detect subtle differences between human and AI text, improving overall classification performance.\n",
        "\"\"\"\n",
        "result, proba = predict_human_or_not(input_text)\n",
        "print(f\"Prediction: {result}\")\n",
        "print(f\"Probability of Not AI: {round(proba[0]*100)} %\")\n",
        "print(f\"Probability of AI: {(proba[1])*100} %\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}