urls,Extracted Paragraphs
https://medium.com/deep-learning-demystified/introduction-to-neural-networks-part-1-e13f132c6d7e,"Sign up Sign in Sign up Sign in Harsha Bommana Follow Deep Learning Demystified -- 2 Listen Share Neural Networks have become a huge hit in the recent Machine Learning craze due to their significantly better performance than traditional Machine Learning algorithms in many cases. The art and science ofDeep Learningis built on the foundation of Neural Networks and how they work. Hence demystifying Neural Networks is going to be the first step in demystifying Deep Learning. Letâ€™s dive in! How do we define a Neural Network? It is essentially a naive implementation of how our brains might work. Itâ€™s not a very accurate representation but it tries to replicate some of the methods our brain uses to learn from itâ€™s mistakes. Letâ€™s look at how our brains work from a simplified perspective and then compare it with a Neural Network. The brain is essentially a bunch of neurons connected to each other in a huge interconnected network. There are a lot of neurons and even more connections. These neurons pass a small amount of electrical charge to each other as a way to transmit information. Another important feature of these neural connections is that the connection between two neurons can be vary betweenstrongandweak.A strong connection allows more charge to flow between them and a weak one allows lesser. A neuron pathway which frequently transmits charge will eventually become astrong pathway. Now as the brain takes input from any external source, letâ€™s say for example we touch a hot pan. The nerves from our hand transmits info to certain neurons in our brain. Now there is a pathway from these neurons to the neurons which control our hand. And in these cases our brain haslearntthat the best option is to move our hand from the pan ASAP. Hence this certainpathwaybetween the neurons taking input from the hand and the neurons controlling the hand will bestrong. Neural pathways becomestrongerupon frequent usage, and our brain essentially tries to use pathways which have proven to give us better results over time. So essentially as we humans live our lives and decide whether our actions are good or bad, we are training our brain to make sure we donâ€™t repeat our previous mistakes or keep doing things which we think resulted in a good outcome. This is a highly simplified explanation and doesnâ€™t fully portray whatâ€™s going on, but hopefully it helps you understand the basic concept. Now letâ€™s understand how a Neural Network is represented. A neural network consists of manyNodes(Neurons) in manylayers.Each layer can haveany numberof nodes and a neural network can haveany numberof layers. Letâ€™s have a closer look at a couple of layers. Now as you can see, there are many interconnections between both the layers. These interconnections exist betweeneach nodein the first layer witheach and every nodein the second layer. These are also called theweightsbetween two layers. Now letâ€™s see how exactly these weights function. Here we take the example of whatâ€™s going on with asingle nodein the network. Here we are considering all the values from theprevious layerconnecting toone node in the next layer. Y is thefinal valueof the node. W represents theweightsbetween the nodes in the previous layer and the output node. X represents thevalues of the nodesof the previous layer. B representsbias, which is an additional value present for each neuron. Bias is essentially a weight without an input term. Itâ€™s useful for having anextra bit of adjustabilitywhich is not dependant on previous layer. H is theintermediate node value. This is not the final value of the node. f( ) is called anActivation Functionand it is something we can choose. We will go through itâ€™s importance later. So finally, the output value of this node will bef(0.57) Now letâ€™s look at the calculations between two complete layers: The weights in this case have been colour coded for easier understanding. We can represent the entire calculation as a matrix multiplication. If we represent the weights corresponding to each input node as vectors and arrange them horizontally, we can form a matrix, this is called theweight matrix.Now we can multiply the weight matrix with the input vector and then add the bias vector to get the intermediate node values. We can summarize the entire calculation asY = f(W*X + B). Here, Y is the output vector, X is the input vector, W represents the weight matrix between the two layers and B is the bias vector. We can determine the size of the weight matrix by looking at the number of input nodes and output nodes. An M*N weight matrix means that it is between two layers with thefirst layerhavingN nodesand thesecond layerhavingM nodes. Now letâ€™s look at a complete neural network. This is a small neural network of four layers. The input layer is where we feed ourexternal stimulus, or basically thedatafrom which our neural network has tolearn from. The output layer is where we are supposed to get the target value, this represents what exactly our neural network is trying topredictorlearn.All layers in between are calledhidden layers.When we feed the inputs into the first layer, the values of the nodes will be calculated layer by layer using the matrix multiplications and activation functions till we get the final values at the output layer. That is how we get anoutputfrom a neural network. So essentially a neural network is, simply put, a series of matrix multiplications and activation functions. When we input a vector containing the input data, the data is multiplied with the sequence of weight matrices and subjected to activation functions till it reaches the output layer, which contains thepredictionsof the neural network corresponding to that particular input. Even though our neural network has a very complex configuration of weights, it will not be able to solve a problem without the activation function. The reason for this lies in the concept ofNon Linearity. Letâ€™s revise what linearity and non linearity means. The above equation represents alinear relationshipbetween Y and X1,X2. Regardless of what values W1 and W2 have, at the end of the day the change of value of X1 and X2 will result in alinearchange in Y. Now if we look at real world data we realize this is actually not desirable because data often hasnon linearrelationships between the input and output variables. The above diagram represents a typical dataset which shows a non-linear relationship between X and Y. If we try to fit a linear relationship on the data, we will end up with thered line,which is not a very accurate representation of the data. However if our relationship can benon linear, we are able to get the green line, which is much better. Now letâ€™s compare the neural network equationwith and without the activation function. We can observe that in this equation, there exists alinear relationshipbetween the input and the output. However in the case of the equationwith activation function, we can say that the relationship between input and output can be non linear, IF the activation function isitself non linear. Hence all we have to do is keep some non linear function as the activation function for each neuron and our neural network is nowcapableof fitting on non linear data. Letâ€™s look at a couple of popular activation functions: ReLU:ReLU stands for Rectified Linear Unit. It essentially becomes an identity function (y = x) when x â‰¥ 0 and becomes 0 when x < 0. This is a very widely used activation function because its a nonlinear function and it is very simple. Sigmoid:Sigmoid is essentially a function bounded between 0 and 1. It will become 0 for values which are very negative and 1 for values which are very positive. Hence this functionsquishesvalues which are very high or very low to values between 0 and 1. This is useful in neural networks sometimes to ensure values arenâ€™t extremely high or low. This function is usually used at the last layer when we need values which are binary (0 or 1). This concludes this part of the tutorial. The next part will explain in detail how exactly we can use our data to train our neural network. Thank you for reading! Link to Part 2 Read more Deep Learning articles athttps://deeplearningdemystified.com -- -- 2 Simple intuitive explanations for  everything Deep Learning. From basic concepts to cutting edge advances. https://www.deeplearningdemystified.com/ Help Status About Careers Press Blog Privacy Terms Text to speech Teams"
https://www.sciencedirect.com/journal/neural-networks,Error: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/journal/neural-networks
https://en.wikipedia.org/wiki/Neural_network,"Aneural networkis a group of interconnected units calledneuronsthat send signals to one another. Neurons can be eitherbiological cellsormathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network. In the context of biology, a neural network is a population of biologicalneuronschemically connected to each other bysynapses. A given neuron can be connected to hundreds of thousands of synapses.[1]Each neuron sends and receiveselectrochemicalsignals calledaction potentialsto its connected neighbors. A neuron can serve anexcitatoryrole, amplifying and propagating signals it receives, or aninhibitoryrole, suppressing signals instead.[1] Populations of interconnected neurons that are smaller than neural networks are calledneural circuits. Very large interconnected networks are calledlarge scale brain networks, and many of these together formbrainsandnervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and acrossneuromuscular junctionstomuscle cells, where they cause contraction and thereby motion.[2] In machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines,[3]today they are almost always implemented insoftware. Neuronsin an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer).[4]The ""signal"" input to each neuron is a number, specifically alinear combinationof the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to itsactivation function. The behavior of the network depends on the strengths (orweights) of the connections between neurons. A network is trained by modifying these weights throughempirical risk minimizationorbackpropagationin order to fit some preexisting dataset.[5] Neural networks are used to solve problems inartificial intelligence, and have thereby found applications in many disciplines, includingpredictive modeling,adaptive control,facial recognition,handwriting recognition,general game playing, andgenerative AI. The theoretical base for contemporary neural networks was independently proposed byAlexander Bainin 1873[6]andWilliam Jamesin 1890.[7]Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949,Donald HebbdescribedHebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.[8] Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach ofconnectionism. However, starting with the invention of theperceptron, a simple artificial neural network, byWarren McCullochandWalter Pittsin 1943,[9]followed by the implementation of one in hardware byFrank Rosenblattin 1957,[3]artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts."
https://www.ibm.com/think/topics/neural-networks,"A neural network is amachine learningprogram, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions. Every neural network consists of layers of nodes, or artificial neuronsâ€”an input layer, one or more hidden layers, and an output layer. Each node connects to others, and has its own associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network. Neural networks rely on training data to learn and improve their accuracy over time. Once they are fine-tuned for accuracy, they are powerful tools in computer science andartificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the best-known examples of a neural network is Googleâ€™s search algorithm. Neural networks are sometimes calledÂ artificial neural networksÂ (ANNs) orÂ simulated neural networksÂ (SNNs).Â They are a subset of machine learning, and at the heart ofdeep learningmodels. Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. Think of each individual node as its ownlinear regressionmodel, composed of input data, weights, a bias (or threshold), and an output.Â The formula would look something like this: âˆ‘wixi + bias = w1x1 + w2x2 + w3x3 + bias output = f(x) = 1 if âˆ‘w1x1 + b>= 0; 0 if âˆ‘w1x1 + b < 0 Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it â€œfiresâ€ (or activates) the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network. Letâ€™s break down what one single node might look like using binary values. We can apply this concept to a more tangible example, like whether you should go surfing (Yes: 1, No: 0). The decision to go or not to go is our predicted outcome, or y-hat. Letâ€™s assume that there are three factors influencing your decision-making: Then, letâ€™s assume the following, giving us the following inputs: Now, we need to assign some weights to determine importance. Larger weights signify that particular variables are of greater importance to the decision or outcome. Finally, weâ€™ll also assume a threshold value of 3, which would translate to a bias value of â€“3. With all the various inputs, we can start to plug in values into the formula to get the desired output. Y-hat = (1*5) + (0*2) + (1*4) â€“ 3 = 6 If we use the activation function from the beginning of this section, we can determine that the output of this node would be 1, since 6 is greater than 0. In this instance, you would go surfing; but if we adjust the weights or the threshold, we can achieve different outcomes from the model. When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers. In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network. As we start to think about more practical use cases for neural networks, like image recognition or classification, weâ€™ll leverage supervised learning, or labeled datasets, to train the algorithm. As we train the model, weâ€™ll want to evaluate its accuracy using a cost (or loss) function. This is also commonly referred to as the mean squared error (MSE). In the equation below, ð¶ð‘œð‘ ð‘¡ ð¹ð‘¢ð‘›ð‘ð‘¡ð‘–ð‘œð‘›= ð‘€ð‘†ð¸=1/2ð‘š âˆ‘129_(ð‘–=1)^ð‘šâ–’(ð‘¦Â Ì‚^((ð‘–) )âˆ’ð‘¦^((ð‘–) ) )^2 Ultimately, the goal is to minimize our cost function to ensure correctness of fit for any given observation. As the model adjusts its weights and bias, it uses the cost function and reinforcement learning to reach the point of convergence, or the local minimum. The process in which the algorithm adjusts its weights is through gradient descent, allowing the model to determine the direction to take to reduce errors (or minimize the cost function). With each training example, the parameters of the model adjust to gradually converge at the minimum. See thisIBM Developer article for a deeper explanation of the quantitative concepts involved in neural networks. Most deep neural networks are feedforward, meaning they flow in one direction only, from input to output. However, you can also train your model through backpropagation; that is, move in the opposite direction from output to input. Backpropagation allows us to calculate and attribute the error associated with each neuron, allowing us to adjust and fit the parameters of the model(s) appropriately. Neural networks can be classified into different types, which are used for different purposes. While this isnâ€™t a comprehensive list of types, the below would be representative of the most common types of neural networks that youâ€™ll come across for its common use cases: The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958. Feedforward neural networks, or multi-layer perceptrons (MLPs), are what weâ€™ve primarily been focusing on within this article. They are comprised of an input layer, a hidden layer or layers, and an output layer. While these neural networks are also commonly referred to as MLPs, itâ€™s important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation for computer vision,natural language processing, and other neural networks. Convolutional neural networks (CNNs)are similar to feedforward networks, but theyâ€™re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image. Recurrent neural networks (RNNs)are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting. Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing. As a result, itâ€™s worth noting that the â€œdeepâ€ in deep learning is just referring to the depth of layers in a neural network. A neural network that consists of more than three layersâ€”which would be inclusive of the inputs and the outputâ€”can be considered a deep learning algorithm. A neural network that only has two or three layers is just a basic neural network. To learn more about the differences between neural networks and other forms of artificial intelligence, like machine learning, please read the blog post â€œAI vs. Machine Learning vs. Deep Learning vs. Neural Networks: Whatâ€™s the Difference?â€ The history of neural networks is longer than most people think. While the idea of â€œa machine that thinksâ€ can be traced to the Ancient Greeks, weâ€™ll focus on the key events that led to the evolution of thinking around neural networks, which has ebbed and flowed in popularity over the years: 1943:Warren S. McCulloch and Walter Pitts published â€œA logical calculus of the ideas immanent in nervous activity(link resides outside ibm.com)â€ This research sought to understand how the human brain could produce complex patterns through connected brain cells, or neurons. One of the main ideas that came out of this work was the comparison of neurons with a binary threshold to Boolean logic (i.e., 0/1 or true/false statements). 1958:Frank Rosenblatt is credited with the development of the perceptron, documented in his research, â€œThe Perceptron: A Probabilistic Model for Information Storage and Organization in the Brainâ€ (link resides outside ibm.com). He takes McCulloch and Pittâ€™s work a step further by introducing weights to the equation. Leveraging an IBM 704, Rosenblatt was able to get a computer to learn how to distinguish cards marked on the left vs. cards marked on the right. 1974:While numerous researchers contributed to the idea of backpropagation, Paul Werbos was the first person in the US to note its application within neural networks within hisPhD thesis(link resides outside ibm.com). 1989:Yann LeCun published apaper(link resides outside ibm.com) illustrating how the use of constraints in backpropagation and its integration into the neural network architecture can be used to train algorithms. This research successfully leveraged a neural network to recognize hand-written zip code digits provided by the U.S. Postal Service. Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights. Learn how to confidently incorporate generative AI and machine learning into your business. Get an in-depth understanding of neural networks, their basic functions and the fundamentals of building one. IBMÂ® Graniteâ„¢ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options. We surveyed 2,000 organizations about their AI initiatives to discover what's working, what's not and how you can get ahead. Learn how to confidently incorporate generative AI and machine learning into your business. Learn how to select the most suitable AI foundation model for your use case. Learn how CEOs can balance the value generative AI can create against the investment it demands and the risks it introduces. Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions. Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data. Put AI to work in your business with IBM's industry-leading AI expertise and portfolio of solutions at your side. Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value. Get one-stop access to capabilities that span the AI development lifecycle. Produce powerful AI solutions with user-friendly interfaces, workflows and access to industry-standard APIs and SDKs."
https://www.geeksforgeeks.org/types-of-neural-networks/,"Artificial neural networks are a kind of machine learning algorithms that are created to reproduce the functions of the biological neural systems. Amongst which, networks like those which are a collection of interconnected nodes or neurons are the most prominent, which are organized into layers.In this article, we will discuss about the types of neural networks. Neural networks are computational models that mimic the way biological neural networks in the human brain process information. They consist of layers of neurons that transform the input data into meaningful outputs through a series of mathematical operations. Table of Content  The uses of neural networks are diverse and cut across many distinct industries and domains; processes and innovations are being transformed and even revolutionized by this advancement in technology. A neural network is a basic backbone of modern artificial intelligence that changes the way machines learn from data and carry out sophisticated tasks that were once considered to be human. Research technology is developing everyday, also computational resources are getting more readily available on daily basis. Consequently, neural networks are constantly evolving with innovation in mind, thus, transforming industries. The upcoming age will present an integration of smart systems into our daily living that will unveil a technology-oriented world, leading to many possibilities among healthcare, finance, entertainment, manufacturing, and transportation, and others. K "
https://aws.amazon.com/what-is/neural-network/,"A neural network is a method inartificial intelligence (AI)that teaches computers to process data in a way that is inspired by the human brain. It is a type ofmachine learning (ML)process, calleddeep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain. It creates an adaptive system that computers use to learn from their mistakes and improve continuously. Thus, artificial neural networks attempt to solve complicated problems, like summarizing documents or recognizing faces, with greater accuracy. Neural networks can help computers make intelligent decisions with limited human assistance. This is because they can learn and model the relationships between input and output data that are nonlinear and complex. For instance, they can do the following tasks. Neural networks can comprehend unstructured data and make general observations without explicit training. For instance, they can recognize that two different input sentences have a similar meaning: A neural network would know that both sentences mean the same thing. Or it would be able to broadly recognize that Baxter Road is a place, but Baxter Smith is a personâ€™s name. Neural networks have several use cases across many industries, such as the following: We give four of the important applications of neural networks below. Computer vision is the ability of computers to extract information and insights from images and videos. With neural networks, computers can distinguish and recognize images similar to humans. Computer vision has several applications, such as the following: Neural networks can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Virtual assistants like Amazon Alexa and automatic transcription software use speech recognition to do tasks like these: Natural language processing (NLP) is the ability to process natural, human-created text. Neural networks help computers gather insights and meaning from text data and documents. NLP has several use cases, including in these functions: Neural networks can track user activity to develop personalized recommendations. They can also analyze all user behavior and discover new products or services that interest a specific user. For example, Curalate, a Philadelphia-based startup, helps brands convert social media posts into sales. Brands use Curalateâ€™s intelligent product tagging (IPT) service to automate the collection and curation of user-generated social content. IPT uses neural networks to automatically find and recommend products relevant to the userâ€™s social media activity. Consumers don't have to hunt through online catalogs to find a specific product from a social media image. Instead, they can use Curalateâ€™s auto product tagging to purchase the product with ease. The human brain is the inspiration behind neural network architecture. Human brain cells, called neurons, form a complex, highly interconnected network and send electrical signals to each other to help humans process information. Similarly, an artificial neural network is made of artificial neurons that work together to solve a problem. Artificial neurons are software modules, called nodes, and artificial neural networks are software programs or algorithms that, at their core, use computing systems to solve mathematical calculations. A basic neural network has interconnected artificial neurons in three layers: Information from the outside world enters the artificial neural network from the input layer. Input nodes process the data, analyze or categorize it, and pass it on to the next layer. Hidden layers take their input from the input layer or other hidden layers. Artificial neural networks can have a large number of hidden layers. Each hidden layer analyzes the output from the previous layer, processes it further, and passes it on to the next layer. The output layer gives the final result of all the data processing by the artificial neural network. It can have single or multiple nodes. For instance, if we have a binary (yes/no) classification problem, the output layer will have one output node, which will give the result as 1 or 0. However, if we have a multi-class classification problem, the output layer might consist of more than one output node. Deep neural networks, or deep learning networks, have several hidden layers with millions of artificial neurons linked together. A number, called weight, represents the connections between one node and another. The weight is a positive number if one node excites another, or negative if one node suppresses the other. Nodes with higher weight values have more influence on the other nodes.Theoretically, deep neural networks can map any input type to any output type. However, they also need much more training as compared to other machine learning methods. They need millions of examples of training data rather than perhaps the hundreds or thousands that a simpler network might need. Artificial neural networks can be categorized by how the data flows from the input node to the output node. Below are some examples: Feedforward neural networks process data in one direction, from the input node to the output node. Every node in one layer is connected to every node in the next layer. A feedforward network uses a feedback process to improve predictions over time. Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms, you can think of the data flowing from the input node to the output node through many different paths in the neural network. Only one path is the correct one that maps the input node to the correct output node. To find this path, the neural network uses a feedback loop, which works as follows: The hidden layers in convolutional neural networks perform specific mathematical functions, like summarizing or filtering, called convolutions. They are very useful for image classification because they can extract relevant features from images that are useful for image recognition and classification. The new form is easier to process without losing features that are critical for making a good prediction. Each hidden layer extracts and processes different image features, like edges, color, and depth. Neural network training is the process of teaching a neural network to perform a task. Neural networks learn by initially processing several large sets of labeled or unlabeled data. By using these examples, they can then process unknown inputs more accurately. In supervised learning, data scientists give artificial neural networks labeled datasets that provide the right answer in advance. For example, a deep learning network training in facial recognition initially processes hundreds of thousands of images of human faces, with various terms related to ethnic origin, country, or emotion describing each image. The neural network slowly builds knowledge from these datasets, which provide the right answer in advance. After the network has been trained, it starts making guesses about the ethnic origin or emotion of a new image of a human face that it has never processed before. Artificial intelligence is the field of computer science that researches methods of giving machines the ability to perform tasks that require human intelligence. Machine learning is an artificial intelligence technique that gives computers access to very large datasets and teaches them to learn from this data. Machine learning software finds patterns in existing data and applies those patterns to new data to make intelligent decisions. Deep learning is a subset of machine learning that uses deep learning networks to process data. Traditional machine learning methods require human input for the machine learning software to work sufficiently well. A data scientist manually determines the set of relevant features that the software must analyze. This limits the softwareâ€™s ability, which makes it tedious to create and manage. On the other hand, in deep learning, the data scientist gives only raw data to the software. The deep learning network derives the features by itself and learns more independently. It can analyze unstructured datasets like text documents, identify which data attributes to prioritize, and solve more complex problems. For example, if you were training a machine learning software to identify an image of a pet correctly, you would need to take these steps: AWSdeep learningservicesÂ harness the power of cloud computing so that you can scale your deep learning neural networks at a lower cost and optimize them for speed. You can also use AWS services like these to fully manage specific deep learning applications: Get started with deep learning neural networks on AWS withAmazon SageMakerand quickly and easily build, train, anddeploy models at scale. You can also use theAWS Deep Learning AMIsto build custom environments and workflows for deep learning. Create afree AWS accountto get started today!"
https://www.geeksforgeeks.org/artificial-neural-networks-and-its-applications/,"As you read this article, which organ in your body is thinking about it? Itâ€™s the brain of course! But do you know how the brain works? Well, it has neurons or nerve cells that are the primary units of both the brain and the nervous system. These neurons receive sensory input from the outside world which they process and then provide the output which might act as the input to the next neuron. Each of these neurons is connected to other neurons in complex arrangements at synapses. Now, are you wondering how this is related toArtificial Neural Networks?  Letâ€™s check out what they are in detail and how they learn information. Well, Artificial Neural Networks are modeled after the neurons in the human brain. If you want to gain practical skills in Artificial Neural Networks and explore their diverse applications through ourinteractive live data science course, perfect for aspiring data scientists.  Artificial Neural Networks contain artificial neurons which are calledunits. These units are arranged in a series of layers that together constitute the whole Artificial Neural Network in a system. A layer can have only a dozen units or millions of units as this depends on how the complex neural networks will be required to learn the hidden patterns in the dataset. Commonly, Artificial Neural Network has an input layer, an output layer as well as hidden layers. The input layer receives data from the outside world which the neural network needs to analyze or learn about. Then this data passes through one or multiple hidden layers that transform the input into data that is valuable for the output layer. Finally, the output layer provides an output in the form of a response of the Artificial Neural Networks to input data provided. In the majority of neural networks, units are interconnected from one layer to another. Each of these connections has weights that determine the influence of one unit on another unit. As the data transfers from one unit to another, the neural network learns more and more about the data which eventually results in an output from the output layer. Neural Networks Architecture The structures and operations of human neurons serve as the basis for artificial neural networks. It is also known as neural networks or neural nets. The input layer of an artificial neural network is the first layer, and it receivesÂ input from external sources and releases it to the hidden layer, which is the second layer. In the hidden layer, each neuron receives input from the previous layer neurons, computes the weighted sum, and sends it to the neurons in the next layer. These connections are weighted means effects of the inputs from the previous layer are optimized more or less by assigning different-different weights to each input and it is adjusted during the training process by optimizing these weights for improved model performance. The concept of artificial neural networks comes from biological neurons found in animal brains So they share a lot of similarities in structure and function wise. Biological Neuron Artificial Neuron Dendrite Inputs Cell nucleus or Soma Nodes Synapses Weights Axon Output Biological Neuron Artificial Neuron Biological neurons to Artificial neurons Artificial neural networks are trained using a training set. For example, suppose you want to teach an ANN to recognize a cat. Then it is shown thousands of different images of cats so that the network can learn to identify a cat. Once the neural network has been trained enough using images of cats, then you need to check if it can identify cat images correctly. This is done by making the ANN classify the images it is provided by deciding whether they are cat images or not. The output obtained by the ANN is corroborated by a human-provided description of whether the image is a cat image or not. If the ANN identifies incorrectly thenback-propagationis used to adjust whatever it has learned during training.Backpropagationis done by fine-tuning the weights of the connections in ANN units based on the error rate obtained. This process continues until the artificial neural network can correctly recognize a cat in an image with minimal possible error rates. "
https://www.cloudflare.com/learning/ai/what-is-neural-network/,Error: 403 Client Error: Forbidden for url: https://www.cloudflare.com/learning/ai/what-is-neural-network/
https://www.techtarget.com/searchenterpriseai/definition/neural-network,"A neural network is a machine learning (ML) model designed to process data in a way that mimics the function and structure of the human brain. Neural networks are intricate networks of interconnected nodes, or artificial neurons, that collaborate to tackle complicated problems. Also referred to as artificial neural networks (ANNs), neural nets or deep neural networks, neural networks represent a type ofdeep learningtechnology that's classified under the broader field of artificial intelligence (AI). Neural networks are widely used in a variety of applications, includingimage recognition, predictive modeling, decision-making and natural language processing (NLP). Examples of significant commercial applications over the past 25 years include handwriting recognition for check processing, speech-to-text transcription, oil exploration data analysis, weather prediction andfacial recognition. An ANN usually involves manyprocessorsoperating in parallel and arranged in tiers or layers. There are typically three layers in a neural network: an input layer, an output layer and several hidden layers. The first tier -- analogous to optic nerves in human visual processing -- receives the raw input information. Each successive tier receives the output from the tier preceding it rather than the raw input, the same way biological neurons further from the optic nerve receive signals from those closer to it. The last tier produces the system's output. This article is part of Each processing node has its own small sphere of knowledge, including what it has seen and any rules it was originally programmed with or developed for itself. The tiers are highly interconnected, which means each node in TierNwill be connected to many nodes in TierN-1-- its inputs -- and in TierN+1, which provides input data for the TierN-1nodes. There could be one or more nodes in the output layer, from which the answer it produces can be read. ANNs are noted for beingadaptive, which means they modify themselves as they learn from initial training, and subsequent runs provide more information about the world. The most basic learning model is centered on weighting the input streams, which is how each node measures the importance of input data from each of its predecessors. Inputs that contribute to getting the right answers are weighted higher. Image recognition was one of the first areas in which neural networks were successfully applied. But the technologyuses of neural networkshave expanded to many additional areas, including the following: Prime uses involve any process that operates according to strict rules or patterns and has large amounts of data. If the data involved is too large for a human to make sense of in a reasonable amount of time, the process is likely a prime candidate for automation through artificial neural networks. Typically, an ANN is initially trained, or fed large amounts of data. Training consists of providing input and telling the network what the output should be. For example, to build a network that identifies the faces of actors, the initial training might be a series of pictures, including actors, non-actors, masks, statues and animal faces. Each input is accompanied by matching identification, such as actors' names or ""not actor"" or ""not human"" information. Providing the answers enables the model to adjust its internal weightings to do its job better. For example, if nodes David, Dianne and Dakota tell node Ernie that the current input image is a picture of Brad Pitt, but node Durango says it's George Clooney, and the training program confirms it's Pitt, Ernie decreases the weight it assigns to Durango's input and increases the weight it gives to David, Dianne and Dakota. In defining the rules and making determinations -- the decisions of each node on what to send to the next layer based on inputs from the previous tier -- neural networks use several principles. These include gradient-based training,fuzzy logic, genetic algorithms and Bayesian methods. They might be given some basic rules about object relationships in the data being modeled. For example, a facial recognition system might be instructed, ""Eyebrows are found above eyes,"" or ""Mustaches are below a nose. Mustaches are above and/or beside a mouth."" Preloading rules can make training faster and the model more powerful faster. But it also includes assumptions about the nature of the problem, which could prove to be either irrelevant and unhelpful, or incorrect and counterproductive, making the decision about what, if any, rules to build unimportant. Further, the assumptions people make when training algorithms cause neural networks to amplify cultural biases.Biased data sets are an ongoing challengein training systems that find answers on their own through pattern recognition in data. If the data feeding the algorithm isn't neutral -- and almost no data is -- the machine propagates bias. Neural networks are sometimes described in terms of their depth, including how many layers they have between input and output, or the model's so-called hidden layers. This is why the termneural networkis used almost synonymously withdeep learning. Neural networks can also be described by the number of hidden nodes the model has, or in terms of how many input layers and output layers each node has. Variations on the classic neural network design enable various forms of forward and backward propagation of information among tiers. Specific types of ANNs include the following: One of the simplest variants of neural networks, these pass information in one direction, through various input nodes, until it makes it to the output node. The network might or might not have hidden node layers, making their functioning more interpretable. It's prepared to process large amounts of noise. This type of ANN computational model is used in technologies such as facial recognition andcomputer vision. More complex in nature, recurrent neural networks (RNNs) save the output of processing nodes and feed the result back into the model. This is how the model learns to predict the outcome of a layer. Each node in the RNN model acts as a memory cell, continuing the computation and execution of operations. This neural network starts with the same front propagation as a feed-forward network, but then goes on to remember all processed information to reuse it in the future. If the network's prediction is incorrect, then the system self-learns and continues working toward the correct prediction duringbackpropagation. This type of ANN is frequently used in text-to-speech conversions. Convolutional neural networks (CNNs) are one of the most popular models used today. This computational model uses a variation of multilayerperceptronsand contains one or more convolutional layers that can be either entirely connected or pooled. These convolutional layers create feature maps that record a region of the image that's ultimately broken into rectangles and sent out for nonlinear processing. The CNN model is particularly popular in the realm of image recognition. It has been used in many of the most advanced applications of AI, including facial recognition, text digitization and NLP. Other use cases include paraphrase detection, signal processing andimage classification. Deconvolutional neural networksuse a reversed CNN learning process. They try to find lost features or signals that might have originally been considered unimportant to the CNN system's task. This network model can be used in image synthesis and analysis. These contain multiple neural networks working separately from one another. The networks don't communicate or interfere with each other's activities during the computation process. Consequently, complex or big computational processes can be performed more efficiently. These represent the most basic form of neural networks and were introduced in 1958 by Frank Rosenblatt, an American psychologist who's also considered to be the father of deep learning. The perceptron is specifically designed for binary classification tasks, enabling it to differentiate between two classes based on input data. Multilayer perceptron (MLP) networks consist of multiple layers of neurons, including an input layer, one or more hidden layers, and an output layer. Each layer is fully connected to the next, meaning that every neuron in one layer is connected to every neuron in the subsequent layer. This architecture enables MLPs to learn complex patterns and relationships in data, making them suitable for various classification andregression tasks. Radial basis function networks use radial basis functions as activation functions. They're typically used for function approximation, time series prediction and control systems. Transformer neural networks are reshaping NLPand other fields through a range of advancements. Introduced by Google in a 2017 paper, transformers are specifically designed to process sequential data, such as text, by effectively capturing relationships and dependencies between elements in the sequence, regardless of their distance from one another. Transformer neural networks have gained popularity as an alternative to CNNs and RNNs because their ""attention mechanism"" enables them to capture and process multiple elements in a sequence simultaneously, which is a distinct advantage over other neural network architectures. Generative adversarial networksconsist of two neural networks -- a generator and a discriminator -- that compete against each other. The generator creates fake data, while the discriminator evaluates its authenticity. These types of neural networks are widely used for generating realistic images and data augmentation processes. Artificial neural networks offer the following benefits: Along with their numerous benefits, neural networks also have some drawbacks, including the following: Thehistory of neural networksspans several decades and has seen considerable advancements. The following examines the important milestones and developments in the history of neural networks: Discover the process for building a machine learning model, including data collection, preparation, training, evaluation and iteration. Follow theseessential steps to kick-start your ML project. To help you find the right BI software for your analytics needs, here's a look at 20 top tools and the key features that ... Data preparation is a crucial but complex part of analytics applications. Don't let seven common challenges send your data prep ... A year after getting acquired by private equity firms amid declining revenue growth and a slow transition to the cloud, the ... President-elect Donald Trump has been vocal in his criticisms of big tech's content censorship power and President Joe Biden's ... Internal tech procurement can be a risky undertaking. Tech pilot programs can potentially reduce some of that risk, but IT ... Apart from President-elect Donald Trump's promise to take a strong stance on goods imported from China, collaboration might be ... Based on its AI development and expansion within data management, the record $10 billion the vendor raised shows it is viewed ... The data lakehouse pioneer has expanded into AI development and plans to use the funding to fuel further investments in AI, make ... The complementary partnership combines the data integration vendor's discovery and retrieval capabilities with the tech giant's ... The supply chain might be losing its seat at the executive table, but the work must continue to provide more resilience and ... Nothing is certain except death, taxes and supply chain disruptions. Companies should plan accordingly, especially as the ... Essential TMS features are real-time tracking, route optimization, carrier management, load planning, fleet management, automated... All Rights Reserved,Copyright 2018 - 2024, TechTargetPrivacy PolicyCookie PreferencesCookie PreferencesDo Not Sell or Share My Personal Information"
https://www.datacamp.com/blog/what-are-neural-networks,Error: 403 Client Error: Forbidden for url: https://www.datacamp.com/blog/what-are-neural-networks
https://mlu-explain.github.io/neural-networks/,
http://playground.tensorflow.org/,"Which dataset do you want to use?    Which properties do you want to feed in? Itâ€™s a technique for building a computer program that learns from data. It is based very loosely on how we think the human brain works. First, a collection of software â€œneuronsâ€ are created and connected together, allowing them to send messages to each other. Next, the network is asked to solve a problem, which it attempts to do over and over, each time strengthening the connections that lead to success and diminishing those that lead to failure. For a more detailed introduction to neural networks, Michael Nielsenâ€™sNeural Networks and Deep Learningis a good place to start. For a more technical overview, tryDeep Learningby Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Please do! Weâ€™ve open sourced it onGitHubwith the hope that it can make neural networks a little more accessible and easier to learn. Youâ€™re free to use it in any way that follows ourApache License. And if you have any suggestions for additions or changes, pleaselet us know. Weâ€™ve also provided some controls below to enable you tailor the playground to a specific topic or lesson. Just choose which features youâ€™d like to be visible below then savethis link, orrefreshthe page. Orange and blue are used throughout the visualization in slightly different ways, but in general orange shows negative values while blue shows positive values. The data points (represented by small circles) are initially colored orange or blue, which correspond to positive one and negative one. In the hidden layers, the lines are colored by the weights of the connections between neurons. Blue shows a positive weight, which means the network is using that output of the neuron as given. An orange line shows that the network is assiging a negative weight. In the output layer, the dots are colored orange or blue depending on their original values. The background color shows what the network is predicting for a particular area. The intensity of the color shows how confident that prediction is. We wrote a tiny neural networklibrarythat meets the demands of this educational visualization. For real-world applications, consider theTensorFlowlibrary. This was created by Daniel Smilkov and Shan Carter.
        This is a continuation of many peopleâ€™s previous work â€” most notably Andrej Karpathyâ€™sconvnet.js demoand Chris Olahâ€™sarticlesabout neural networks.
        Many thanks also to D. Sculley for help with the original idea and to Fernanda ViÃ©gas and Martin Wattenberg and the rest of theBig PictureandGoogle Brainteams for feedback and guidance."
https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414,"Suggestions or feedback? Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under aCreative Commons Attribution Non-Commercial No Derivatives license.
    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided 
    below, credit the images to ""MIT."" Previous imageNext image In the past 10 years, the best-performing artificial-intelligence systems â€” such as the speech recognizers on smartphones or Googleâ€™s latest automatic translator â€” have resulted from a technique called â€œdeep learning.â€ Deep learning is in fact a new name for an approach to artificial intelligence called neural networks, which have been going in and out of fashion for more than 70 years. Neural networks were first proposed in 1944 by Warren McCullough and Walter Pitts, two University of Chicago researchers who moved to MIT in 1952 as founding members of whatâ€™ssometimes calledthe first cognitive science department. Neural nets were a major area of research in both neuroscience and computer science until 1969, when, according to computer science lore, they were killed off by the MIT mathematicians Marvin Minsky and Seymour Papert, who a year later would become co-directors of the new MIT Artificial Intelligence Laboratory. The technique then enjoyed a resurgence in the 1980s, fell into eclipse again in the first decade of the new century, and has returned like gangbusters in the second, fueled largely by the increased processing power of graphics chips. â€œThereâ€™s this idea that ideas in science are a bit like epidemics of viruses,â€ says Tomaso Poggio, the Eugene McDermott Professor of Brain and Cognitive Sciences at MIT, an investigator at MITâ€™s McGovern Institute for Brain Research, and director of MITâ€™sCenter for Brains, Minds, and Machines. â€œThere are apparently five or six basic strains of flu viruses, and apparently each one comes back with a period of around 25 years. People get infected, and they develop an immune response, and so they donâ€™t get infected for the next 25 years. And then there is a new generation that is ready to be infected by the same strain of virus. In science, people fall in love with an idea, get excited about it, hammer it to death, and then get immunized â€” they get tired of it. So ideas should have the same kind of periodicity!â€ Weighty matters Neural nets are a means of doing machine learning, in which a computer learns to perform some task by analyzing training examples. Usually, the examples have been hand-labeled in advance. An object recognition system, for instance, might be fed thousands of labeled images of cars, houses, coffee cups, and so on, and it would find visual patterns in the images that consistently correlate with particular labels. Modeled loosely on the human brain, a neural net consists of thousands or even millions of simple processing nodes that are densely interconnected. Most of todayâ€™s neural nets are organized into layers of nodes, and theyâ€™re â€œfeed-forward,â€ meaning that data moves through them in only one direction. An individual node might be connected to several nodes in the layer beneath it, from which it receives data, and several nodes in the layer above it, to which it sends data. To each of its incoming connections, a node will assign a number known as a â€œweight.â€ When the network is active, the node receives a different data item â€” a different number â€” over each of its connections and multiplies it by the associated weight. It then adds the resulting products together, yielding a single number. If that number is below a threshold value, the node passes no data to the next layer. If the number exceeds the threshold value, the node â€œfires,â€ which in todayâ€™s neural nets generally means sending the number â€” the sum of the weighted inputs â€” along all its outgoing connections. When a neural net is being trained, all of its weights and thresholds are initially set to random values. Training data is fed to the bottom layer â€” the input layer â€” and it passes through the succeeding layers, getting multiplied and added together in complex ways, until it finally arrives, radically transformed, at the output layer. During training, the weights and thresholds are continually adjusted until training data with the same labels consistently yield similar outputs. Minds and machines The neural nets described by McCullough and Pitts in 1944 had thresholds and weights, but they werenâ€™t arranged into layers, and the researchers didnâ€™t specify any training mechanism. What McCullough and Pitts showed was that a neural net could, in principle, compute any function that a digital computer could. The result was more neuroscience than computer science: The point was to suggest that the human brain could be thought of as a computing device. Neural nets continue to be a valuable tool for neuroscientific research. For instance, particularnetwork layoutsorrulesfor adjusting weights and thresholds have reproduced observed features of human neuroanatomy and cognition, an indication that they capture something about how the brain processes information. The first trainable neural network, the Perceptron, was demonstrated by the Cornell University psychologist Frank Rosenblatt in 1957. The Perceptronâ€™s design was much like that of the modern neural net, except that it had only one layer with adjustable weights and thresholds, sandwiched between input and output layers. Perceptrons were an active area of research in both psychology and the fledgling discipline of computer science until 1959, when Minsky and Papert published a book titled â€œPerceptrons,â€ which demonstrated that executing certain fairly common computations on Perceptrons would be impractically time consuming. â€œOf course, all of these limitations kind of disappear if you take machinery that is a little more complicated â€” like, two layers,â€ Poggio says. But at the time, the book had a chilling effect on neural-net research. â€œYou have to put these things in historical context,â€ Poggio says. â€œThey were arguing for programming â€” for languages like Lisp. Not many years before, people were still using analog computers. It was not clear at all at the time that programming was the way to go. I think they went a little bit overboard, but as usual, itâ€™s not black and white. If you think of this as this competition between analog computing and digital computing, they fought for what at the time was the right thing.â€ Periodicity By the 1980s, however, researchers had developed algorithms for modifying neural netsâ€™ weights and thresholds that were efficient enough for networks with more than one layer, removing many of the limitations identified by Minsky and Papert. The field enjoyed a renaissance. But intellectually, thereâ€™s something unsatisfying about neural nets. Enough training may revise a networkâ€™s settings to the point that it can usefully classify data, but what do those settings mean? What image features is an object recognizer looking at, and how does it piece them together into the distinctive visual signatures of cars, houses, and coffee cups? Looking at the weights of individual connections wonâ€™t answer that question. In recent years, computer scientists have begun to come up withingeniousmethods fordeducingthe analytic strategies adopted by neural nets. But in the 1980s, the networksâ€™ strategies were indecipherable. So around the turn of the century, neural networks were supplanted by support vector machines, an alternative approach to machine learning thatâ€™s based on some very clean and elegant mathematics. The recent resurgence in neural networks â€” the deep-learning revolution â€” comes courtesy of the computer-game industry. The complex imagery and rapid pace of todayâ€™s video games require hardware that can keep up, and the result has been the graphics processing unit (GPU), which packs thousands of relatively simple processing cores on a single chip. It didnâ€™t take long for researchers to realize that the architecture of a GPU is remarkably like that of a neural net. Modern GPUs enabled the one-layer networks of the 1960s and the two- to three-layer networks of the 1980s to blossom into the 10-, 15-, even 50-layer networks of today. Thatâ€™s what the â€œdeepâ€ in â€œdeep learningâ€ refers to â€” the depth of the networkâ€™s layers. And currently, deep learning is responsible for the best-performing systems in almost every area of artificial-intelligence research. Under the hood The networksâ€™ opacity is still unsettling to theorists, but thereâ€™s headway on that front, too. In addition to directing the Center for Brains, Minds, and Machines (CBMM), Poggio leads the centerâ€™s research program inTheoretical Frameworks for Intelligence. Recently, Poggio and his CBMM colleagues have released a three-part theoretical study of neural networks. Thefirst part, which was published last month in theInternational Journal of Automation and Computing, addresses the range of computations that deep-learning networks can execute and when deep networks offer advantages over shallower ones. Partstwoandthree, which have been released as CBMM technical reports, address the problems of global optimization, or guaranteeing that a network has found the settings that best accord with its training data, and overfitting, or cases in which the network becomes so attuned to the specifics of its training data that it fails to generalize to other instances of the same categories. There are still plenty of theoretical questions to be answered, but CBMM researchersâ€™ work could help ensure that neural networks finally break the generational cycle that has brought them in and out of favor for seven decades. Previous itemNext item Read full storyâ†’ Read full storyâ†’ Read full storyâ†’ Read full storyâ†’ Read full storyâ†’ Read full storyâ†’ This website is managed by the MIT News Office, part of theInstitute Office of Communications. Massachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA"
https://www.xueshu.com/sci/41613/,"Gold OAæ–‡ç« å æ¯”ï¼š19.75% OAè¢«å¼•ç”¨å æ¯”ï¼š0.0623... å¼€æºå æ¯”ï¼š0.1207 ç ”ç©¶ç±»æ–‡ç« å æ¯”ï¼š98.15% å›½é™…æ ‡å‡†ç®€ç§°ï¼šNEURAL NETWORKS äººæ°”1703 ã€ŠNeural Networksã€‹æ˜¯ä¸€æœ¬ä¸“æ³¨äºŽCOMPUTER SCIENCE, ARTIFICIAL INTELLIGENCEé¢†åŸŸçš„Englishå­¦æœ¯æœŸåˆŠï¼Œåˆ›åˆŠäºŽ1988å¹´ï¼Œç”±Elsevier Ltdå‡ºç‰ˆå•†å‡ºç‰ˆï¼Œå‡ºç‰ˆå‘¨æœŸMonthlyã€‚è¯¥åˆŠå‘æ–‡èŒƒå›´æ¶µç›–COMPUTER SCIENCE, ARTIFICIAL INTELLIGENCEç­‰é¢†åŸŸï¼Œæ—¨åœ¨åŠæ—¶ã€å‡†ç¡®ã€å…¨é¢åœ°æŠ¥é“å›½å†…å¤–COMPUTER SCIENCE, ARTIFICIAL INTELLIGENCEå·¥ä½œè€…åœ¨è¯¥é¢†åŸŸçš„ç§‘å­¦ç ”ç©¶ç­‰å·¥ä½œä¸­å–å¾—çš„ç»éªŒã€ç§‘ç ”æˆæžœã€æŠ€æœ¯é©æ–°ã€å­¦æœ¯åŠ¨æ€ç­‰ã€‚è¯¥åˆŠå·²è¢«SCIEæ•°æ®åº“æ”¶å½•ï¼Œåœ¨ä¸­ç§‘é™¢JCRæœ€æ–°å‡çº§ç‰ˆåˆ†åŒºè¡¨ä¸­ï¼Œè¯¥åˆŠåˆ†åŒºä¿¡æ¯ä¸ºå¤§ç±»å­¦ç§‘è®¡ç®—æœºç§‘å­¦1åŒºï¼Œ2023å¹´å½±å“å› å­ä¸º6ã€‚ 1åŒº Q1 SCIE å¦ Neural Networks is the archival journal of the world's three oldest neural modeling societies: the International Neural Network Society (INNS), the European Neural Network Society (ENNS), and the Japanese Neural Network Society (JNNS). A subscription to the journal is included with membership in each of these societies. Neural Networks provides a forum for developing and nurturing an international community of scholars and practitioners who are interested in all aspects of neural networks and related approaches to computational intelligence. Neural Networks welcomes high quality submissions that contribute to the full range of neural networks research, from behavioral and brain modeling, learning algorithms, through mathematical and computational analyses, to engineering and technological applications of systems that significantly use neural network concepts and techniques. This uniquely broad range facilitates the cross-fertilization of ideas between biological and technological studies, and helps to foster the development of the interdisciplinary community that is interested in biologically-inspired computational intelligence. Accordingly, Neural Networks editorial board represents experts in fields including psychology, neurobiology, computer science, engineering, mathematics, and physics. The journal publishes articles, letters and reviews, as well as letters to the editor, editorials, current events, software surveys, and patent information. Articles are published in one of five sections: Cognitive Science, Neuroscience, Learning Systems, Mathematical and Computational Analysis, Engineering and Applications. ä¸­ç§‘é™¢åˆ†åŒºï¼šä¸­ç§‘é™¢åˆ†åŒºæ˜¯SCIæœŸåˆŠåˆ†åŒºçš„ä¸€ç§ï¼Œæ˜¯ç”±ä¸­å›½ç§‘å­¦é™¢å›½å®¶ç§‘å­¦å›¾ä¹¦é¦†åˆ¶å®šå‡ºæ¥çš„åˆ†åŒºã€‚ä¸»è¦æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œå³åŸºç¡€ç‰ˆå’Œå‡çº§ç‰ˆã€‚2019å¹´ä¸­å›½ç§‘å­¦é™¢æ–‡çŒ®æƒ…æŠ¥ä¸­å¿ƒæœŸåˆŠåˆ†åŒºè¡¨æŽ¨å‡ºäº†å‡çº§ç‰ˆï¼Œå®žçŽ°äº†åŸºç¡€ç‰ˆå’Œå‡çº§ç‰ˆçš„å¹¶å­˜è¿‡æ¸¡ï¼›å‡çº§ç‰ˆæ˜¯å¯¹åŸºç¡€ç‰ˆçš„å»¶ç»­å’Œæ”¹è¿›ï¼Œå°†æœŸåˆŠç”±åŸºç¡€ç‰ˆçš„13ä¸ªå­¦ç§‘æ‰©å±•è‡³18ä¸ªï¼Œç§‘ç ”è¯„ä»·å°†æ›´åŠ æ˜Žç¡®ã€‚ JCRåˆ†åŒºï¼šJCRï¼ˆJournal Citation Reportsï¼‰ç”±ç§‘ç¿å”¯å®‰å…¬å¸ï¼ˆå‰èº«ä¸ºæ±¤æ£®è·¯é€ï¼‰å¼€å‘ã€‚JCRæ²¡æœ‰è®¾ç½®å¤§ç±»,åªå°†æœŸåˆŠåˆ†ä¸º176ä¸ªå…·ä½“å­¦ç§‘ï¼Œä¹Ÿå°±æ˜¯ä¸­ç§‘é™¢åˆ†åŒºä¸­çš„å°ç±»å­¦ç§‘ã€‚åŸºäºŽä¸åŒå­¦ç§‘çš„å½“å¹´å½±å“å› å­é«˜ä½Žè¿›è¡ŒæŽ’åºï¼Œå°†æœŸåˆŠçš„æ•°é‡å‡åŒ€åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼ŒQ1åŒºä»£è¡¨å­¦ç§‘åˆ†ç±»ä¸­å½±å“å› å­æŽ’åå‰25%çš„æœŸåˆŠï¼Œä»¥æ­¤ç±»æŽ¨ï¼ŒQ2åŒºä¸ºå‰25%-50%æœŸåˆŠï¼ŒQ3åŒºä¸ºå‰50%-75%æœŸåˆŠï¼ŒQ4åŒºä¸º75%ä»¥åŽæœŸåˆŠã€‚ CiteScoreæŽ’åï¼š CiteScoreå€¼è®¡ç®—æ–¹å¼ï¼šä¾‹å¦‚2024å…¬å¸ƒçš„CiteScoreæ˜¯å°†ç»Ÿè®¡åœ¨ 2020å¹´-2023å¹´é—´å¹´æ‰€å‘è¡¨æ–‡ç« çš„å¼•ç”¨æ¬¡æ•°é™¤ä»¥åœ¨ 2020å¹´-2023å¹´é—´æ‰€å‘è¡¨çš„å‘æ–‡æ€»æ•°ã€‚ CiteScoreæ•°æ®æ¥æºï¼šæ˜¯ç”±å…¨çƒè‘—åå­¦æœ¯å‡ºç‰ˆå•†Elsevierï¼ˆçˆ±æ€å”¯å°”ï¼‰åŸºäºŽå…¶Scopusæ•°æ®åº“æŽ¨å‡ºçš„æœŸåˆŠè¯„ä»·æŒ‡æ ‡ã€‚CiteScoreæŒ‡æ•°ä»¥å››å¹´åŒºé—´ä¸ºåŸºå‡†æ¥è®¡ç®—æ¯æœ¬æœŸåˆŠçš„å¹³å‡è¢«å¼•ç”¨æ¬¡æ•°ï¼Œå¹¶æä¾›æœŸåˆŠé¢†åŸŸæŽ’åã€æœŸåˆŠåˆ†åŒºçš„ç›¸å…³ä¿¡æ¯ï¼Œå®ƒçš„ä½œç”¨æ˜¯æµ‹é‡æœŸåˆŠçš„ç¯‡å‡å½±å“åŠ›ã€‚ è¿‘å¹´ä¸­ç§‘é™¢åˆ†åŒºè¶‹åŠ¿å›¾ è¿‘å¹´IFå€¼(å½±å“å› å­)è¶‹åŠ¿å›¾ å½±å“å› å­ï¼šæ˜¯ç¾Žå›½ç§‘å­¦ä¿¡æ¯ç ”ç©¶æ‰€(ISI)çš„æœŸåˆŠå¼•è¯æŠ¥å‘Š(JCR)ä¸­çš„ä¸€é¡¹æ•°æ®ã€‚æŒ‡çš„æ˜¯æŸä¸€æœŸåˆŠçš„æ–‡ç« åœ¨ç‰¹å®šå¹´ä»½æˆ–æ—¶æœŸè¢«å¼•ç”¨çš„é¢‘çŽ‡ï¼Œæ˜¯è¡¡é‡å­¦æœ¯æœŸåˆŠå½±å“åŠ›çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡ã€‚è‡ª1975å¹´ä»¥æ¥ï¼Œæ¯å¹´å®šæœŸå‘å¸ƒäºŽâ€œæœŸåˆŠå¼•è¯æŠ¥å‘Šâ€(JCR)ã€‚ 1ã€å»ºè®®ç¨¿ä»¶æŽ§åˆ¶10é¡µä»¥ä¸Šï¼Œæ–‡ç« æ’°å†™è¯­è¨€ä¸ºè‹±è¯­ï¼›ï¼ˆå•æ æ ¼å¼ï¼Œå•å€è¡Œè·ï¼Œå†…å®¹10å·å­—ä½“ï¼Œæ–‡ç¨¿ç±»åž‹åŒ…å«ï¼šåŽŸåˆ›ç ”ç©¶(Original Research)ã€æ¡ˆä¾‹æŠ¥å‘Š(Case Report)ã€æ–‡çŒ®ç»¼è¿°(Literature Review)ç­‰ï¼›æ–‡ä»¶æ ¼å¼åŒ…å«wordã€PDFã€LaTeXç­‰ã€‚ 2ã€ç¨¿ä»¶é‡å¤çŽ‡æŽ§åˆ¶10%ä»¥å†…ï¼Œè®ºæ–‡åŠ¡å¿…ä¿è¯åŽŸåˆ›æ€§ã€å›¾æ ‡ã€å…¬å¼ã€å¼•æ–‡ç­‰è¦ç´ é½å¤‡ï¼Œä¿è¯é™„å±žèµ„æ–™çš„å®Œæ•´ã€‚å·²å‘è¡¨æˆ–å¼•ç”¨è¿‡åº¦çš„æ–‡ç« å°†ä¸ä¼šè¢«å‡ºç‰ˆå’Œæ£€ç´¢ï¼Œç¦æ­¢ä¸€ç¨¿å¤šæŠ•ï¼Œæ‹’ç»æŠ„è¢­ã€æœºæ¢°æ€§çš„ç¨¿ä»¶ã€‚ 3ã€ç¨¿ä»¶å¿…é¡»æœ‰è¾ƒå¥½çš„è‹±è¯­è¡¨è¾¾æ°´å¹³ï¼Œæœ‰å›¾ï¼Œæœ‰è¡¨ï¼Œæœ‰å…¬å¼ï¼Œæœ‰æ•°æ®æˆ–è®¾è®¡ï¼Œæœ‰ç®—æ³•ï¼ˆæ–¹æ¡ˆï¼Œæ¨¡åž‹ï¼‰ï¼Œå®žéªŒï¼Œä»¿çœŸç­‰ï¼›å‚è€ƒæ–‡çŒ®æŽ§åˆ¶25æ¡ä»¥ä¸Šï¼Œå‚è€ƒæ–‡çŒ®å¼•ç”¨ä¸€åŠä»¥ä¸ŠæŽ§åˆ¶åœ¨è¿‘5å¹´ä»¥å†…ã€‚ 1ã€å»ºè®®ä½¿ç”¨TIFFã€EPSã€JPEGæ ¼å¼ ï¼ŒTIFFæ ¼å¼ ä½¿ç”¨LZWåŽ‹ç¼©ã€‚ 2ã€æ–‡ä»¶å¤§å°æœ€å¤§ä¸è¶…è¿‡20MBï¼Œä¸è¦ä»¥å•ä¸ªæ–‡ä»¶çš„å½¢å¼ä¸Šä¼ æ•°æ®ã€‚ 3ã€å½©è‰²å›¾ç‰‡çš„åˆ†è¾¨çŽ‡â‰¥300dpiï¼›é»‘ç™½å›¾ç‰‡çš„åˆ†è¾¨çŽ‡åœ¨â‰¥500dpiï¼›line artå›¾ç‰‡ç±»åž‹çš„åˆ†è¾¨çŽ‡â‰¥1000dpiï¼›è‰²å½©æ¨¡å¼å»ºè®®é‡‡ç”¨RGBï¼Œé™¤éžæœŸåˆŠæ³¨æ˜Žè¦CMYKã€‚ 4ã€çº¿æ¡ä¸è¦ç»†äºŽ0.25pt,ä¹Ÿä¸èƒ½å¤ªç²—ï¼Œè¶…è¿‡1.5pt,è¿‡ç»†æˆ–è¿‡ç²—éƒ½å½±å“ç¾Žè§‚ã€‚ 5ã€è¡¨æ ¼ä¸€èˆ¬å’Œmanuscrriptæ”¾ç½®åœ¨ä¸€ä¸ªwordæ–‡æ¡£é‡Œéƒ¨åˆ†æœŸåˆŠ éœ€è¦å•ç‹¬ä¸Šä¼ è¡¨æ ¼ã€‚ 1ã€åŒ…æ‹¬ä½œè€…å§“åã€æœ€é«˜å­¦ä½ï¼Œä½œè€…å•ä½ï¼ˆç²¾ç¡®åˆ°éƒ¨é—¨ï¼‰ï¼Œé‚®ç®±ï¼Œåœ°å€ï¼Œé‚®ç¼–ï¼Œå…³é”®è¯ï¼Œå†…å®¹ï¼Œæ€»ç»“ï¼Œé¡¹ç›®åŸºé‡‘ï¼Œå‚è€ƒæ–‡çŒ®ï¼Œä½œè€…ç›¸ç‰‡+ç®€ä»‹ï¼ˆä¸€å®šè¦ç¡®ä¿ä½œè€…ä¿¡æ¯å‡†ç¡®æ— è¯¯ï¼Œæäº¤ç¨¿ä»¶ä¹‹åŽè¿™éƒ¨åˆ†ä¸èƒ½å†ä½œæ”¹åŠ¨ï¼‰ã€‚ æ›´å¤šå¾ç¨¿ç»†åˆ™è¯·æŸ¥é˜…æ‚å¿—ç¤¾å¾ç¨¿è¦æ±‚ã€‚æœ¬ç«™ä¸“æ³¨æœŸåˆŠæŠ•ç¨¿æœåŠ¡åå¹´ï¼Œç¡®ä¿SCIæ£€ç´¢ï¼Œç¨¿ä»¶ä¿¡æ¯å®‰å…¨ä¿å¯†ï¼Œåˆä¹Žå­¦æœ¯è§„èŒƒä¸æˆåŠŸä¸æ”¶è´¹ï¼Œè¯¦æƒ…è¯·å’¨è¯¢å®¢æœã€‚ PERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD, ENGLAND, OX5 1GB è‹¥ç”¨æˆ·éœ€è¦å‡ºç‰ˆæœåŠ¡ï¼Œè¯·è”ç³»å‡ºç‰ˆå•†ï¼šPERGAMON-ELSEVIER SCIENCE LTD, THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD, ENGLAND, OX5 1GBã€‚ ä¸­ç§‘é™¢åˆ†åŒº1åŒº ä¸­ç§‘é™¢åˆ†åŒº1åŒº ä¸­ç§‘é™¢åˆ†åŒº1åŒº ä¸­ç§‘é™¢åˆ†åŒº1åŒº ä¸­ç§‘é™¢åˆ†åŒº1åŒº ä¸­ç§‘é™¢åˆ†åŒº1åŒº ä¸­ç§‘é™¢åˆ†åŒº1åŒº ä¸­ç§‘é™¢åˆ†åŒº1åŒº"
https://blog.csdn.net/weixin_39910711/article/details/100775918,
https://brilliant.org/wiki/artificial-neural-network/,"Reset passwordNew user?Sign up Existing user?Log in Already have an account?Log in here. A simple artificial neural network.  The first column of circles represents the ANN's inputs, the middle column represents computational units that act on that input, and the third column represents the ANN's output.  Lines connecting circles indicate dependencies.[1] Artificial neural networks(ANNs) are computational models inspired by the human brain.  They are comprised of a large number of connectednodes, each of which performs a simplemathematical operation.  Each node's output is determined by this operation, as well as a set of parameters that are specific to that node.  By connecting these nodes together and carefully setting their parameters, very complex functions can be learned and calculated. Artificial neural networks are responsible for many of the recent advances inartificial intelligence, including voice recognition, image recognition, androbotics.  For example, ANNs can perform image recognition on hand drawn digits.  An interactive example can be foundhere. With the advent of computers in the 1940s, computer scientists' attention turned towards developing intelligent systems that could learn to performprediction and decision making.  Of particular interest werealgorithmsthat could performonline learning, which is a learning method that can be applied to data points arriving sequentially.  This is in opposition tobatch learning, which requires that all of the data be present at the time of training. Online learning is especially useful in scenarios where training data is arriving sequentially over time, such as speech data or the movement of stock prices.  With a system capable of online learning, one doesn't have to wait until the system has received a ton of data before it can make a prediction or decision. If the human brain learned by batch learning, then human children would take 10 years before they could learn to speak, mostly just to gather enough speech data and grammatical rules to speak correctly. Instead, children learn to speak by observing the speech patterns of those around them and gradually incorporating that knowledge to improve their own speech, an example of online learning. Given that the brain is such a powerful online learner, it is natural to try to emulate it mathematically.  ANNs are one attempt at a model with the bare minimum level of complexity required to approximate the function of the human brain, and so are among the most powerful machine learning methods discovered thus far. The human brain is primarily comprised ofneurons, small cells that learn to fire electrical and chemical signals based on some function.  There are on the order of \(10^{11}\) neurons in the human brain, about \(15\) times the total number of people in the world.  Each neuron is, on average, connected to \(10000\) other neurons, so that there are a total of \(10^{15}\) connections between neurons. Neurons and microglial cells stained red and green respectively.[2] Since individual neurons aren't capable of very complicated calculations, it is thought that the huge number of neurons and connections are what gives the brain its computational power.  While there are in fact thousands of different types of neurons in the human brain, ANNs usually attempt to replicate only one type in an effort to simplify the model calculation and analysis. The electrical current for a neuron going from rest to firing to rest again.[3] Neurons function by firing when they receive enough input from the other neurons to which they're connected.  Typically, the output function is modeled as anactivation function, where inputs below a certain threshold don't cause the neuron to fire, and those above the threshold do.  Thus, a neuron exhibits what is known asall-or-nothingfiring, meaning it is either firing, or it is completely off and no output is produced. From the point of view of a particular neuron, its connections can generally be split into two classes, incoming connections and outgoing connections.  Incoming connections form the input to the neuron, while the output of the neuron flows through the outgoing connections.  Thus, neurons whose incoming connections are the outgoing connections of other neurons treat other neurons' outputs as inputs.  The repeated transformation of outputs of some neurons into inputs of other neurons gives rise to the power of the human brain, since thecompositionof activation functions can create highly complex functions. It turns out that incoming connections for a particular neuron are not considered equal.  Specifically, some incoming connections are stronger than others, and provide more input to a neuron than weak connections.  Since a neuron fires when it receives input above a certain threshold, these strong incoming connections contribute more to neural firing.  Neurons actually learn to make some connections stronger than others, in a process calledlong-term potentiation, allowing them to learn when to fire in response to the activities of neurons they're connected to.   Neurons can also make connections weaker through an analogous process calledlong-term depression. As discussed in the above sections, as well as the later section titledThe Universal Approximation Theorem, a good computational model of the brain will have three characteristics: Biologically-InspiredThe brain's computational power is derived from its neurons and the connections between them.  Thus, a good computational approximation of the brain will have individual computational units (a la neurons), as well as ways for those neurons to communicate (a la connections).  Specifically, the outputs of some computational units will be the inputs to other computational units.  Furthermore, each computational unit should calculate some function akin to the activation function of real neurons. FlexibleThe brain is flexible enough to learn seemingly endless types and forms of data.  For example, even though most teenagers under the age of 16 have never driven a car before, most learn very quickly to drive upon receiving their driver's license.  No person's brain is preprogrammed to learn how to drive, and yet almost anyone can do it given a small amount of training.  The brain's ability to learn to solve new tasks that it has no prior experience with is part of what makes it so powerful.  Thus, a good computational approximation of the brain should be able to learn many different types of functions without knowing the forms those functions will take beforehand. Capable of Online LearningThe brain doesn't need to learn everything at once, so neither should a good model of it.  Thus, a good computational approximation of the brain should be able to improve by online learning, meaning it gradually improves over time as it learns to correct past errors. By the first desideratum, the model will consist of many computational units connected in some way.  Each computational unit will perform a simple computation whose output will be passed as input to other units.  This process will repeat itself some number of times, so that outputs from some computational units are the inputs to others.  With any luck, connecting enough of these units together will give sufficient complexity to compute any function, satisfying the second desideratum.  However, what kind of function the model ends up computing will depend on the data it is exposed to, as well as alearning algorithmthat determines how the model learns that data.  Ideally, this algorithm will be able to perform online learning, the third desideratum. Thus, building a good computational approximation to the brain consists of three steps.  The first is to develop a computational model of the neuron and to connect those models together to replicate the way the brain performs computations.  This is covered in the sections titledA Computational Model of the Neuron,The Sigmoid Function, andPutting It All Together.  The second is to prove that this model is sufficiently complex to calculate any function and learn any type of data it is given, which is covered in the section titledThe Universal Approximation Theorem.  The third is to develop a learning algorithm that can learn to calculate a function, given a model and some data, in an online manner.  This is covered in the section titledTraining The Model. The step function.[4] As stated above, neurons fire above a certain threshold and do nothing below that threshold, so a model of the neuron requires a function exhibiting the same properties.  The simplest function that does this is thestep function. The step function is defined as:\(H(x) = \begin{cases}
  1 & \mbox{if } x \ge 0, \\
  0 & \mbox{if } x \lt 0. \\
\end{cases}\) In this simple neuron model, the input is a single number that must exceed the activation threshold in order to trigger firing.  However, neurons can (and should, if they're to do anything useful) have connections to multiple incoming neurons, so we need some way of ""integrating"" these incoming neuron's inputs into a single number.  The most common way of doing this is to take a weighted sum of the neuron's incoming inputs, so that the neuron fires when the weighted sum exceeds the threshold.  If the vector of outputs from the incoming neurons is represented by \(\vec{x}\), then the weighted sum of \(\vec{x}\) is thedot product\(\vec{w} \cdot \vec{x}\), where \(\vec{w}\) is called theweight vector. To further improve the modeling capacity of the neuron, we want to be able to set the threshold arbitrarily.  This can be achieved by adding ascalar(which may be positive or negative) to the weighted sum of the inputs.  Adding a scalar of \(-b\) will force the neuron's activation threshold to be set to \(b\), since the new step function \(H(x+(-b))\) at \(x = b\) equals \(0\), which is the threshold of the step function.  The value \(b\) is known as thebiassince it biases the step function away from the natural threshold at \(x = 0\). Thus, calculating the output of our neuron model is comprised of two steps:1) Calculate theintegration.  The integration, as defined above, is the sum \(\vec{w} \cdot \vec{x} + b\) for vectors \(\vec{w}\), \(\vec{x}\) and scalar \(b\). 2) Calculate theoutput.  The output is the activation function applied to the result of step 1.  Since the activation function in our model is the 
step function, the output of the neuron is \(H(\vec{w} \cdot \vec{x} + b)\), which is \(1\) when \(\vec{w} \cdot \vec{x} + b >= 0\) and \(0\) otherwise. A linear classifier, where squares evaluate to 1 and circles to 0.[5] Following from the description of step 2, our neuron model defines alinear classfier, i.e. a function that splits the inputs into two regions with a linear boundary.  In two dimensions, this is a line, while in higher dimensions the boundary is known as ahyperplane.  The weight vector \(\vec{w}\) defines the slope of the linear boundary while the bias \(b\) defines the intercept of the linear boundary.  The following diagram illustrates a neuron's output for two incoming connections (i.e. a two dimensional input vector \(\vec{x}\).  Note that the neuron inputs are clearly separated into values of \(0\) and \(1\) by a line (defined by \(\vec{w} \cdot \vec{x} + b = 0\)). By adjusting the values of \(\vec{w}\) and \(b\), the step function unit can adjust its linear boundary and learn to split its inputs into classes, \(0\) and \(1\), as shown in the previous image.  As a corollary, different values of \(\vec{w}\) and \(b\) for multiple step function units will yield multiple different linear classifiers.  Part of what makes ANNs so powerful is their ability to adjust \(\vec{w}\) and \(b\) for many units at the same time, effectively learning many linear classifiers simultaneously.  This learning is discussed in more depth in the section titledTraining the Model. Since the brain can calculate more than just linear functions by connecting many neurons together, this suggests that connecting many linear classifiers together should produce a nonlinear function.  In fact, it is proven that for certain activation functions and a very large number of neurons, ANNs can model any continuous, smooth function arbitrarily well, a result known as theuniversal approximation theorem. This is very convenient because, like the brain, an ANN should ideally be able to learn any function handed to it.  If ANNs could only learn one type of function (e.g. third degreepolynomials), this would severely limit the types of problems to which they could be applied.  Furthermore, learning often happens in an environment where the type of function to be learned is not known beforehand, so it is advantageous to have a model that does not depend on knowing a priori the form of the data it will be exposed to. Unfortunately, since the step function can only output two different values, \(0\) and \(1\), an ANN of step function neurons cannot be a universal approximator (generally speaking, continuous functions take on more than two values).   Luckily, there is a continuous function called the sigmoid function, described in the next section, that is very similar to the step function and can be used in universal approximators. The sigmoid function.[6] There is a continuous approximation of the step function called the logistic curve, orsigmoid function, denoted as \(\sigma(x)\).  This function's output ranges over all values between \(0\) and \(1\) and makes a transition from values near \(0\) to values near \(1\) at \(x = 0\), similar to the step function \(H(x)\). The sigmoid function is defined as:\(\sigma(x) = \frac{1}{1 + e^{-x}}\) So, for a computational unit that uses the sigmoid function, instead of firing \(0\) or \(1\) like a step function unit, it's output will be between \(0\) and \(1\), non-inclusive.  This changes slightly the interpretation of this unit as a model of a neuron, since it no longer exhibits all-or-nothing behavior since it will never take on the value of \(0\) (nothing) or \(1\) (all).  However, the sigmoid function is very close to \(0\) for \(x \lt 0\) and very close to \(1\) for \(x \gt 0\), so it can be interpreted as exhibiting practically all-or-nothing behavior on most (\(x \not\approx 0\)) inputs. The output for a  sigmoidal unit with weight vector \(\vec{w}\) and bias \(b\) on input \(\vec{x}\) is:\(\sigma(\vec{w} \cdot \vec{x} + b) = \left(1+\exp\left(-(\vec{w} \cdot \vec{x} + b)\right)\right)^{-1}\) Thus, a sigmoid unit is like a linear classifier with a boundary defined at \(\vec{w} \cdot \vec{x} + b = 0\).  The value of the sigmoid function at the boundary is \(\sigma(0) = .5\).  Inputs \(\vec{x}\) that are far from the linear boundary will be approximately \(0\) or \(1\), while those very close to the boundary will be closer to \(.5\). The sigmoid function turns out to be a member of the class of activation functions for universal approximators, so it imitates the behavior of real neurons (by approximating the step function) while also permitting the possibility of arbitrary function approximation.  These happen to be exactly the first twodesiderataspecified for a good mathematical model of the brain.  In fact, some ANNs use activation functions that are different from the sigmoidal function, because those functions are also proven to be in the class of functions for which universal approximators can be built.  Two well-known activation functions used in the same manner as the sigmoidal function are thehyperbolic tangentand therectifier.  The proof that these functions can be used to build ANN universal approximators is fairly advanced, so it is not covered here. Calculate the output of a sigmoidal neuron with weight vector \(\vec{w} = (.25, .75)\) and bias \(b = -.75\) for the following two inputs vectors: \(\vec{m} = (1, 2)\)\(\vec{n} = (1, -.5)\) Recalling that the output of a sigmoidal neuron with input \(\vec{x}\) is \(\sigma(\vec{w} \cdot \vec{x} + b)\), \(\begin{align*} 
d &= \vec{w} \cdot \vec{m} + b \\
&= w_1 \cdot m_1 + w_2 \cdot m_2 + b \\
&= .25 \cdot 1 + .75 \cdot 2 -.75 \\
&= 1
\end{align*}\) \(\begin{align*} 
s &= \sigma(d) \\
&=  \frac{1}{1 + e^{-d}} \\
&= \frac{1}{1+e^{-1}} \\
&= .73105857863
\end{align*}\) Thus, the output on \(\vec{m} = (1, 2)\) is \(.73105857863\).  The same reasoning applied to \(\vec{n} = (1, -.5)\)  yields \(.29421497216\).  Like the step function unit describe above, the sigmoid function unit's linear boundary can be adjusted by changing the values of \(\vec{w}\) and \(b\).  The weight vector defines the slope of the linear boundary while the bias defines the intercept of the linear boundary.  Since, like the brain, the final model will include many individual computational units (a la neurons), a learning algorithm that can learn, ortrain, many \(\vec{w}\) and \(b\) values simultaneously is required.  This algorithm is described in the section titledTraining the Model. Neurons are connected to one another, with each neuron's incoming connections made up of the outgoing connections of other neurons.  Thus, the ANN will need to connect the outputs of sigmoidal units to the inputs of other sigmoidal units. The diagram below shows a sigmoidal unit with three inputs \(\vec{x} = (x_1, x_2, x_3)\), one output \(y\), bias \(b\), and weight vector \(\vec{w} = (w_1, w_2, w_3)\).  Each of the inputs \(\vec{x} = (x_1, x_2, x_3)\) can be the output of another sigmoidal unit (though it could also be raw input, analogous to unprocessed sense data in the brain, such as sound), and the unit's output \(y\) can be the input to other sigmoidal units (though it could also be a final output, analogous to an action associated neuron in the brain, such as one that bends your left elbow).  Notice that each component \(w_i\) of the weight vector corresponds to each component \(x_i\) of the input vector.  Thus, the summation of the product of the individual \(w_i, x_i\) pairs is equivalent to the dot product, as discussed in the previous sections. A sigmoidal unit with three inputs \(\vec{x} = (x_1, x_2, x_3)\), weight vector \(\vec{w}\), and bias \(b\).[7] Artificial neural networks are most easily visualized in terms of adirected graph.  In the case of sigmoidal units,node\(s\) represents sigmoidal unit \(s\) (as in the diagram above) anddirected edge\(e = (u, v)\) indicates that one of sigmoidal unit \(v\)'s inputs is the output of sigmoidal unit \(u\). Thus, if the diagram above represents sigmoidal unit \(s\) and inputs \(x_1\), \(x_2\), and \(x_3\) are the outputs of sigmoidal units \(a\), \(b\), and \(c\), respectively, then a graph representation of the above sigmoidal unit will have nodes \(a\), \(b\), \(c\), and \(s\) with directed edges \((a, s)\), \((b, s)\), and \((c, s)\).  Furthermore, since each incoming directed edge is associated with a component of the weight vector for sigmoidal unit \(s\), each incoming edge will be labeled with its corresponding weight component.  Thus edge \((a, s)\) will have label \(w_1\), \((b, s)\) will have label \(w_2\), and \((c, s)\) will have label \(w_3\).  The corresponding graph is shown below, with the edges feeding into nodes \(a\), \(b\), and \(c\) representing inputs to those nodes. Directed graph representing ANN with sigmoidal units \(a\), \(b\), \(c\), and \(s\).  Unit \(s\)'s weight vector \(\vec{w}\) is \((w_1, w_2, w_3)\) While the above ANN is very simple, ANNs in general can have many more nodes (e.g. modern machine vision applications use ANNs with more than \(10^6\) nodes) in very complicated connection patterns (see the wiki aboutconvolutional neural networks). The outputs of sigmoidal units are the inputs of other sigmoidal units, indicated by directed edges, so computation follow the edges in the graph representation of the ANN.  Thus, in the example above, computation of \(s\)'s output is preceded by the computation of \(a\), \(b\), and \(c\)'s outputs.  If the graph above was modified so that's \(s\)'s output was an input of \(a\), a directed edge passing from \(s\) to \(a\) would be added, creating what is known as acycle.  This would mean that \(s\)'s output is dependent on itself.  Cyclic computation graphs greatly complicate computation and learning, so computation graphs are commonly restricted to bedirected acyclic graphs(or DAGs), which have no cycles.  ANNs with DAG computation graphs are known asfeedforward neural networks, while ANNs with cycles are known asrecurrent neural networks. Ultimately, ANNs are used to compute and learn functions.  This consists of giving the ANN a series of input-output pairs \(\vec{(x_i}, \vec{y_i})\), and training the model to approximate the function \(f\) such that \(f(\vec{x_i}) = \vec{y_i}\) for all pairs.  Thus, if \(\vec{x}\) is \(n\)-dimensional and \(\vec{y}\) is \(m\)-dimensional, the final sigmoidal ANN graph will consist of \(n\) input nodes (i.e. raw input, not coming from other sigmoidal units) representing \(\vec{x} = (x_1, \dots, x_n)\), \(k\) sigmoidal units (some of which will be connected to the input nodes), and \(m\) output nodes (i.e. final output, not fed into other sigmoidal units) representing \(\vec{y} = (y_1, \dots, y_m)\). Like sigmoidal units, output nodes have multiple incoming connections and output one value.  This necessitates an integration scheme and an activation function, as defined in the section titledThe Step Function.  Sometimes, output nodes use the same integration and activation as sigmoidal units, while other times they may use more complicated functions, such as thesoftmax function, which is heavily used in classification problems.  Often, the choice of integration and activation functions is dependent on the form of the output.  For example, since sigmoidal units can only output values in the range \((0, 1)\), they are ill-suited to problems where the expected value of \(y\) lies outside that range. An example graph for an ANN computing a two dimensional output \(\vec{y}\) on a three dimensional input \(\vec{x}\) using five sigmoidal units \(s_1, \dots, s_5\) is shown below.  An edge labeled with weight \(w_{ab}\) represents the component of the weight vector for node \(b\) that corresponds to the input coming from node \(a\).  Note that this graph, because it has no cycles, is a feedforward neural network. ANN for three dimensional input, two dimensional output, and five sigmoidal units Thus, the above ANN would start by computing the outputs of nodes \(s_1\) and \(s_2\) given \(x_1\), \(x_2\), and \(x_3\).  Once that was complete, the ANN would next compute the outputs of nodes \(s_3\), \(s_4\), and \(s_5\), dependent on the outputs of \(s_1\) and \(s_2\).  Once that was complete, the ANN would do the final calculation of nodes \(y_1\) and \(y_2\), dependent on the outputs of nodes \(s_3\), \(s_4\), and \(s_5\). It is obvious from this computational flow that certain sets of nodes tend to be computed at the same time, since a different set of nodes uses their outputs as inputs.  For example, set \(\{s_3, s_4, s_5\}\) depends on set \(\{s_1, s_2\}\).  These sets of nodes that are computed together are known aslayers, and ANNs are generally thought of a series of such layers, with each layer \(l_i\) dependent on previous layer \(l_{i-1}\)  Thus, the above graph is composed of four layers.  The first layer \(l_0\) is called theinput layer(which does not need to be computed, since it is given), while the final layer \(l_3\) is called theoutput layer.  The intermediate layers are known ashidden layers, which in this case are the layers \(l_1 = \{s_1, s_2\}\) and \(l_2 = \{s_3, s_4, s_5\}\), are usually numbered so that hidden layer \(h_i\) corresponds to layer \(l_i\).  Thus, hidden layer \(h_1=\{s_1, s_2\}\) and hidden layer \(h_2=\{s_3, s_4, s_5\}\).  The diagram below shows the example ANN with each node grouped into its appropriate layer. The same ANN grouped into layers ANN LayersThe image source:Artificial Neural Network The ANN can now calculate some function \(f_{\theta}(\vec{x})\) that depends on the values of the individual nodes' weight vectors and biases, which together are known as the ANN'sparameters\(\theta\).  The logical next step is to determine how to alter those biases and weight vectors so that the ANN computes known values of the function.  That is, given a series of input-output pairs \((\vec{x_i}, \vec{y_i})\), how can the weight vectors and biases be altered such that \(f_{\theta}(\vec{x_i}) \approx \vec{y_i}\) for all \(i\)? The typical way to do this is define an error function \(E\) over the set of pairs \(X = \{(\vec{x_1}, \vec{y_1}), \dots, (\vec{x_N},\vec{y_N})\}\) such that \(E(X, \theta)\) is small when \(f_{\theta}(\vec{x_i}) \approx \vec{y_i}\) for all \(i\).  Common choices for \(E\) are themean squared error(MSE) in the case ofregressionproblems and thecross entropyin the case ofclassificationproblems.  Thus, training the ANN reduces to minimizing the error \(E(X, \theta)\) with respect to the parameters (since \(X\) is fixed).  For example, for the mean square error function, given two input-output pairs \(X= \{(\vec{x_1}, \vec{y_1}), (\vec{x_2}, \vec{y_2})\}\) and an ANN with parameters \(\theta\) that outputs \( f_{\theta}(\vec{x})\) for input \(\vec{x}\), the error function \(E(X, \theta)\) is \[E(X, \theta)=\frac{(y_1 - f_{\theta}(\vec{x_1}))^2}{2} + \frac{(y_2 - f_{\theta}(\vec{x_2}))^2}{2}\] Since the error function \(E(X, \theta)\) defines a fairly complex function (it is a function of the output of the ANN, which is a composition of many nonlinear functions), finding the minimum analytically is generally impossible.  Luckily, there exists a general method for minimizingdifferentiable functionscalledgradient descent.  Basically, gradient descent finds thegradientof a function \(f\) at a particular value \(x\) (for an ANN, that value will be the parameters \(\theta\)) and then updates that value by moving (or stepping) in the direction of the negative of the gradient.  Generally speaking (it depends on the size of the step \(\eta\)), this will find a nearby value \(x^{\prime} = x - \eta \nabla f(x)\) for which \(f(x^{\prime}) \lt f(x)\).  This process repeats until alocal minimumis found, or the gradient sufficiently converges (i.e. becomes smaller than some threshold).  Learning for an ANN typically starts with a random initialization of the parameters (the weight vectors and biases) followed by successive updates to those parameters based on gradient descent until the error function \(E(X, \theta)\) converges. A major advantage of gradient descent is that it can be used foronline learning, since the parameters are not solved in one calculation but are instead gradually improved by moving in the direction of the negative gradient.  Thus, if input-output pairs are arriving in a sequential fashion, the ANN can perform gradient descent on one input-output pair for a certain number of steps, and then do the same once the next input-output pair arrives.  For an appropriate choice of step size \(\eta\), this approach can yield results similar to gradient descent on the entire dataset \(X\) (known asbatch learning). Because gradient descent is a local method (the step direction is determined by the gradient at a single point), it can only find local minima.  While this is generally a significant problem for most optimization applications, recent research has suggested that finding local minima is not actually an issue for ANNs, since the vast majority of local minima are evenly distributed and similar in magnitude for large ANNs. For a long time, calculating the gradient for ANNs was thought to be mathematically intractable, since ANNs can have large numbers of nodes and very many layers, making the error function \(E(X, \theta)\) highly nonlinear.  However, in the mid-1980s, computer scientists were able to derive a method for calculating the gradient with respect to an ANN's parameters, known asbackpropagation, or ""backpropagation by errors"".  The method works for bothfeedforward neural networks(for which it was originally designed) as well as forrecurrent neural networks, in which case it is calledbackpropagation through time, or BPTT.  The discovery of this method brought about a renaissance in artificial neural network research, as training non-trivial ANNs had finally become feasible. , D.Neuralnetwork.
    Retrieved
    June 4, 2005,
    fromhttps://commons.wikimedia.org/wiki/File:Neuralnetwork.png, G.Microglia_and_neurons.
    Retrieved
    July 25, 2005,
    fromhttps://commons.wikimedia.org/wiki/File:Microglia_and_neurons.jpg, B.Current_Clamp_recording_of_Neuron.
    Retrieved
    October 6, 2006,
    fromhttps://commons.wikimedia.org/wiki/File:Current_Clamp_recording_of_Neuron.GIF, L.Heaviside.
    Retrieved
    August 25, 2007,
    fromhttps://commons.wikimedia.org/wiki/File:Heaviside.svg, M.Linearna_separovatelnost_v_prikladovom_priestore.
    Retrieved
    December 13, 2013,
    fromhttps://commons.wikimedia.org/wiki/File:Linearna_separovatelnost_v_prikladovom_priestore.png, Q.Logistic-curve.
    Retrieved
    July 2, 2008,
    fromhttps://commons.wikimedia.org/wiki/File:Logistic-curve.svg, C.ArtificialNeuronModel.
    Retrieved
    July 14, 2005,
    fromhttps://commons.wikimedia.org/wiki/File:ArtificialNeuronModel.png Reset passwordNew user?Sign up Existing user?Log in Problem Loading... Note Loading... Set Loading..."
https://towardsdatascience.com/simple-introduction-to-neural-networks-ac1d7c3d7a2c,"Sign up Sign in Sign up Sign in Matthew Stewart, PhD Follow Towards Data Science -- 10 Listen Share â€œYour brain does not manufacture thoughts. Your thoughts shape neural networks.â€â€” Deepak Chopra This article is the first in a series of articles aimed at demystifying the theory behind neural networks and how to design and implement them. The article was designed to be a detailed and comprehensive introduction to neural networks that is accessible to a wide range of individuals: people who have little to no understanding of how a neural network works as well as those who are relatively well-versed in their uses, but perhaps not experts. In this article, I will cover the motivation and basics of neural networks. Future articles will go into more detailed topics about the design and optimization of neural networks and deep learning. These tutorials are largely based on the notes and examples from multiple classes taught at Harvard and Stanford in the computer science and data science departments. I hope you enjoy the article and learn something regardless of your prior understanding of neural networks. Letâ€™s begin! Untrained neural network models are much like new-born babies: They are created ignorant of the world (if consideringtabula rasaepistemological theory), and it is only through exposure to the world, i.e.a posterioriknowledge, that their ignorance is slowly revised. Algorithms experience the world through data â€” by training a neural network on a relevant dataset, we seek to decrease its ignorance. The way we measure progress is by monitoring the error produced by the network. Before delving into the world of neural networks, it is important to get an understanding of the motivation behind these networks and why they work. To do this, we have to talk a bit about logistic regression. Methods that are centered around modeling and prediction of aquantitativeresponse variable (e.g. number of taxi pickups, number of bike rentals) are called regressions (and Ridge, LASSO, etc.). When the response variable iscategorical, then the problem is no longer called a regression problem but is instead labeled as a classification problem. Let us consider a binary classification problem. The goal is to attempt to classify each observation into a category (such as a class or cluster) defined byY, based on a set of predictor variablesX. Letâ€™s say that we would like to predict whether a patient has heart disease based on features about the patient. The response variable here is categorical, there are finite outcomes, or more explicitly, binary since there are only two categories (yes/no). There is a lot of features here â€” for now, we will only use the MaxHR variable. To make this prediction, we would use a method known as logistic regression. Logistic regression addresses the problem of estimating a probability that someone has heart disease,P(y=1), given an input valueX. The logistic regression model uses a function, called thelogisticfunction, to modelP(y=1): As a result, the model will predictP(y=1)with anS-shaped curve, which is the general shape of the logistic function. Î²â‚€shifts the curve right or left byc = âˆ’ Î²â‚€ / Î²â‚,whereasÎ²â‚controls the steepness of theS-shaped curve. Note that ifÎ²â‚is positive, then the predictedP(y=1)goes from zero for small values ofXto one for large values ofXand ifÎ²â‚is negative, then it has the opposite association. This is summarized graphically below. Now that we understand how to manipulate our logistic regression curve, we can play with some of the variables in order to get the sort of curve that we want. We can change theÎ²â‚€value in order to move our offset. We can change theÎ²â‚value in order to distort our gradient. Doing this by hand is pretty tedious and it is unlikely you will converge to the optimal value. To solve this problem we use a loss function in order to quantify the level of error that belongs to our current parameters. We then find the coefficients that minimize this loss function. For this binary classification, we can use a binary loss function to optimize our logistic regression. So the parameters of the neural network have a relationship with the error the net produces, and when the parameters change, the error does, too. We change the parameters using an optimization algorithm calledgradient descent, which is useful for finding the minimum of a function. We are seeking to minimize the error, which is also known as theloss functionor theobjective function. So what is the point of what we just did? How does this relate to neural networks? Actually, what we just did is essentially the same procedure that is performed by neural network algorithms. We only used one feature for our previous model. Instead, we can take multiple features and illustrate these in a network format. We have weights for each of the features and we also have a bias term, which together makes up our regression parameters. Depending on whether the problem is a classification or regression problem, the formulation will be slightly different. When we talk about weights in neural networks, it is these regression parameters of our various incoming functions that we are discussing. This is then passed to an activation function which decides whether the result is significant enough to â€˜fireâ€™ the node. I will discuss different activation functions in more detail later in the next article. So now we have developed a very simple network that consists of multiple logistic regression with four features. We need to start with some arbitrary formulation of values in order for us to start updating and optimizing the parameters, which we will do by assessing the loss function after each update and performing gradient descent. The first thing we do is set randomly selected weights. Most likely it will perform horribly â€” in our heart data, the model will give us the wrong answer. We then â€˜trainâ€™ the network by essentially punishing it for performing poorly. However, merely telling the computer it is performing good or bad is not particularly helpful. You need to tell it how to change those weights in order for the performance of the model to improve. We already know how to tell the computer it is performing well, we just need to consult our loss function. Now, the procedure is more complicated because we have 5 weights to deal with. I will only consider one weight but the procedure is analogous for all the weights. Ideally, we want to know the value ofwthat gives the minimumâ„’ (w). To find the optimal point of a functionâ„’ (w),we can differentiate with respect to the weight and then set this equal to zero. We then need to find thewthat satisfies that equation. Sometimes there is no explicit solution for that. A more flexible method is to start from any point and then determine which direction to go to reduce the loss (left or right in this case). Specifically, we can calculate the slope of the function at this point. We then shift to the right if the slope is negative or shift to the left if the slope is positive. This procedure is then repeated until convergence. If the step is proportional to the slope then you avoid overshooting the minimum. How do we perform this update? This is done using a method known as gradient descent, which was briefly mentioned earlier. Gradient descent is an iterative method for finding the minimum of a function. There are various flavors of gradient descent, and I will discuss these in detail in the subsequent article.This blog postpresents the different methods available to update the weights. For now, we will stick with the vanilla gradient descent algorithm, sometimes known as thedelta rule. We know that we want to go in the opposite direction of the derivative (since we are trying to â€˜go awayâ€™ from the error) and we know we want to be making a step proportional to the derivative. This step is controlled by a parameter Î» known as the learning rate. Our new weight is the addition of the old weight and the new step, whereby the step was derived from the loss function and how important our relevant parameter is in influencing the learning rate (hence the derivative). A large learning rate means more weight is put on the derivative, such that large steps can be made for each iteration of the algorithm. A smaller learning rate means that less weight is put on the derivative, so smaller steps can be made for each iteration. If the step size is too small, the algorithm will take a long time to converge, and if the step size is too large, the algorithm will continually miss the optimal parameter choice. Clearly, selecting the learning rate can be an important parameter when setting up a neural network. There are various considerations to make for gradient descent: Deriving the derivatives is nowadays done using automatic differentiation, so this is of little concern to us. However, deciding the learning rate is an important and complicated problem, which I will discuss later in the set of tutorials. Local minimum can be very problematic for neural networks since the formulation of neural networks gives no guarantee that we will attain the global minimum. Getting stuck in a local minimum means we have a locally good optimization of our parameters, but there is a better optimization somewhere on our loss surface. Neural network loss surfaces can have many of these local optima, which is problematic for network optimization. See, for example, the loss surface illustrated below. How might we solve this problem? One suggestion is the use of batch and stochastic gradient descent. This idea sounds complicated, but the idea is simple â€” to use a batch (a subset) of data as opposed to the whole set of data, such that the loss surface is partially morphed during each iteration. For each iterationk, the following loss (likelihood) function can be used to derive the derivatives: which is an approximation to the full loss function. We can illustrate this with an example. First, we start off with the full loss (likelihood) surface, and our randomly assigned network weights provide us an initial value. We then select a batch of data, perhaps 10% of the full dataset, and construct a new loss surface. We then perform gradient descent on this batch and perform our update. We are now in a new location. We select a new random subset of the full data set and again construct our loss surface. We then perform gradient descent on this batch and perform our update. We continue this procedure again with a new subset. And perform our update. This procedure continues for multiple iterations. Until the network begins to converge to the global minimum. We now have sufficient knowledge in our tool kit to go about building our first neural network. Now that we understand how logistic regression works, how we can assess the performance of our network, and how we can update the network to improve our performance, we can go about building a neural network. First, I want us to understand why neural networks are called neural networks. You have probably heard that it is because they mimic the structure of neurons, the cells present in the brain. The structure of a neuron looks a lot more complicated than a neural network, but the functioning is similar. The way an actual neuron works involves the accumulation of electric potential, which when exceeding a particular value causes the pre-synaptic neuron to discharge across the axon and stimulate the post-synaptic neuron. Humans have billions of neurons which are interconnected and can produce incredibly complex firing patterns. The capabilities of the human brain are incredible compared to what we can do even with state-of-the-art neural networks. Due to this, we will likely not see neural networks mimicking the function of the human brain anytime soon. We can draw a neural diagram that makes the analogy between the neuron structure and the artificial neurons in a neural network. Given the capabilities of the human brain, it should be apparent that the capabilities of artificial neural networks are fairly limitless in scope â€” especially once we begin to link these to sensors, actuators, as well as the wealth of the internet â€” which explains their ubiquity in the world despite the fact we are in the relatively nascent phases of their development. After all, a reductionist could argue that humans are merely an aggregation of neural networks connected to sensors and actuators through the various parts of the nervous system. Now letâ€™s imagine that we have multiple features. Each of the features is passed through something called an affine transformation, which is basically an addition (or subtraction) and/or multiplication. This gives us something resembling a regression equation. The affine transformation becomes important when we have multiple nodes converging at a node in a multilayer perceptron. We then pass this result through our activation function, which gives us some form of probability. This probability determines whether the neuron will fire â€” our result can then be plugged into our loss function in order to assess the performance of the algorithm. From now, I will abstract the affine and activation blocks into a single block. However, be clear that the affine transformation is the amalgamation of the outputs from upstream nodes and the summed output is then passed to an activation function, which assesses the probability to determine whether itâ€™s the quantiative value (the probability) sufficient to make the neuron fire. We can now go back to our first example with our heart disease data. We can take two logistic regressions and merge them together. The individual logistic regressions look like the below case: When we connect these two networks, we obtain a network with increased flexibility due to the increased number of degrees of freedom. This illustrates the power of neural networks quite well, we are able to string together (sum) multiple functions such that with a large number of functions â€” which come from a large number of neurons â€” we are able to produce highly non-linear functions. With a large enough set of neurons, continuous functions of arbitrary complexity can be produced. This is a very simple example of a neural network, however, we see that we already run into a problem even with such a simple network. How are we supposed to update the value of our weights? We need to be able to calculate the derivatives of the loss function with respect to these weights. In order to learn the missing weights, wâ‚, wâ‚‚, and wâ‚ƒ, we need to utilize something known as backpropagation. Backpropagation is the central mechanism by which neural networks learn. It is the messenger telling the network whether or not the network made a mistake during prediction. The discovery of backpropagation is one of the most important milestones in the whole of neural network research. To propagateis to transmit something (e.g. light, sound) in a particular direction or through a particular medium. When we discuss backpropagation in the context of neural networks, we are talking about the transmission of information, and that information relates to the error produced by the neural network when they make a guess about data. During prediction, a neural network propagates signal forward through the nodes of the network until it reaches the output layer where a decision is made. The network then backpropagates information about this error backward through the network such that it can alter each of the parameters. Backpropagation is the way in which we calculate the derivatives for each of the parameters in the network, which is necessary in order to perform gradient descent. This is an important distinction to make as it can be easy to mix up backpropagation and gradient descent. Backpropagation is performed first in order to gain the information necessary to perform gradient descent. You might have noticed that we still need to calculate the derivatives. Computers cannot differentiate, but a function library can be built in order to do this without the network designer needing to get involved, it abstracts the process for us. This is known as automatic differentiation. Below is an example of this. We could do it by hand like this, and then change it for every network architecture and for each node. Or we can write a function library that is inherently linked to the architecture such that the procedure is abstracted and updates automatically as the network architecture is updated. If you really want to understand how useful this abstracted automatic differentiation process is, try making a multilayer neural network with half a dozen nodes and writing the code to implement backpropagation (if anyone has the patience and grit to do this, kudos to you). Having a network with two nodes is not particularly useful for most applications. Typically, we use neural networks to approximate complex functions that cannot be easily described by traditional methods. Neural networks are special as they follow something called theuniversal approximation theorem. This theorem states that, given an infinite amount of neurons in a neural network, an arbitrarily complex continuous function can be represented exactly. This is quite a profound statement, as it means that, given enough computational power, we can approximate essentially any function. Obviously, in practice, there are several issues with this idea. Firstly, we are limited by the data we have available to us, which limits our potential accuracy in predicting categories or estimating values. Secondly, we are limited by our computational power. It is fairly easy to design a network that far exceeds the capabilities of even the most powerful supercomputers in the world. The trick is to design a network architecture such that we are able to achieve high accuracy using relatively little computational power, with minimal data. What is even more impressive is that one hidden layer is enough to represent an approximation of any function to an arbitrary degree of accuracy. So why do people use multilayer neural networks if one layer is enough? The answer is simple. This network would need to have a neural architecture that is very wide since shallow networks require (exponentially) more width than a deep network. Furthermore, shallow networks have a higher affinity for overfitting. This is the stimulus behind why the field of deep learning exists (deep referring to the multiple layers of a neural network) and dominates contemporary research literature in machine learning and most fields involving data classification and prediction. This article discussed the motivation and background surrounding neural networks and outlined how they can be trained. We talked about loss functions, error propagation, activation functions, and network architectures. The diagram below provides a great summary of all of the concepts discussed and how they are interconnected. The knowledge from this article will provide us with a strong basis from which we can build upon in future articles discussing how to improve the performance of neural networks and use them for deep learning applications. For updates on new blog posts and extra content, sign up for my newsletter. mailchi.mp J. Nocedal y S. Wright, â€œNumerical optimizationâ€, Springer, 1999 TLDR: J. Bullinaria, â€œLearning with Momentum, Conjugate Gradient Learningâ€, 2015 -- -- 10 Your home for data science and AI. The worldâ€™s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals. CTO @Harbor.ai | Privacy Consultant @DandelionHealth | Data Science PhD + postdoc @Harvard | Blogger @TDS | Content Creator @EdX.https://mpstewart.io Help Status About Careers Press Blog Privacy Terms Text to speech Teams"
https://www.ibm.com/es-es/topics/neural-networks,"Inicio Topics neural networks Una red neuronal es un programa, o modelo, demachine learningque toma decisiones de forma similar al cerebro humano, utilizando procesos que imitan la forma en que las neuronas biolÃ³gicas trabajan juntas para identificar fenÃ³menos, sopesar opciones y llegar a conclusiones. Toda red neuronal consta de capas de nodos o neuronas artificiales: una capa de entrada, una o varias capas ocultas y una capa de salida. Cada nodo se conecta a los demÃ¡s y tiene su propia ponderaciÃ³n y umbral asociados. Si la salida de cualquier nodo individual estÃ¡ por encima del valor umbral especificado, ese nodo se activa y envÃ­a datos a la siguiente capa de la red. De lo contrario, no se pasa ningÃºn dato a la siguiente capa de la red. Las redes neuronales se basan en datos de entrenamiento para aprender y mejorar su precisiÃ³n con el tiempo. Una perfeccionadas, se convierten en potentes herramientas en informÃ¡tica einteligencia artificial, que nos permiten clasificar y agrupar datos a gran velocidad. Las tareas de reconocimiento de voz o de imÃ¡genes pueden llevar minutos frente a horas si se comparan con la identificaciÃ³n manual por parte de expertos humanos. Uno de los ejemplos mÃ¡s conocidos de red neuronal es el algoritmo de bÃºsqueda de Google. Las redes neuronales a veces se denominan redes neuronales artificiales (ANN) o redes neuronales simuladas (SNN).Â Son un subconjunto del machine learning y el nÃºcleo de los modelos dedeep learning. Descubra los componentes bÃ¡sicos y las buenas prÃ¡cticas para ayudar a sus equipos a acelerar la IA responsable. Piense en cada nodo individual como su propio modelo deregresiÃ³n lineal, compuesto por datos de entrada, ponderaciones, un sesgo (o umbral) y una salida.Â La fÃ³rmula serÃ­a la siguiente: âˆ‘wixi + sesgo = w1x1 + w2x2 + w3x3 + sesgo salida = f(x) = 1 if âˆ‘w1x1 + b>= 0; 0 if âˆ‘w1x1 + b < 0 Una vez determinada la capa de entrada, se asignan las ponderaciones. Estas ponderaciones ayudan a determinar la importancia de cualquier variable, ya que las mÃ¡s grandes contribuyen de forma mÃ¡s significativa a la salida en comparaciÃ³n con otras entradas. A continuaciÃ³n, todas las entradas se multiplican por sus respectivas ponderaciones y se suman. DespuÃ©s, la salida se pasa a travÃ©s de una funciÃ³n de activaciÃ³n, que determina la salida. Si esa salida supera un umbral determinado, se Â«disparaÂ» (o activa) el nodo, pasando los datos a la siguiente capa de la red. Esto da como resultado que la salida de un nodo se convierta en la entrada del siguiente nodo. Este proceso de pasar datos de una capa a la siguiente capa define esta red neuronal como una red de proalimentaciÃ³n. Desglosemos el aspecto de un Ãºnico nodo utilizando valores binarios. Podemos aplicar este concepto a un ejemplo mÃ¡s tangible, como si deberÃ­as ir a hacer surf (SÃ­: 1, No: 0). La decisiÃ³n de ir o no ir es nuestro resultado previsto, o y-hat. Supongamos que hay tres factores que influyen en tu decisiÃ³n: Entonces, supongamos lo siguiente, dÃ¡ndonos las siguientes entradas: Ahora, tenemos que asignar algunas ponderaciones para determinar la importancia. Unas ponderaciones mayores significan que determinadas variables son mÃ¡s importantes para la decisiÃ³n o el resultado. Por Ãºltimo, tambiÃ©n supondremos un valor umbral de 3, lo que se traducirÃ­a en un valor de sesgo de â€“3. Con todas las entradas, podemos empezar a introducir valores en la fÃ³rmula para obtener la salida deseada. Y-hat = (1*5) + (0*2) + (1*4) â€“ 3 = 6 Si utilizamos la funciÃ³n de activaciÃ³n del principio de esta secciÃ³n, podemos determinar que la salida de este nodo serÃ­a 1, ya que 6 es mayor que 0. En este caso, irÃ­a a surfear; pero si ajustamos las ponderaciones o el umbral, podemos obtener resultados diferentes del modelo. Cuando observamos una decisiÃ³n, como en el ejemplo anterior, podemos ver cÃ³mo una red neuronal podrÃ­a tomar decisiones cada vez mÃ¡s complejas en funciÃ³n de la salida de las decisiones o capas anteriores. En el ejemplo anterior, utilizamos perceptrones para ilustrar algunas de las matemÃ¡ticas que estÃ¡n en juego aquÃ­, pero las redes neuronales aprovechan las neuronas sigmoidales, que se distinguen por tener valores entre 0 y 1. Dado que las redes neuronales se comportan de forma similar a los Ã¡rboles de decisiÃ³n, con datos en cascada de un nodo a otro, tener valores x entre 0 y 1 reducirÃ¡ el impacto de cualquier cambio dado de una sola variable en la salida de cualquier nodo dado y, posteriormente, en la salida de la red neuronal. Cuando empecemos a pensar en casos de uso mÃ¡s prÃ¡cticos para las redes neuronales, como el reconocimiento o la clasificaciÃ³n de imÃ¡genes, aprovecharemos el aprendizaje supervisado, o conjuntos de datos etiquetados, para entrenar el algoritmo. Al entrenar el modelo, querremos evaluar su precisiÃ³n utilizando una funciÃ³n de coste (o pÃ©rdida). TambiÃ©n se conoce como error cuadrÃ¡tico medio (MSE). En la siguiente ecuaciÃ³n, = =1/2 âˆ‘129_(=1)^â–’(Â Ì‚^(() )âˆ’^(() ) )^2 En Ãºltima instancia, el objetivo es minimizar nuestra funciÃ³n de coste para garantizar la correcciÃ³n del ajuste para cualquier observaciÃ³n dada. A medida que el modelo ajusta sus ponderaciones y sesgos, utiliza la funciÃ³n de coste y el aprendizaje por refuerzo para alcanzar el punto de convergencia, o el mÃ­nimo local. El proceso por el que el algoritmo ajusta sus ponderaciones es el descenso gradiente, que permite al modelo determinar la direcciÃ³n que debe tomar para reducir los errores (o minimizar la funciÃ³n de coste). Con cada ejemplo de entrenamiento, los parÃ¡metros del modelo se ajustan para converger gradualmente en el mÃ­nimo. Consulte esteartÃ­culo de IBM Developer para obtener una explicaciÃ³n mÃ¡s detallada de los conceptos cuantitativos implicados en las redes neuronales. La mayorÃ­a de las redes neuronales profundas son alimentadas, lo que significa que fluyen en una sola direcciÃ³n, de la entrada a la salida. Sin embargo, tambiÃ©n puede entrenar su modelo mediante retropropagaciÃ³n; es decir, moverse en la direcciÃ³n opuesta, de la salida a la entrada. La retropropagaciÃ³n nos permite calcular y atribuir el error asociado a cada neurona, lo que nos permite ajustar y encajar adecuadamente los parÃ¡metros del modelo o modelos. El nuevo estudio empresarial que aÃºna el machine learning tradicional con las nuevas funciones de IA generativa basadas en modelos fundacionales. Las redes neuronales se pueden clasificar en distintos tipos, que se utilizan para fines diferentes. Aunque no se trata de una lista exhaustiva de tipos, la siguiente serÃ­a representativa de los tipos mÃ¡s comunes de redes neuronales que encontrarÃ¡ para sus casos de uso habituales: El perceptrÃ³n es la red neuronal mÃ¡s antigua, creada por Frank Rosenblatt en 1958. En este artÃ­culo nos hemos centrado principalmente en las redes neuronales de avance o perceptrones multicapa (MLP). Se componen de una capa de entrada, una o varias capas ocultas y una capa de salida. Aunque estas redes neuronales tambiÃ©n suelen denominarse MLP, es importante seÃ±alar que en realidad estÃ¡n formadas por neuronas sigmoidales, no por perceptrones, ya que la mayorÃ­a de los problemas del mundo real no son lineales. Los datos generalmente se introducen en estos modelos para entrenarlos, y son la base para la computer vision, elprocesamiento del lenguaje naturaly otras redes neuronales. Lasredes neuronales convolucionales (CNN)son similares a las redes de propagaciÃ³n hacia adelante (feedforward), pero generalmente se utilizan para el reconocimiento de imÃ¡genes, el reconocimiento de patrones y/o la computer vision. Estas redes aprovechan principios del Ã¡lgebra lineal, en particular la multiplicaciÃ³n de matrices, para identificar patrones dentro de una imagen. Las redes neuronales recurrentes (RNN)se identifican por sus bucles de retroalimentaciÃ³n. Estos algoritmos de aprendizaje se utilizan principalmente cuando se emplean datos de series temporales para hacer predicciones sobre resultados futuros, como predicciones bursÃ¡tiles o previsiones de ventas. El deep learning y las redes neuronales tienden a utilizarse indistintamente en la conversaciÃ³n, lo que puede ser confuso. Por lo tanto, cabe seÃ±alar que el tÃ©rmino ""deep"" (profundo) en el deep learning se refiere Ãºnicamente a la profundidad de las capas de una red neuronal. Una red neuronal que conste de mÃ¡s de tres capas, que incluirÃ­an las entradas y la salida, puede considerarse un algoritmo de deep learning. Una red neuronal que solo tiene dos o tres capas es una red neuronal bÃ¡sica. Para obtener mÃ¡s informaciÃ³n sobre las diferencias entre las redes neuronales y otras formas de inteligencia artificial, como el machine learning, lea la entrada de blog ""Diferencias entre IA frente, machine learning, deep learning frente y redes neuronales"" La historia de las redes neuronales es mÃ¡s larga de lo que la mayorÃ­a de la gente cree. Aunque la idea de ""una mÃ¡quina que piensa"" se remonta a los antiguos griegos, nos centraremos en los acontecimientos clave que condujeron a la evoluciÃ³n del pensamiento en torno a las redes neuronales, cuya popularidad ha sufrido altibajos a lo largo de los aÃ±os: 1943:Warren S. McCulloch y Valeriano Pitts publicaron ""CÃ¡lculo lÃ³gico de las ideas inherentes a la actividad nerviosa"" Esta investigaciÃ³n pretendÃ­a entender cÃ³mo el cerebro humano podÃ­a producir patrones complejos a travÃ©s de cÃ©lulas cerebrales conectadas, o neuronas. Una de las principales ideas que surgieron de este trabajo fue la comparaciÃ³n de las neuronas con un umbral binario a la lÃ³gica booleana (es decir, 0/1 o afirmaciones verdadero/falso). 1958:Se atribuye a Frank Rosenblatt el desarrollo del perceptrÃ³n, documentado en su investigaciÃ³n ""El perceptrÃ³n: Un modelo probabilÃ­stico de almacenamiento y organizaciÃ³n de la informaciÃ³n en el cerebro"". Lleva el trabajo de McCulloch y Pitt un paso mÃ¡s allÃ¡ introduciendo ponderaciones en la ecuaciÃ³n. Aprovechando un IBM 704, Rosenblatt consiguiÃ³ que un ordenador aprendiera a distinguir las cartas marcadas a la izquierda de las marcadas a la derecha. 1974:Aunque numerosos investigadores contribuyeron a la idea de la retropropagaciÃ³n, Paul Werbos fue la primera persona en EE.Â UU. en anotar su aplicaciÃ³n en redes neuronales dentro de sutesis de doctorado. 1989:Yann LeCun publicÃ³ unartÃ­culoque ilustra cÃ³mo el uso de restricciones en la retropropagaciÃ³n y su integraciÃ³n en la arquitectura de la red neuronal puede utilizarse para entrenar algoritmos. Esta investigaciÃ³n aprovechÃ³ con Ã©xito una red neuronal para reconocer dÃ­gitos de cÃ³digos postales escritos a mano proporcionados por el Servicio Postal de Estados Unidos. DiseÃ±ar redes neuronales complejas. Experimente a escala para implementar modelos de aprendizaje optimizados en IBM Watson Studio. Cree y escale una IA de confianza en cualquier cloud. Automatice el ciclo de vida de la IA para ModelOps. DÃ© el siguiente paso para empezar a operacionalizar y ampliar la IA generativa y el aprendizaje automÃ¡tico para las empresas. RegÃ­strese en nuestro libro electrÃ³nico para obtener informaciÃ³n sobre las oportunidades, los retos y las lecciones aprendidas de la introducciÃ³n de la IA en las empresas. Estos tÃ©rminos suelen utilizarse indistintamente, pero Â¿quÃ© diferencias hacen de cada uno de ellos una tecnologÃ­a Ãºnica? Conozca en profundidad las redes neuronales, sus funciones bÃ¡sicas y los fundamentos para construir una. Entrene, valide, ajuste e implemente IA generativa, modelos fundacionales y capacidades de machine learning conÂ IBM watsonx.ai, un estudio empresarial de prÃ³xima generaciÃ³n para constructores de IA. Cree aplicaciones de IA en menos tiempo y con menos datos."
https://www.cloudflare.com/learning/ai/what-is-neural-network/,Error: 403 Client Error: Forbidden for url: https://www.cloudflare.com/learning/ai/what-is-neural-network/
https://link.springer.com/chapter/10.1007/978-3-030-89010-0_10,"Advertisement You have full access to thisopen accesschapterDownloadbook PDFDownloadbook EPUB You have full access to thisopen accesschapter 43kAccesses 50Citations In this chapter, we go through the fundamentals of artificial neural networks and deep learning methods. We describe the inspiration for artificial neural networks and how the methods of deep learning are built. We define the activation function and its role in capturing nonlinear patterns in the input data. We explain the universal approximation theorem for understanding the power and limitation of these methods and describe the main topologies of artificial neural networks that play an important role in the successful implementation of these methods. We also describe loss functions (and their penalized versions) and give details about in which circumstances each of them should be used or preferred. In addition to the Ridge, Lasso, and Elastic Net regularization methods, we provide details of the dropout and the early stopping methods. Finally, we provide the backpropagation method and illustrate it with two simple artificial neural networks. You have full access to this open access chapter,Download chapter PDF The inspiration for artificial neural networks (ANN), or simply neural networks, resulted from the admiration for how the human brain computes complex processes, which is entirely different from the way conventional digital computers do this. The power of the human brain is superior to many information-processing systems, since it can perform highly complex, nonlinear, and parallel processing by organizing its structural constituents (neurons) to perform such tasks as accurate predictions, pattern recognition, perception, motor control, etc. It is also many times faster than the fastest digital computer in existence today. An example is the sophisticated functioning of the information-processing task called human vision. This system helps us to understand and capture the key components of the environment and supplies us with the information we need to interact with the environment. That is, the brain very often performs perceptual recognition tasks (e.g., voice recognition embedded in a complex scene) in around 100â€“200Â ms, whereas less complex tasks many times take longer even on a powerful computer (Haykin2009). Another interesting example is the sonar of a bat, since the sonar is an active echolocation system. The sonar provides information not only about how far away the target is located but also about the relative velocity of the target, its size, and the size of various features of the target, including its azimuth and elevation. Within a brain the size of a plum occur the computations required to extract all this information from the target echo. Also, it is documented that an echolocating bat has a high rate of success when pursuing and capturing its target and, for this reason, is the envy of radar and sonar engineers (Haykin2009). This bat capacity inspired the development of radar, which is able to detect objects that are in its path, without needing to see them, thanks to the emission of an ultrasonic wave, the subsequent reception and processing of the echo, which allows it to detect obstacles in its flight with surprising speed and accuracy (Francisco-Caicedo and LÃ³pez-Sotelo2009). In general, the functioning of the brains of humans and other animals is intriguing because they are able to perform very complex tasks in a very short time and with high efficiency. For example, signals from sensors in the body convey information related to sight, hearing, taste, smell, touch, balance, temperature, pain, etc. Then the brainâ€™s neurons, which are autonomous units, transmit, process, and store this information so that we can respond successfully to external and internal stimuli (Dougherty2013). The neurons of many animals transmit spikes of electrical activity through a long, thin strand called an axon. An axon is divided into thousands of terminals or branches, where depending on the size of the signal they synapse to dendrites of other neurons (Fig.10.1). It is estimated that the brain is composed of around 1011neurons that work in parallel, since the processing done by the neurons and the memory captured by the synapses are distributed together over the network. The amount of information processed and stored depends on the threshold firing levels and also on the weight given by each neuron to each of its inputs (Dougherty2013). A graphic representation of a biological neuron One of the characteristics of biological neurons, to which they owe their great capacity to process and perform highly complex tasks, is that they are highly connected to other neurons from which they receive stimuli from an event as it occurs, or hundreds of electrical signals with the information learned. When it reaches the body of the neuron, this information affects its behavior and can also affect a neighboring neuron or muscle (Francisco-Caicedo and LÃ³pez-Sotelo2009). Francisco-Caicedo and LÃ³pez-Sotelo (2009) also point out that the communication between neurons goes through the so-called synapses. A synapse is a space that is occupied by chemicals called neurotransmitters. These neurotransmitters are responsible for blocking or passing on signals that come from other neurons. The neurons receive electrical signals from other neurons with which they are in contact. These signals accumulate in the body of the neuron and determine what to do. If the total electrical signal received by the neuron is sufficiently large, the action potential can be overcome, which allows the neuron to be activated or, on the contrary, to remain inactive. When a neuron is activated, it is able to transmit an electrical impulse to the neurons with which it is in contact. This new impulse, for example, acts as an input to other neurons or as a stimulus in some muscles (Francisco-Caicedo and LÃ³pez-Sotelo2009). The architecture of biological neural networks is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as shown in Fig.10.2. Multiple layers in a biological neural network of human cortex ANN

 are machines designed to perform specific tasks by imitating how the human brain works, and build a neural network made up of hundreds or even thousands of artificial neurons or processing units. The artificial neural network is implemented by developing a computational learning algorithm that does not need to program all the rules since it is able to build up its own rules of behavior through what we usually refer to as â€œexperience.â€ The practical implementation of neural networks is possible due to the fact that they are massively parallel computing systems made up of a huge number of basic processing units (neurons) that are interconnected and learn from their environment, and the synaptic weights capture and store the strengths of the interconnected neurons. The job of the learning algorithm consists of modifying the synaptic weights of the network in a sequential and supervised way to reach a specific objective (Haykin2009). There is evidence that neurons working together are able to learn complex linear and nonlinear inputâ€“output relationships by using sequential training procedures. It is important to point out that even though the inspiration for these models was quite different from what inspired statistical models, the building blocks of both types of models are quite similar. Anderson et al. (1990) and Ripley (1993) pointed out that neural networks are simply no more thangeneralized nonlinear statistical models. However, Anderson et al. (1990) were more expressive in this sense and also pointed out that â€œANN are statistics for amateurs since most neural networks conceal the statistics from the user.â€ To get a clear idea of the main elements used to construct ANN models, in Fig.10.3we provide a general artificial neural network model that contains the main components for this type of models. General artificial neural network model x1, â€¦,xprepresents the information (input) that the neuron receives from the external sensory system or from other neurons with which it has a connection.w=Â (w1,â€‰â€¦,wp) is the vector of synaptic weights that modifies the received information emulating the synapse between the biological neurons. These can be interpreted as gains that can attenuate or amplify the values that they wish to propagate toward the neuron. Parameterbjis known as the bias (intercept or threshold) of a neuron. Here in ANN, learning refers to the method of modifying the weights of connections between the nodes (neurons) of a specified network. The different values that the neuron receives are modified by the synaptic weights, which then are added together to produce what is called thenet input. In mathematical notation, that is equal to This net input (vj) is what determines whether the neuron is activated or not. The activation of the neuron depends on what we call theactivation function. The net input is evaluated in this function and we obtain the output of the network as shown next: wheregis the activation function. For example, if we define this function as a unit step (also called threshold), the output will be 1 if the net input is greater than zero; otherwise the output will be 0. Although there is no biological behavior indicating the presence of something similar to the neurons of the brain, the use of the activation function is an artifice that allows applying ANN to a great diversity of real problems. According to what has been mentioned, outputyjof the neuron is generated when evaluating the net input (vj) in the activation function. We can propagate the output of the neuron to other neurons or it can be the output of the network, which, according to the application, will have an interpretation for the user. In general, the job of an artificial neural network model is done by simple elements called neurons. The signals are passed between neurons through connection links. Each connection link has an associated weight, which, in a typical neuronal network, multiplies the transmitted signal. Each neuron applies an activation function (usually nonlinear) to the network inputs (sum of the heavy input signals) for determining its corresponding sign. Later in this chapter, we describe the many options for activation functions and the context in which they can be used. A unilayer ANN like that in Fig.10.3has a low processing capacity by itself and its level of applicability is low; its true power lies in the interconnection of many ANNs, as happens in the human brain. This has motivated different researchers to propose various topologies (architectures) to connect neurons to each other in the context of ANN. Next, we provide two definitions of ANN and one definition of deep learning: Definition 1. An artificial neural network is a system composed of many simple elements of processing which operate in parallel and whose function is determined by the structure of the network and the weight of connections, where the processing is done in each of the nodes or computing elements that has a low processing capacity (Francisco-Caicedo and LÃ³pez-Sotelo2009). Definition 2. An artificial neural network is a structure containing simple elements that are interconnected in many ways with hierarchical organization, which tries to interact with objects in the real world in the same way as the biological nervous system does (Kohonen2000). Deep learning model. We define deep learning


 as a generalization of ANN where more than one hidden layer is used, which implies that more neurons are used for implementing the model. For this reason, an artificial neural network with multiple hidden layers is called a Deep Neural Network (DNN) and the practice of training this type of networks is calleddeep learning (DL), which is a branch of statistical machine learning where a multilayered (deep) topology is used to map the relations between input variables (independent variables) and the response variable (outcome). Chollet and Allaire (2017) point out that DL puts the â€œemphasis on learning successive layers of increasingly meaningful representations.â€ The adjective â€œdeepâ€ applies not to the acquired knowledge, but to the way in which the knowledge is acquired (Lewis2016), since it stands for the idea of successive layers of representations. The â€œdeepâ€ of the model refers to the number of layers that contribute to the model. For this reason, this field is also called layered representation learning and hierarchical representation learning (Chollet and Allaire2017). It is important to point out that DL


 as a subset of machine learning is an aspect of artificial intelligence (AI) that has more complex ways of connecting layers than conventional ANN, which uses more neurons than previous networks to capture nonlinear aspects of complex data better, but at the cost of more computing power required to automatically extract useful knowledge from complex data. To have a more complete picture


 of ANN, we provide another model, which is a DL model since it has two hidden layers, as shown in Fig.10.4. Artificial deep neural network with a feedforward neural network with eight input variables (x1, â€¦ ,x8), four output variables (y1,y2,y3,y4), and two hidden layers with three neurons each From Fig.10.4we can see that an artificial neural


 network is a directed graph whosenodescorrespond to neurons and whoseedgescorrespond tolinksbetween them. Each neuron receives, as input, a weighted sum of the outputs of the neurons connected to its incoming edges (Shalev-Shwartz and Ben-David2014). In the artificial deep neural network given in Fig.10.4, there are four layers (V0,V1,V2, andV3):V0represents the input layer,V1andV2are the hidden layers, andV3denotes the output layer. In this artificial deep neural network, three is the number of layers of the network sinceV0, which contains the input information, is excluded. This is also called the â€œdepthâ€ of the network. The size of this network is\( \left|V\right|=\left|\bigcup \limits_{t=0}^{\mathrm{T}}{V}_t\right|=\left|9+4+4+4\right|=21 \). Note that in each layer we added +1 to the observed units to represent the node of the bias (or intercept). The width of the network is max|Vt|Â =Â 9. The analytical form of the model given in Fig.10.4for outputo, withdinputs,M1hidden neurons (units) in hidden layer 1,M2hidden units in hidden layer 2, and O output neurons is given by the following (10.1)â€“(10.3): where (10.1) produces the output of each of the neurons


 in the first hidden layer, (10.2) produces the output of each of the neurons in the second hidden layer, and finally (10.3) produces the output of each response variable of interest. The learning process is obtained with the weights (\( {w}_{ji}^{(1)},{w}_{kj}^{(2)}, \)and\( {w}_{lk}^{(3)}\Big) \), which are accommodated in the following vector:\( \boldsymbol{w}=\left({w}_{11}^{(1)},{w}_{12}^{(1)},\dots, {w}_{1d}^{(1)},{w}_{21}^{(2)},{w}_{22}^{(2)},\dots, {w}_{2{M}_1}^{(2)},{w}_{31}^{(3)},{w}_{32}^{(3)},\dots, {w}_{3{M}_2}^{(3)}\right), \)g1,g2, andg3are the activation functions in hidden layers 1, 2, and the output layer, respectively. The model given in Fig.10.4is organized as several interconnected layers: the input layer, hidden layers, and output layer, where each layer that performs nonlinear transformations is a collection of artificial neurons, and connections among these layers are made using weights (Fig.10.4). When only one output variable is present in Fig.10.4, the model is called univariate DL model. Also, when only one hidden layer is present in Fig.10.4, the DL model is reduced to a conventional artificial neural network model, but when more than one hidden layer


 is included, it is possible to better capture complex interactions, nonlinearities, and nonadditive effects. To better understand the elements of the model depicted in Fig.10.4, it is important to distinguish between the types of layers and the types of neurons; for this reason, next we will explain the type of layers and then the type of neurons in more detail. Input layer: It is the set of neurons that directly receives the information coming from the external sources of the network. In the context of Fig.10.4, this information isx1, â€¦ ,x8 (Francisco-Caicedo and LÃ³pez-Sotelo2009). Therefore, the number of neurons in an input layer is most of the time the same as the number of the input explanatory variables provided to the network. Usually input layers are followed by at least one hidden layer. Only in feedforward neuronal networks, input layers are fully connected to the next hidden layer (Patterson and Gibson2017). Hidden layers: Consist of a set of internal neurons


 of the network that do not have direct contact with the outside. The number of hidden layers can be 0, 1, or more. In general, the neurons of each hidden layer share the same type of information; for this reason, they are called hidden layers. The neurons of the hidden layers can be interconnected in different ways; this determines, together with their number, the different topologies of ANN and DNN (Francisco-Caicedo and LÃ³pez-Sotelo2009). The learned information extracted from the training data is stored and captured by the weight values of the connections between the layers of the artificial neural network. Also, it is important to point out that hidden layers are key components for capturing complex nonlinear behaviors of data more efficiently (Patterson and Gibson2017). Output layer: It is a set of neurons that transfers the information that the network has processed to the outside (Francisco-Caicedo and LÃ³pez-Sotelo2009). In Fig.10.4the output neurons


 correspond to the output variablesy1,y2,y3, andy4. This means that the output layer gives the answer or prediction of the artificial neural network model based on the input from the input layer. The final output can be continuous, binary, ordinal, or count depending on the setup of the ANN which is controlled by the activation (or inverse link in the statistical domain) function we specified on the neurons in the output layer (Patterson and Gibson2017). Next, we define the types of neurons: (1)input neuron.A neuron that receives external inputs from outside the network; (2)output neuron.A neuron that produces some of the outputs of the network; and (3)hidden neuron.A neuron that has no direct interaction with the â€œoutside worldâ€ but only with other neurons within the network. Similar terminology is used at the layer level formultilayer neural networks. As can be seen in Fig.10.4, the distribution of neurons within an artificial neural network is done by forming levels of a certain number of neurons. If a set of artificial neurons simultaneously receives the same type of information, we call it a layer. We also described a network of three types


 of levels called layers. Figure10.5shows another six networks with different numbers of layers, and half of them (Fig.10.5a, c, e) are univariate since the response variable we wish to predict is only one, while the other half (Fig.10.5b, d, f) are multivariate since the interest of the network is to predict two outputs. It is important to point out that subpanels a and b in Fig.10.5are networks with only one layer and without hidden layers; for this reason, this type of networks corresponds to conventional regression or classification regression models. Different feedforward topologies with univariate and multivariate outputs and different number of layers. (a) Unilayer and univariate output. (b) Unilayer and multivariate output. (c) Three layer and univariate output. (d) Three layer and multivariate output. (e) Four layer univariate output. (f) Four layer multivariate output Therefore, the topology of an artificial neural network is the way in which neurons are organized inside the network; it is closely linked to the learning algorithm used to train the network. Depending on the number of layers, we define the networks asmonolayerandmultilayer; and if we take as a classification element the way information flows, we define the networks as feedforward or recurrent. Each type of topology will be described in another section. In summary, an artificial (deep) neural network model is an information processing system that mimics the behavior of biological neural networks, which was developed as a generalization of mathematical models of human knowledge or neuronal biology. The mapping between inputs


 and a hidden layer in ANN and DNN is determined by activation functions. Activation functions propagate the output of one layerâ€™s nodes forward to the next layer (up to and including the output layer). Activation functions are scalar-to-scalar functions that provide a specific output of the neuron. Activation functions allow nonlinearities to be introduced into the networkâ€™s modeling capabilities (Wiley2016). The activation function of a neuron (node) defines the functional form for how a neuron gets activated. For example, if we define a linear activation function asg(z)Â =z, in this case the value of the neuron would be the raw input,x, times the learned weight, that is, a linear model. Next, we describe the most popular activation functions. Figure10.6shows a linear activation



 function that is basically the identity function. It is defined asg(z)Â =Wz, where the dependent variable has a direct, proportional relationship with the independent variable. In practical terms, it means the function passes the signal through unchanged. The problem with making activation functions linear is that this does not permit any nonlinear functional forms to be learned (Patterson and Gibson2017). Representation of a linear



 activation function The rectifier linear unit (ReLU) activation function is one of the most popular. The ReLU activation function



 is flat below some threshold (usually the threshold is zero) and then linear. The ReLU activates a node only if the input is above a certain quantity. When the input is below zero, the output is zero, but when the input rises above a certain threshold, it has a linear relationship with the dependent variableg(z)Â =Â Â maxÂ (0,z), as demonstrated in Fig.10.7. Despite its simplicity, the ReLU activation function provides nonlinear transformation, and enough linear rectifiers can be used to approximate arbitrary nonlinear functions, unlike when only linear activation functions are used (Patterson and Gibson2017). ReLUs are the current state of the art because they have proven to work in many different situations. Because the gradient of a ReLU is either zero or a constant, it is not easy to control the vanishing exploding gradient issue, also known as the â€œdying ReLUâ€ issue. ReLU activation functions have been shown to train better in practice than sigmoid activation functions. This activation function is the most used in hidden layers and in output layers when the response variable



 is continuous and larger than zero. Representation

 of the ReLU



 activation function Leaky ReLUs are a strategy



 to mitigate the â€œdying ReLUâ€ issue. As opposed to having the function be zero whenz<Â 0, the leaky ReLU will instead have a small negative slope,Î±,whereÎ±is a value between 0 and 1 (Fig.10.8). In practice, some success has been achieved with this ReLU variation, but results are not always consistent. The function of this activation function is given here: Representation of the Leaky ReLU



 activation function withÎ±=Â 0.1 A sigmoid activation



 function is a machine that converts independent variables of near infinite range into simple probabilities between 0 and 1, and most of its output will be very close to 0 or 1. Like all logistic transformations, sigmoids can reduce extreme values or outliers in data without removing them. This activation function resembles an S (Wiley2016; Patterson and Gibson2017) and is defined asg(z)Â =Â (1Â +eâˆ’z)âˆ’1. This activation function is one of the most common types of activation functions used to construct ANNs and DNNs, where the outcome is a probability or binary outcome. This activation function is a strictly increasing function that exhibits a graceful balance between linear and nonlinear behavior but has the propensity to get â€œstuck,â€ i.e., the output values would be very close to 1 or 0 when the input values are strongly positive or negative (Fig.10.9). By getting â€œstuckâ€ we mean that the learning process is not improving due to the large or small values



 of the output values of this activation function. Representation

 of the sigmoid activation function Softmax



 is a generalization of the sigmoid activation function that handles multinomial labeling systems, that is, it is appropriate for categorical outcomes. Softmax is the function you will often find in the output layer of a classifier with more than two categories. The softmax activation function returns the probability distribution over mutually exclusive output classes. To further illustrate the idea of the softmax output layer and how to use it, letâ€™s consider two types of uses. If we have a multiclass modeling problem we only care about the best score across these classes, weâ€™d use a softmax output layer with anargmax()function to get the highest score across all classes. For example, let us assume that our categorical response has ten classes; with this activation function we calculate a probability for each category (the sum of the ten categories is one) and we classify a particular individual



 in the class with the largest probability. It is important to recall that if we want to get binary classifications per output (e.g., â€œdiseased and not diseasedâ€), we do not want softmax as an output layer. Instead, we will use the sigmoid activation function explained before. The softmax function is defined as This activation function is a generalization of the sigmoid activation function that squeezes (force) a C dimensional vector of arbitrary real values to a C dimensional vector of real values in the range [0,1] that adds up to 1. A strong prediction would have a single entry in the vector close to 1, while the remaining entries would be close to 0. A weak prediction would have multiple possible categories (labels) that are more or less equally likely. The sigmoid and softmax activation functions are suitable for probabilistic interpretation due to the fact that the output is a probabilistic distribution of the classes. This activation function is mostly recommended for output layers when the response



 variable is categorical. The hyperbolic tangent (Tanh) activation



 function is defined as\( \tanh \left(\mathrm{z}\right)=\sinh \left(\mathrm{z}\right)/\cosh \left(\mathrm{z}\right)=\frac{\exp (z)-\exp \left(-z\right)}{\exp (z)+\exp \left(-z\right)} \). The hyperbolic tangent works well in some cases and, like the sigmoid activation function, has a sigmoidal (â€œSâ€ shaped) output, with the advantage that it is less likely to get â€œstuckâ€ than the sigmoid activation function since its output values are between âˆ’1 and 1, as shown in Fig.10.10. For this reason, for hidden layers should be preferred the Tanh activation function. Large negative inputs to the tanh function will give negative outputs, while large positive inputs will give positive outputs (Patterson and Gibson2017). The advantage of tanh is that it can deal more easily with negative numbers. Representation of the tanh activation function It is important to point out that there are more activations functions like thethresholdactivation function introduced in the pioneering work



 on ANN by McCulloch and Pitts (1943), but the ones just mentioned are some of the most used. The universal approximation theorem

 is at the heart of ANN since it provides the mathematical basis of why artificial neural networks work in practice for nonlinear inputâ€“output mapping. According to Haykin (2009), this theorem can be stated as follows. Letg() be a bounded, and monotone-increasing continuous function. Let\( {I}_{m_0} \)denote them0-dimensional unit hypercube\( {\left[0,1\right]}^{m_0} \). The space of continuous functions on\( {I}_{m_0} \)is denoted by\( C\left({I}_{m_0}\right) \). Then given any function\( f\ni C\left({I}_{m_0}\right) \)andÎµ>Â 0, there is an integerm1and sets of real constantsÎ±i,bi, andwij, wherei=Â 1,â€¦,m1andj=Â 1,â€¦,m0such that we may define as an approximate realization of functionf(Â·); that is, For all\( {x}_1,\dots, {x}_{m_0} \)that lie in the input space. m0represents the input nodes of a multilayer perceptron with a single hidden layer.m1is the number of neurons in the single hidden layer,\( {x}_1,\dots, {x}_{m_0} \)are the inputs,wijdenotes the weight of neuroniin inputj,bidenotes the bias corresponding to neuroni, andÎ±iis the weight of the output layer in neuroni. This theorem states that any feedforward neural network containing a finite number of neurons is capable of approximating any continuous functions of arbitrary complexity to arbitrary accuracy, if provided enough neurons

 in even a single hidden layer, under mild assumptions of the activation function. In other words, this theorem says that any continuous function that maps intervals of real numbers to some output interval of real numbers can be approximated arbitrarily and closely by a multilayer perceptron with just one hidden layer and a finiteverylarge number of neurons(Cybenko1989; Hornik1991). However, this theorem only guarantees a reasonable approximation; for this reason, this theorem is an existence theorem. This implies that simple ANNs are able to represent a wide variety of interesting functions if given enough neurons and appropriate parameters; but nothing is mentioned about the algorithmic learnability of those parameters, nor about their time of learning, ease of implementation, generalization, or that a single hidden layer is optimum. The first version of this theorem was given by Cybenko (1989) for sigmoid activation functions. Two years later, Hornik (1991) pointed out that the potential of â€œANN of being universal approximators is not due to the specific choice of the activation function, but to the multilayer feedforward architecture itself.â€ From this theorem, we can deduce that when an artificial neural network has more than two hidden layers, it will not always improve the prediction performance since there is a higher risk of converging to a local minimum. However, using two hidden layers is recommended when the data has discontinuities. Although the proof of this theorem was done for only a single output, it is also valid for the multi-output scenario and can easily be deduced from the single output case. It is important to point out that this theorem states that all activation functions will perform equally well in specific learning problems

 since their performance depends on the data and additional issues such as minimal redundancy, computational efficiency, etc. In this subsection, we describe the most popular network

 topologies. An artificialneural network topologyrepresents the way in which neurons are connected to form a network. In other words, the neural network topology can be seen as the relationship between the neurons by means of their connections. The topology of a neural network plays a fundamental role in its functionality and performance, as illustrated throughout this chapter. The generic termsstructureandarchitectureare used as synonyms for network topology. However, caution should be exercised when using these terms since their meaning is not well defined and causes confusion in other domains where the same terms are used for other purposes. More precisely, the topology of a neural network

 consists of itsframeorframeworkof neurons, together with itsinterconnection structureorconnectivity: The next two subsections are devoted to these two components. Most neural networks, including many biological ones, have a layered topology. There are a few exceptions where the network

 is not explicitly layered, but those can usually be interpreted as having a layered topology, for example, in someassociative memory networks,which can be seen as one-layer neural networks where all neurons function both as input and output units. At the framework level, neurons are considered abstract entities, therefore possible differences between them are not considered. The framework of an artificial neural network can therefore be described by the number of neurons, number of layers (denoted byL),and the size of the layer, which consists of the number of neurons in each of the layers. The interconnection structure of an artificial neural network determines the way in which the neurons are linked. Based on a layered structure, several different kinds of connections can be distinguished (see Fig.10.11): (a)Interlayer connection:This connects neurons

 in adjacent layers whose layer indices differ by one; (b)Intralayer connection:This is a connection between neurons in the same layer; (c)Self-connection:This is a special kind of intralayer connection that connects a neuron to itself; (d)Supralayer connection:This is a connection between neurons that are in distinct nonadjacent layers; in other words, these connections â€œcrossâ€ or â€œjumpâ€ at least one hidden layer. Network topology with two layers. (i) denotes the six interlayer connections, (s) denotes the four supralayered connections, and (a) denotes four intralayer connections of which two are self-connections With each connection(interconnection), aweight (strength)is associated which is a weighting factor that reflects its importance. This weight is a scalar value (a number), which can be positive(excitatory)or negative(inhibitory).If a connection has zero weight, it is considered to be nonexistent at that point in time. Note that the basic concept of layeredness

 is based on the presence of interlayer connections. In other words, every layered neural network has at least one interlayer connection between adjacent layers. If interlayer connections are absent between any two adjacent clusters in the network, a spatial reordering can be applied to the topology, after which certain connections become the interlayer connections of the transformed, layered network. Now that we have described the two key components

 of an artificial neural network topology, we will present two of the most commonly used topologies. In this type of artificial neural network, the information flows in a single direction from the input neurons to the processing layer or layers (only interlayer connections) for monolayer and multilayer networks, respectively, until reaching the output layer of the neural network. This means that there are no connections between neurons in the same layer (no intralayer), and there are no connections that transmit data

 from a higher layer to a lower layer, that is, no supralayer connections (Fig.10.12). This type of network is simple to analyze, but is not restricted to only one hidden layer. A simple two-layer feedforward artificial neural network In this type of neural network, information does not always flow in one direction, since it can feed back into previous layers

 through synaptic connections. This type of neural network can be monolayer or multilayer. In this network, all the neurons have (1) incoming connections emanating from all the neurons in the previous layer, (2) ongoing connections leading to all the neurons in the subsequent layer, and (3) recurrent connections that propagate information between neurons of the same layer. Recurrent neural networks (RNNs) are different from a feedforward neural network in that they have at least one feedback loop since the signals travel in both directions. This type of network is frequently used in time series prediction since short-term memory, or delay, increases the power of recurrent networks immensely. In this case, we present an example of a recurrent two-layer neural network. The output of each neuron is passed through a delay unit and then taken to all the neurons, except itself. In Figs.10.13and10.14, we can see that only one input variable is presented to the input units, the feedforward flow is computed, and the outputs are fed back as auxiliary inputs. This leads to a different set of hidden unit activations, new output

 activations, and so on. Ultimately, the activations stabilize, and the final output values are used for predictions. A simple two-layer recurrent artificial neural network with univariate output A two-layer recurrent artificial neural network with multivariate outputs However, it is important to point out out that despite the just mentioned virtues of recurrent artificial neural networks, they are still largely theoretical and produce mixed results (good and bad) in real applications. On the other hand, the feedforward networks are the most popular since they are successfully implemented in all areas of domain; the multilayer perceptron (MLP; that is, onother name give to feedforward networks) is the de facto standard artificial neural network topology (Lantz2015). There are other DNN

 topologies like convolutional neural networks that are presented in Chap.13, but they can be found also in books specializing in deep learning. The success of ANN


 and DL is due to remarkable results on perceptual problems such as seeing and hearingâ€”problems involving skills that seem natural and intuitive to humans but have long been elusive for machines. Next, we provide some of these successful applications: Near-human-level image classification, speech recognition, handwriting transcription, autonomous driving (Chollet and Allaire2017) Automatic translation of text and images (LeCun et al.2015) Improved text-to-speech conversion (Chollet and Allaire2017) Digital assistants such as Google Now and Amazon Alexa Improved ad targeting, as used by Google, Baidu, and Bing Improved search results on the Web (Chollet and Allaire2017) Ability to answer natural language questions (Goldberg2016) In games like chess, Jeopardy, GO, and poker (Makridakis et al.2018) Self-driving cars (Liu et al.2017), Voice search and voice-activated intelligent assistants (LeCun et al.2015) Automatically adding sound to silent movies (Chollet and Allaire2017) Energy market price forecasting (Weron2014) Image recognition (LeCun et al.2015) Prediction of time series (Dingli and Fournier2017) Predicting breast, brain (Cole et al.2017), or skin cancer Automatic image captioning (Chollet and Allaire2017) Predicting earthquakes (Rouet-Leduc et al.2017) Genomic prediction (Montesinos-LÃ³pez et al.2018a,b) It is important to point out that the applications


 of ANN and DL are not restricted to perception and natural language understanding, such as formal reasoning. There are also many successful applications in biological science. For example, deep learning has been successfully applied for predicting univariate continuous traits (Montesinos-LÃ³pez et al.2018a), multivariate continuous traits (Montesinos-LÃ³pez et al.2018b), univariate ordinal traits (Montesinos-LÃ³pez et al.2019a), and multivariate traits with mixed outcomes (Montesinos-LÃ³pez et al.2019b) in the context of genomic-based prediction. Menden et al. (2013) applied a DL method to predict the viability of a cancer cell line exposed to a drug. Alipanahi et al. (2015) used DL with a convolutional network architecture (an ANN with convolutional operations; see Chap.13) to predict specificities of DNA- and RNA-binding proteins. Tavanaei et al. (2017) used a DL method for predicting tumor suppressor genes and oncogenes. DL methods have also made accurate predictions of single-cell DNA methylation states (Angermueller et al.2016). In the area of genomic selection, we mention two reports only: (a) McDowell and Grant (2016) found that DL methods performed similarly to several Bayesian and linear regression techniques that are commonly employed for phenotype prediction and genomic selection in plant breeding and (b) Ma et al. (2017) also used a DL method with a convolutional neural network architecture to predict phenotypes from genotypes in wheat and found that the DL


 method outperformed the GBLUP method. However, a review of DL application to genomic selection is provided by Montesinos-LÃ³pez et al. (2021). Loss function


 (also known as objective function) in general terms is a function that maps an event or values of one or more variables onto a real number intuitively representing some â€œcostâ€ associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its negative (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case now the goal is a maximization process. In the statistical machine learning domain, a loss function tries to quantify how close the predicted values produced by an artificial neural network or DL model are to the true values. That is, the loss function measures the quality of the networkâ€™s output by computing a distance score between the observed and predicted values (Chollet and Allaire2017). The basic idea is to calculate a metric based on the observed error between the true and predicted values to measure how well the artificial neural network modelâ€™s prediction matches what was expected. Then these errors are averaged over the entire data set to provide only a single number that represents how the artificial neural network is performing with regard to its ideal. In looking for this ideal, it is possible to find the parameters (weights and biases) of the artificial neural network that will minimize the â€œlossâ€ produced by the errors. Training ANN models with loss functions allows the use of optimization methods to estimate the required parameters. Although most of the time it is not possible to obtain an analytical solution to estimate the parameters, very often good approximations can be obtained using iterative optimization algorithms like gradient descent (Patterson and Gibson2017). Next, we provide the most used loss functions


 for each type of response variable. This loss function



 is appropriate for continuous response variables (outcomes), assuming that we want to predict L response variables. The error (difference between observed (yij) and predicted (\( {\hat{y}}_{ij} \)) values) in a prediction is squared and summed over the number of observations, since the training of the network is not local but global. To capture all possible trends in the training data, the expression used for sum of square error (SSE) loss is Note thatnis the size of your data set, andL, the number of targets (outputs) the network has to predict. It is important to point out that when there is only one response variable, theLis dropped. Also, the division by two is added for mathematical convenience (which will become clearer in the context of its gradient in backpropagation). One disadvantage of this loss function is that it is quite sensitive to outliers and, for this reason, other loss functions have been proposed for continuous response variables. With the loss function, it is possible to calculate the loss score, which is used as a feedback signal to adjust the weights of the artificial neural network; this process



 of adjusting the weights in ANN is illustrated in Fig.10.15(Chollet and Allaire2017). It is also common practice to use as a loss function, the SSE divided by the training sample (n) multiplied by the number of outputs (L). The loss score is used as a feedback signal to adjust the weights Figure10.15shows that in the learning process of an artificial neural network are involved the interaction of layers, input data, loss function which defines the feedback signal used for learning, and the optimizer which determines how the learning proceeds and uses the loss value to update the networkâ€™s weights. Initially, the weights of the network are assigned small random values, but when this provides an output far from the ideal values, it also implies a high loss score. But at each iteration of the network process, the weights are adjusted a little to reduce the difference between the observed and predicted values and, of course, to decrease the loss score. This is the basic step of the training process of statistical machine learning models in general, and when this process is repeated a sufficient number of times (on the order of thousands of iterations), it yields weight values that minimize the loss function. A network with minimal loss is one in which the observed and predicted values are very close; it is called a trained network (Chollet and Allaire2017). There are other options of loss functions for continuous



 data like the sum of absolute percentage error loss (SAPE):\( L\left(\boldsymbol{w}\right)={\sum}_{i=1}^n{\sum}_{j=1}^L\left|\frac{{\hat{y}}_{ij}-{y}_{ij}}{y_{ij}}\right| \)and the sum of squared log error loss (Patterson and Gibson2017):\( L\left(\boldsymbol{w}\right)={\sum}_{i=1}^n{\sum}_{j=1}^L{\left(\log \left({\hat{y}}_{ij}\right)-\log \left({y}_{ij}\right)\right)}^2 \), but the SSE is popular in ANN and DL models



 due to its nice mathematical properties. Next, we provide two popular loss functions



 for binary data: the hinge loss and the cross-entropy loss. This loss function originated in the context of the support vector machine for â€œmaximum-marginâ€ classification, and is defined as It is important to point out that since this loss function is appropriate for binary data, the intended response variable output is denoted as +1 for success and âˆ’1 for failure. This loss function is defined as This loss function



 originated as the negative log-likelihood of the product of Bernoulli distributions. It is also known ascross-entropyloss since we arrive at the logistic loss by calculating the cross-entropy (difference between two probability distributions) loss, which is a measure of the divergence between the predicted probability distribution and the true distribution. Logistic loss functions are preferred over the hinge loss when the scientist is mostly interested in the probabilities of success rather than in just the hard classifications. For example, when a scientist is interested in the probability that a patient can get cancer as a function of a set of covariates, the logistic loss is preferred since it allows calculating true probabilities. When the number of classes is more than two according to Patterson and Gibson (2017), that is, when we are in the presence of categorical data, the loss function is known as categorical cross-entropy



 and is equal to This loss function is built as the minus log-likelihood of a Poisson distribution and is appropriate for predicting count outcomes. It is defined as Also, for count data



 the loss function can be obtained under a negative binomial distribution, which can do a better job than the Poisson distribution when the assumption of equal mean and variance is hard to justify. Regularization






 is a method that helps to reduce the complexity of the model and significantly reduces the variance of statistical machine learning models without any substantial increase in their bias. For this reason, to prevent overfitting and improve the generalizability of our models, we use regularization (penalization), which is concerned with reducing testing errors so that the model performs well on new data as well as on training data. Regularized or penalized loss functions are those that instead of minimizing the conventional loss function,L(w), minimize an augmented loss function that consists of the sum of the conventional loss function and a penalty (or regularization) term that is a function of the weights. This is defined as whereL(w,Î»)Â is the regularized (or penalized) loss function,Î»is the degree or strength of the penalty term, andEPis the penalization proposed for the weights; this is known as the regularization term. The regularization term shrinks the weight estimates toward zero, which helps to reduce the variance of the estimates and increase the bias of the weights, which in turn helps to improve the out-of-sample predictions of statistical machine learning models (James et al.2013). As you remember, the way to introduce the penalization term is using exactly the same logic used in Ridge regression in Chap.3. Depending on the form ofEP, there is a name for the type of regularization. For example, whenEP=wTw, it is called Ridge penalty or weight decay penalty. This regularization is also called L2 penalty and has the effect that larger weights (positive or negative) result in larger penalties. On the other hand, when\( {E}_P={\sum}_{p=1}^P\left|{w}_p\right| \), that is, when theEPterm is equal to the sum of the absolute weights, the name of this regularization is Least Absolute Shrinkage and Selection Operator (Lasso) or simply L1 regularization. The L1 penalty produces a sparse solution (more zero weights) because small and larger weights are equally penalized and force some weights to be exactly equal to zero when theÎ»is considerably large (James et al.2013; Wiley2016); for this reason, the Lasso penalization also performs variable selection and provides a model more interpretable than the Ridge penalty. By combining Ridge (L2) and Lasso (L1) regularization, we obtained Elastic Net regularization, where the loss function is defined as\( L\left(\boldsymbol{w},{\lambda}_1,{\lambda}_2\right)=L\left(\boldsymbol{w}\right)+0.5\times {\lambda}_1{\sum}_{p=1}^P\left|{w}_p\right|+0.5\times {\lambda}_2{\sum}_{p=1}^P{w}_p^2 \), and where instead of one lambda parameter, two are needed. It is important to point out that more than one hyperparameter



 is needed in ANN and DL models where different degrees of penalties can be applied to different layers and different hyperparameters. This differential penalization is sometimes desirable to improve the predictions in new data, but this has the disadvantage that more hyperparameters need to be tuned, which increases the computation cost of the optimization process (Wiley2016). In all types of regularization, whenÎ»=Â 0 (orÎ»1=Î»2=Â 0), the penalty term has no effect, but the larger the value ofÎ», the more the shrinkage and penalty grows and the weight estimates will approach zero. The selection of the appropriate value ofÎ»is challenging and critical; for this reason,Î»is also treated as a hyperparameter that needs to be tuned and is usually optimized by evaluating a range of possibleÎ»values through cross-validation. It is also important to point out that scaling the input data before implementing artificial neural networks is recommended, since the effect of the penalty depends on the size of the weights and the size of the weights depends on the scale of the data. Also, the user needs to recall from Chap.3where Ridge regression was presented, that the shrinkage penalty is applied to all the weights except the intercept or bias terms (Wiley2016). Another type of regularization



 that is very popular in ANN and DL is the dropout, which consists of setting to zero a random fraction (or percentage) of the weights of the input neurons or hidden neurons. Suppose that our original topology is like the topology given in Fig.10.16.16a, where all the neurons are active (with weights different to zero), while when a random fraction of neurons is dropped out, this means that all its connections (weights) are set to zero and the topology with the dropout neurons (with weights set to zero) is observed in Fig.10.16b. The contribution of those dropped out neurons to the activation of downstream neurons is temporarily removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. Dropout is only used during the training



 of a model but not when evaluating the skill of the model; it prevents the units from co-adapting too much. Feedforward neural network with four layers. (a) Three input neurons, four neurons in hidden layers 1 and 3, and five neurons in hidden layer 2 without dropout and (b) the same network with dropout; dropping out one in the input neuron, three neurons in hidden layers 1â€“3 This type of regularization



 is very simple and there is a lot of empirical evidence of its power to avoid overfitting. This regularization is quite new in the context of statistical machine learning and was proposed by Srivastava et al. (2014) in the paperDropout: A Simple Way to Prevent Neural Networks from Overfitting. There are no unique rules to choose the percentage of neurons that will be dropped out. Some tips are given below to choose the % dropout: Usually a good starting point is to use 20% dropout, but values between 20% and 50% are reasonable. A percentage that is too low has minimal effect and a value that is too high results in underfitting the network. The larger the network, the better, when you use the dropout method, since then you are more likely to get a better performance, because the model had more chance to learn independent representations. Application






 of dropout is not restricted to hidden neurons; it can also be applied in the input layer. In both cases, there is evidence that it improves the performance of the ANN model. When using dropout, increasing the learning rate (learning rate is atuning parameterin anoptimization algorithmthat regulates the step size at each epoch (iteration) while moving toward a minimum (or maximum) of aloss function) of the ANN algorithm by a factor of 10â€“100 is suggested, as well as increasing the momentum value (another tuning parameter useful for computing the gradient at each iteration), for example, from 0.90 to 0.99. When dropout is used, it is also a good idea to constrain the size of network weights, since the larger the learning rate, the larger the network weights. For this reason, constraining the size of network weights to less than five in absolute values with max-norm regularization has shown to improve results. It is important to point out that all the loss functions



 described in the previous section can be converted to regularized (penalized) loss functions using the elements given in this section. The dropout method can also be implemented with any type of loss function. During the training



 process, the ANN and DL models learn in stages, from simple realizations to complex mapping functions. This process is captured by monitoring the behavior of the mean squared error that compares the match between observed and predicted values, which starts decreasing rapidly by increasing the number of epochs (epoch refers to one cycle through the full training data set) used for training, then decrease slowly when the error surface is close to a local minimum. However, to attain the larger generalization power of a model, it is necessary to figure out when it is best to stop training, which is a very challenging situation since a very early stopping point can produce underfitting, while a very late (no large) stopping point can produce overfitting of the training data. As mentioned in Chap.4, one way to avoid overfitting is to use a CV strategy, where the training set is split into a training-inner and testing-inner set; with the training-inner set, the model is trained for the set of hyperparameters, and with the testing-inner (tuning) set, the power to predict out of sample data is evaluated, and in this way the optimal hyperparameters are obtained. However, we can incorporate the early stopping method to CV to fight better overfitting by using the CV strategy in the usual way with a minor modification, which consists of stopping the training section periodically (i.e., every so many epochs) and testing the model on the validation subset, after reaching the specified number of epochs (Haykin2009). In other words, the stopping method combined with the CV strategy that consists of a periodic â€œestimation-followed-by-validation processâ€ basically proceeds as follows: After a period of estimation (training)â€”every three epochs, for exampleâ€”the weights and bias (intercept) parameters of the multilayer perceptron are all fixed, and the network is operated in its forward mode. Then the training and validation error are computed. When the validation



 prediction performance is completed, the estimation (training) is started again for another period, and the process is repeated. Due to its nature (just described above), which is simple to understand and easy to implement in practice, this method is calledearly stopping method of training.To better understand this method, in Fig.10.17this approach is conceptualized with two learning curves, one for the training subset and the other for the validation subset. Figure10.17shows that the prediction power in terms of MSE is lower in the training set than in the validation set, which is expected. Theestimation learning curvethat corresponds to the training set decreases monotonically as the number of epochs increases, which is normal, while the validation learning curve decreases monotonically to a minimum and then as the training continues, starts to increase. However, the estimation learning curve of the training set suggests that we can do better by going beyond the minimum point on the validation learning curve, but this is not really true since in essence what is learned beyond this point is the noise contained in the training data. For this reason, the minimum point on the validation learning curve could be used as a sensible criterion for stopping the training session. However, the validation sample error doesnotevolve as smoothly as the perfect curve shown in Fig.10.17, over the number of epochs used for training, since the validation sample error many times exhibits few local minima of its own before it starts to increase with an increasing number



 of epochs. For this reason, in the presence of two or more local minima, the selection of a â€œslowerâ€ stopping criterion (i.e., a criterion that stops later than other criteria) makes it possible to attain a small improvement in generalization performance (typically, about 4%, on average) at the cost of a much longer training period (about a factor of four, on average). Schematic representation



 of the early stopping rule based on cross-validation (Haykin2009) The training process

 of ANN, which consists of adjusting connection weights, requires a lot of computational resources. For this reason, although they had been studied for many decades, few real applications of ANN were available until the mid-to-late 1980s, when the backpropagation method made its arrival. This method is attributed to Rumelhart et al. (1986). It is important to point out that, independently, other research teams around the same time published the backpropagation algorithm, but the one previously mentioned is one of the most cited. This algorithm led to the resurgence of ANN after the 1980s, but this algorithm is still considerably slower than other statistical machine learning algorithms. Some advantages of this algorithm are (a) it is able to make predictions of categorical or continuous outcomes, (b) it does a better job in capturing complex patterns than nearly any other algorithm, and (c) few assumptions about the underlying relationships of the data are made. However, this algorithm is not without weaknesses, some of which are (a) it is very slow to train since it requires a lot of computational resources because the more complex the network topology, the more computational resources are needed, this statement is true not only for ANN but also for any algorithm, (b) it is very susceptible to overfitting training data, and (c) its results are difficult to interpret (Lantz2015). Next, we provide the derivation

 of the backpropagation algorithm for the multilayer perceptron network shown in Fig.10.18. Schematic representation of a multilayer feedforward network with one hidden layer, eight input variables, and three output variables As mentioned earlier, the goal of the backpropagation

 algorithm is to find the weights of a multilayered feedforward network. The multilayered feedforward network given in Fig.10.18is able to approximate any function to any degree of accuracy (Cybenko1989) with enough hidden units, as stated in the universal approximation theorem (Sect.10.4), which makes the multilayered feedforward network a powerful statistical machine learning tool. Suppose that we provide this network withninput patterns of the form wherexidenotes the input pattern

 of individualiwithi=Â 1, â€¦,n, andxipdenotes the inputpth ofxi. Letyijdenote the response variable of theith individual for thejth output and this is associated with the input patternxi. For this reason, to be able to train the neural network, we must learn the functional relationship between the inputs and outputs. To illustrate the learning process of this relationship, we use the SSE loss function (explained in the section about â€œloss functionsâ€ to optimize the weights) which is defined as Now, to explain how the backpropagation algorithm works, we will explain how information is first passed forward through the network. Providing the input values to the input layer is the first step, but no operation is performed on this information since it is simply passed to the hidden units. Then the net input into thekth hidden neuron is calculated as HerePis the total number of explanatory variables or input nodes,\( {w}_{kp}^{(h)} \)is the weight from input unitpto hidden unitk, the superscript,h, refers to hidden layer, andxipis the value of thepth input for pattern or individuali. It is important to point out that the bias term (\( {b}_j^{(h)} \)) of neuronkin the hidden layer has been excluded from (10.6) because the bias can be accounted for by adding an extra neuron

 to the input layer and fixing its value at 1. Then the output of thekneuron resulting from applying an activation function to its net input is whereg(h)is the activation function that is applied to the net input of any neuronkof the hidden layer. In a similar vein, now with all the outputs of the neurons in the hidden layer, we can estimate the net input of thejth neuron of the output unitjas whereMis the number of neurons

 in the hidden layer and\( {w}_{jk}^{(l)} \)represents the weights from hidden unitkto outputj. The superscript,l,refers to output layer. Also, here the bias term (\( {b}_j^{(l)}\Big) \)of neuronjin the output layer was not included in (10.8) since it can be included by adding an extra neuron to the hidden layer and fixing its value at 1. Now, by applying the activation function to the output of thejthÂ neuron of the output layer, we get the predicted value of thejth output as where\( {\hat{y}}_{ij} \)is the predicted value of individualiin outputjandg(l)is the activation function of the output layer. We are interested in learning the weights (\( {w}_{kp}^{(h)},{w}_{jk}^{(l)} \)) that minimize the sum of squared errors known as the mean square loss function (10.5), which is a function of the unknown weights, as can be observed in (10.6)â€“(10.8). Therefore, the partial derivatives of the loss function with respect to the weights represent the rate of change of the loss function with respect to the weights (this is the slope of the loss function). The loss function will decrease when moving the weights down this slope. This is the intuition behind the iterative method called backpropagation for finding the optimal weights and biases. This method consists of evaluating the partial derivatives of the loss function with regard to the weights and then moving these values down the slope, until the score of the loss function no longer decreases. For example, if we make the variation of the weights proportional to the negative of the gradient, the change in the weights in the right direction is reached. The gradient of the loss function given in (10.5) with respect to the weights connecting the hidden units to the output units (\( {w}_{jk}^{(l)}\Big) \)is given by whereÎ·is the learning rate that scales

 the step size and is specified by the user. To be able to calculate the adjustments for the weights connecting the hidden neurons to the outputs,\( {w}_{jk}^{(l)} \), first we substitute (10.6)â€“(10.9) in (10.5), which yields Then, by expanding (10.10) using the change rule, we get Next, we get each partial derivative By substituting these partial derivatives

 in (10.10), we obtain the change in weights from the hidden units to the output units,\( \Delta {w}_{jk}^{(l)}, \)as where\( {\delta}_{ij}=\left({y}_{ij}-{\hat{y}}_{ij}\right)\ {g}^{(l)\acute{\mkern6mu}}\left({z}_{ij}^{(l)}\right) \). Therefore, the formula used to update the weights from the hidden units to the output units is This equation reflects that the adjusted weights from (10.13) are added to the current estimate of the weights,\( {w}_{jk}^{(l)(t)} \), to obtain the updated estimates,\( {w}_{jk}^{(l)\left(t+1\right)} \). Next, to update the weights

 connecting the input units to the hidden units, we follow a similar process as in (10.12). Thus Using the chain rule, we get that where\( \frac{\partial E}{\partial {\hat{y}}_{ij}} \)and\( \frac{\partial {\hat{y}}_{ij}}{\partial {z}_{ij}^{(l)}} \)are given in (10.11), while Substituting back into (10.14), we obtain the change in the weights from the input units to the hidden units,\( \Delta {w}_{kp}^{(h)} \), as where\( {\psi}_{ik}={\sum}_{j=1}^L{\delta}_{ij}{w}_{jk}^{(l)}{g}^{(h)\acute{\mkern6mu}}\left({z}_{ik}^{(h)}\right) \). The summation over the number of output units is because each hidden neuron

 is connected to all the output units. Therefore, all the outputs should be affected if the weight connecting an input unit to a hidden unit changes. In a similar way, the formula for updating the weights from the input units to the hidden units is This equation also reflects that the adjusted weights from (10.17) are added to the current estimate of the weights,\( {w}_{kp}^{(h)(t)} \), to obtain the updated estimates,\( {w}_{kp}^{(h)\left(t+1\right)} \). Now we are able to put down the processing steps needed to compute the change in the network weights using the backpropagation

 algorithm. We definewas the entire collection of weights. Step 1. Initialize the weights


 to small random values, and define the learning rate (Î·) and the minimum expected loss score (tol). By tol we can fix a small value that when this value is reached, the training process will stop. Step 2. If the stopping condition is false, perform steps 3â€“14. Step 3. Select a patternxi=Â [xi1,â€‰â€¦,xiP]Tas the input vector sequentially (i=Â 1 till the number of samples) or at random. Step 4. The net inputs of the hidden layer are calculated:\( {z}_{ik}^{(h)}={\sum}_{p=0}^P{w}_{kp}^{(h)}{x}_{ip} \),i=Â 1, â€¦,nandk=Â 0, â€¦,M. Step 5. The outputs of the hidden layer are calculated:\( {V}_{ik}^{(h)}={g}^{(h)}\left({z}_{ik}^{(h)}\right) \) Step 6. The net inputs of the output layer are calculated:\( {z}_{ij}^{(l)}={\sum}_{k=0}^M{w}_{jk}^{(l)}{V}_{ik}^{(h)},j=1,\dots, L \) Step 7. The predicted values


 (outputs) of the neural network are calculated:\( {\hat{y}}_{ij}={g}^{(l)}\left({z}_{ij}^{(l)}\right) \) Step 8. Compute the mean square error (loss function) for patternierror:\( {E}_i=\frac{1}{2 nL}{\sum}_{j=1}^L{\left({\hat{y}}_{ij}-{y}_{ij}\right)}^2+{E}_i \); thenE(w)Â =Ei+E(w);in the first step of an epoch, initializeEi=Â 0. Note that the value of the loss function is accumulated over all data pairs, that is, (yij, xi). Step 9. The output errors are calculated:\( {\delta}_{ij}=\left({y}_{ij}-{\hat{y}}_{ij}\right)\ {g}^{(l)\acute{\mkern6mu}}\left({z}_{ij}^{(l)}\right) \) Step 10. The hidden layer errors are calculated:\( {\psi}_{ik}={g}^{(h)\acute{\mkern6mu}}\left({z}_{ik}^{(h)}\right){\sum}_{j=1}^L{\delta}_{ij}{w}_{jk}^{(l)} \) Step 11. The weights of the output layer are updated:\( {w}_{jk}^{(l)\left(t+1\right)}={w}_{jk}^{(l)(t)}+{\eta \delta}_{ij}{V}_{ik}^{(h)} \) Step 12. The weights of the hidden layer are updated:\( {w}_{kp}^{(h)\left(t+1\right)}={w}_{kp}^{(h)(t)}+{\eta \psi}_{ik}{x}_{ip} \) Step 13. Ifi<n, go to step 3; otherwise go to step 14. Step 14. Once the learning of an epoch is complete,i=n; then we check if the global error is satisfied with the specified tolerance (tol). If this condition is satisfied we terminate the learning process which means that the network has been trained satisfactorily. Otherwise, go to step 3 and start a new learning epoch:i=Â 1, sinceE(w)<tol. The backpropagation algorithm is iterative. This means that the search process occurs over multiple discrete steps, each step hopefully slightly improving the model parameters. Each step involves using the model with the current set of internal parameters to make predictions of some samples, comparing the predictions to the real expected outcomes, calculating the error, and using the error to update the internal model parameters. This update procedure is different for different algorithms, but in the case of ANN, as previously pointed out, the backpropagation update algorithm


 is used. In this section, we provide a simple example that will be computed step by step by hand to fully understand how the training


 is done using the backpropagation method. The topology used for this example is given in Fig.10.19. A simple artificial neural network with one input, one hidden layer with one neuron, and one response variable (output) The data set for this example is given in Table10.1, where we can see that the data collected consist of four observations, the response variable (y) takes values between 0 and 1, and the input information is for only one predictor (x). Additionally, Table10.1gives the starting values for the hidden weights (\( {w}_{kp}^{(h)}\Big) \)and for the output weights (\( {w}_{jk}^{(l)} \)). It is important to point out that due to the fact that the response variable is in the interval between zero and one, we will use the sigmoid activation function for both the hidden layer and the output layer. A learning rate (Î·) equal to 0.1 and tolerance equal to 0.025 were also used. The backpropagation algorithm described before was given for one input pattern at a time; however, to simplify the calculations, we will implement this algorithm using the four patterns of data available simultaneously using matrix calculations. For this reason, first we build the design matrix of inputs and outputs: We also define the vectors of the starting values of the hidden and output weights: Here we can see thatP=Â 1,andM=Â 2.Â Next we calculate the net inputs for the hidden layer as Now the output for the hidden layer


 is calculated using the sigmoid activation function where\( {V}_{ik}^{(h)}={g}^{(h)}\left({z}_{ik}^{(h)}\right),i=1,\dots, 4 \)andg(h)(z)Â =Â 1/(1Â +Â Â expÂ (âˆ’z)), which can be replaced by another desired activation function. Then the net inputs for the output layer are calculated as follows: The predicted values (outputs) of the neural network are calculated as where\( {\hat{y}}_i={g}^{(l)}\left({z}_{i1}^{(l)}\right),i=1,\dots, 4 \)andg(l)(z)Â =Â 1/(1Â +Â Â expÂ (âˆ’z)). Next the output errors are calculated using the Hadamard


 product, âˆ˜, (element-wise matrix multiplication) as The hidden layer errors are calculated as where\( {\boldsymbol{w}}_1^{(l)} \)isw(l)without the weight of the intercept, that is, without the first element. The weights


 of the output layer are updated: where 2 denotes that the output weights are for epoch number 2. Then the weights for epoch 2 of the hidden layer are obtained with We check to see if the global error


 is satisfied with the specified tolerance (tol). Since\( E\left(\boldsymbol{w}\right)=\frac{1}{2n}{\sum}_{i=1}^n{\left({\hat{y}}_i-{y}_i\right)}^2=0.03519>\mathrm{tol}=0.025, \)this means that we need to increase the number of epochs to satisfy the tolÂ =Â 0.025 specified. Epoch 2. Using the updated weights of epoch 1, we obtain the new weights after epoch 2. First for the output layer: And next for the hidden layer: Now the predicted values are\( {\hat{y}}_1=0.8233 \),\( {\hat{y}}_2=0.3754 \),\( {\hat{y}}_3=0.8480 \), and\( {\hat{y}}_4=0.2459 \), and again we found that\( E\left(\boldsymbol{w}\right)=\frac{1}{2n}{\sum}_{i=1}^n{\left({\hat{y}}_i-{y}_i\right)}^2=0.03412>\mathrm{tol}=0.025. \)This means that we need to continue the number of epochs


 to be able to satisfy the tolÂ =Â 0.025 specified. The learning process by decreasing the MSE is observed in Fig.10.20, where we can see that tolÂ =Â 0.025 is reached in epoch number 13, with an MSEÂ =E(w)=0.02425. Behavior of the learning process by monitoring the MSE for Example 10.1â€”a hand computation Table10.2gives the information for this example; the data collected contain five observations, the response variable (y) has a value between âˆ’1 and 1, and there are three inputs (predictors). Table10.2also provides the starting values for the hidden weights (\( {w}_{kp}^{(h)}\Big) \)and for the output weights (\( {w}_{jk}^{(l)} \)). Due to the fact that the response variable is in the interval between âˆ’1 and 1, we will use the hyperbolic tangent activation function (Tanh) for the hidden and output layers. Now we used a learning rate (Î·) equal to 0.05 and a tolerance equal to 0.008 (Fig.10.21). A simple artificial neural network


 with three inputs, one hidden layer with two neurons, and one response variable (output) Here the backpropagation algorithm


 was implemented using the five patterns of data simultaneously using matrix calculations. Again, first we represent the design matrix of inputs and outputs: Then we define the vectors of starting values of the hidden (w(h))Â and output (w(l)) weights: NowP=Â 3Â andM=Â 3.Â Next, we calculate the net inputs for the hidden layer as Now with thetanhactivation function, the output of the hidden layer is calculated: Again\( {V}_{ik}^{(h)}={g}^{(h)}\left({z}_{ik}^{(h)}\right),i=1;k=1,2 \), and\( {g}^{(h)}(z)=\tanh (z)=\frac{\exp (z)-\exp \left(-z\right)}{\exp (z)+\exp \left(-z\right)} \), which can also be replaced by another activation


 function. Then the net inputs for the output layer are calculated as The predicted values (outputs) of the neural network are calculated as where\( {\hat{y}}_{ij}={g}^{(l)}\left({z}_{i1}^{(l)}\right),i=1,\dots, 5 \)and\( {g}^{(l)}(z)=\tanh (z)=\frac{\exp (z)-\exp \left(-z\right)}{\exp (z)+\exp \left(-z\right)} \). The output errors are calculated as The hidden layer errors


 are calculated as where\( {\boldsymbol{w}}_1^{(h)} \)isw(h)without the weights of the intercepts, that is, without the first row. The weights of the output layer


 are updated: Number 2 inw(l)(2)indicates that output weights are for epoch number 2. The weights of the hidden layer in epoch 2 are obtained with We check to see if the global errors


 are satisfied with the specified tolerance (tol).\( E\left(\boldsymbol{w}\right)=\frac{1}{2n}{\sum}_{i=1}^n{\left({\hat{y}}_i-{y}_i\right)}^2=0.01104>\mathrm{tol}=0.008 \)which means that we have to continue with the next epoch by cycling the training data again. Epoch 2. Using the updated weights of epoch 1, we obtain the new weights for epoch 2. For the output layer, these are While for the hidden


 layer, they are Now the predicted values are\( {\hat{y}}_1=0.6372 \),\( {\hat{y}}_2=0.2620 \),\( {\hat{y}}_3=-0.6573 \),\( {\hat{y}}_4=0.6226, \)\( {\hat{y}}_5=-0.9612, \)and the\( E\left(\boldsymbol{w}\right)=\frac{1}{2n}{\sum}_{i=1}^n{\left({\hat{y}}_i-{y}_i\right)}^2=0.01092>\mathrm{tol}=0.008, \)which means that we have to continue with the next epoch by cycling


 the training data again. Figure10.22shows that theE(w)Â =Â 0.00799Â <Â tolÂ =Â 0.008 until epoch 83. Behavior of the learning process by monitoring the MSE for Example 10.2â€”a hand computation In this algorithm, zero weights are not an option because each layer is symmetric in the weights flowing to the different neurons. Then the starting values should be close to zero and can be taken from random uniform or Gaussian distributions (Efron and Hastie2016). One of the disadvantages of the basic backpropagation


 algorithm just described above is that the learning parameterÎ·is fixed. Alipanahi B, Delong A, Weirauch MT, Frey BJ (2015) Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning. Nat Biotechnol 33:831â€“838 ArticleCASGoogle Scholar Anderson J, Pellionisz A, Rosenfeld E (1990) Neurocomputing 2: directions for research. MIT, Cambridge Google Scholar Angermueller C, PÃ¤rnamaa T, Parts L, Stegle O (2016) Deep learning for computational biology. Mol Syst Biol 12(878):1â€“16 Google Scholar Chollet F, Allaire JJ (2017) Deep learning with R. Manning Publications, Manning Early Access Program (MEA), 1st edn Google Scholar Cole JH, Rudra PK, Poudel DT, Matthan WA, Caan CS, Tim D, Spector GM (2017) Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker. NeuroImage 163(1):115â€“124.https://doi.org/10.1016/j.neuroimage.2017.07.059 ArticlePubMedGoogle Scholar Cybenko G (1989) Approximations by superpositions of sigmoidal functions. Math Control Signal Syst 2:303â€“314 ArticleGoogle Scholar Dingli A, Fournier KS (2017) Financial time series forecastingâ€”a deep learning approach. Int J Mach Learn Comput 7(5):118â€“122 ArticleGoogle Scholar Dougherty G (2013) Pattern recognition and classification-an introduction. Springer Science + Business Media, New York BookGoogle Scholar Efron B, Hastie T (2016) Computer age statistical inference. Algorithms, evidence, and data science. Cambridge University Press, New York BookGoogle Scholar Francisco-Caicedo EF, LÃ³pez-Sotelo JA (2009) Una approximaciÃ³n prÃ¡ctica a las redes neuronales artificiales. Universidad del Valle, Cali BookGoogle Scholar Goldberg Y (2016) A primer on neural network models for natural language processing. J Artif Intell Res 57(345):420 Google Scholar Haykin S (2009) Neural networks and learning machines, 3rd edn. Pearson Prentice Hall, New York Google Scholar Hornik K (1991) Approximation capabilities of multilayer feedforward networks. Neural Netw 4:251â€“257 ArticleGoogle Scholar James G, Witten D, Hastie T, Tibshirani R (2013) An introduction to statistical learning with applications in R. Springer, New York BookGoogle Scholar Kohonen T (2000) Self-organizing maps. Springer, Berlin Google Scholar Lantz B (2015) Machine learning with R, 2nd edn. Packt Publishing Ltd, Birmingham Google Scholar LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436â€“444 ArticleCASGoogle Scholar Lewis ND (2016) Deep learning made easy with R. A gentle introduction for data science. CreateSpace Independent Publishing Platform Google Scholar Liu S, Tang J, Zhang Z, Gaudiot JL (2017) CAAD: computer architecture for autonomous driving. ariv preprint ariv:1702.01894 Google Scholar Ma W, Qiu Z, Song J, Cheng Q, Ma C (2017) DeepGS: predicting phenotypes from genotypes using Deep Learning. bioRxiv 241414.https://doi.org/10.1101/241414 Makridakis S, Spiliotis E, Assimakopoulos V (2018) Statistical and Machine Learning forecasting methods: concerns and ways forward. PLoS One 13(3):e0194889.https://doi.org/10.1371/journal.pone.0194889 ArticleCASPubMedPubMed CentralGoogle Scholar McCulloch WS, Pitts W (1943) A logical calculus of the ideas immanent in nervous activity. Bull Math Biophys 5:115â€“133 ArticleGoogle Scholar McDowell R, Grant D (2016) Genomic selection with deep neural networks. Graduate Theses and Dissertations, p 15973.https://lib.dr.iastate.edu/etd/15973 Menden MP, Iorio F, Garnett M, McDermott U, Benes CH et al (2013) Machine learning prediction of cancer cell sensitivity to drugs based on genomic and chemical properties. PLoS One 8:e61318 ArticleCASGoogle Scholar Montesinos-LÃ³pez A, Montesinos-LÃ³pez OA, Gianola D, Crossa J, HernÃ¡ndez-SuÃ¡rez CM (2018a) Multi-environment genomic prediction of plant traits using deep learners with a dense architecture. G3: Genes, Genomes, Genetics 8(12):3813â€“3828.https://doi.org/10.1534/g3.118.200740 ArticleGoogle Scholar Montesinos-LÃ³pez OA, Montesinos-LÃ³pez A, Crossa J, Gianola D, HernÃ¡ndez-SuÃ¡rez CM et al (2018b) Multi-trait, multi-environment deep learning modeling for genomic-enabled prediction of plant traits. G3: Genes, Genomes, Genetics 8(12):3829â€“3840.https://doi.org/10.1534/g3.118.200728 ArticleGoogle Scholar Montesinos-LÃ³pez OA, Vallejo M, Crossa J, Gianola D, HernÃ¡ndez-SuÃ¡rez CM, Montesinos-LÃ³pez A, Juliana P, Singh R (2019a) A benchmarking between deep learning, support vector machine and bayesian threshold best linear unbiased prediction for predicting ordinal traits in plant breeding. G3: Genes, Genomes, Genetics 9(2):601â€“618 ArticleGoogle Scholar Montesinos-LÃ³pez OA, MartÃ­n-Vallejo J, Crossa J, Gianola D, HernÃ¡ndez-SuÃ¡rez CM, Montesinos-LÃ³pez A, Juliana P, Singh R (2019b) New deep learning genomic prediction model for multi-traits with mixed binary, ordinal, and continuous phenotypes. G3: Genes, Genomes, Genetics 9(5):1545â€“1556 ArticleGoogle Scholar Montesinos-LÃ³pez OA, Montesinos-LÃ³pez A, PÃ©rez-RodrÃ­guez P, BarrÃ³n-LÃ³pez JA, Martini JWR, Fajardo-Flores SB, Gaytan-Lugo LS, Santana-Mancilla PC, Crossa J (2021) A review of deep learning applications for genomic selection. BMC Genomics 22:19 ArticleGoogle Scholar Patterson J, Gibson A (2017) Deep learning: a practitionerâ€™s approach. Oâ€™Reilly Media Google Scholar Ripley B (1993) Statistical aspects of neural networks. In: Bornndorff-Nielsen U, Jensen J, Kendal W (eds) Networks and chaosâ€”statistical and probabilistic aspects. Chapman and Hall, London, pp 40â€“123 ChapterGoogle Scholar Rouet-Leduc B, Hulbert C, Lubbers N, Barros K, Humphreys CJ et al (2017) Machine learning predicts laboratory earthquakes. Geophys Res Lett 44(28):9276â€“9282 ArticleGoogle Scholar Rumelhart DE, Hinton GE, Williams RJ (1986) Learning internal representations by backpropagating errors. Nature 323:533â€“536 ArticleGoogle Scholar Shalev-Shwartz, Ben-David (2014) Understanding machine learning: from theory to algorithms. Cambridge University Press, New York BookGoogle Scholar Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R (2014) Dropout: a simple way to prevent neural networks from overfitting. J Mach Learn Res 15(6):1929â€“1958 Google Scholar Tavanaei A, Anandanadarajah N, Maida AS, Loganantharaj R (2017) A deep learning model for predicting tumor suppressor genes and oncogenes from PDB structure. bioRiv 177378.https://doi.org/10.1101/177378 Weron R (2014) Electricity price forecasting: a review of the state-of-the-art with a look into the future. Int J Forecast 30(4):1030â€“1081 ArticleGoogle Scholar Wiley JF (2016) R deep learning essentials: build automatic classification and prediction models using unsupervised learning. Packt Publishing, Birmingham, Mumbai Google Scholar Download references Facultad de TelemÃ¡tica, University of Colima, Colima, MÃ©xico Osval Antonio Montesinos LÃ³pez Departamento de MatemÃ¡ticas, University of Guadalajara, Guadalajara, MÃ©xico Abelardo Montesinos LÃ³pez Biometrics and Statistics Unit, CIMMYT, Edo de MÃ©xico, MÃ©xico Jose Crossa Colegio de Post- Graduado, Edo de MÃ©xico, MÃ©xico Jose Crossa You can also search for this author inPubMedGoogle Scholar You can also search for this author inPubMedGoogle Scholar You can also search for this author inPubMedGoogle Scholar Open AccessThis chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Reprints and permissions Â© 2022 The Author(s) Montesinos LÃ³pez, O.A., Montesinos LÃ³pez, A., Crossa, J. (2022).  Fundamentals of Artificial Neural Networks and Deep Learning.

                     In:  Multivariate Statistical Machine Learning Methods for Genomic Prediction. Springer, Cham. https://doi.org/10.1007/978-3-030-89010-0_10 DOI:https://doi.org/10.1007/978-3-030-89010-0_10 Published:14 January 2022 Publisher Name:Springer, Cham Print ISBN:978-3-030-89009-4 Online ISBN:978-3-030-89010-0 eBook Packages:Biomedical and Life SciencesBiomedical and Life Sciences (R0) Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article.  Provided by the Springer Nature SharedIt content-sharing initiative Policies and ethics 112.134.157.129 Not affiliated Â© 2024 Springer Nature"
https://developers.google.com/machine-learning/crash-course/neural-networks," This module assumes you are familiar with the concepts covered in the
     following modules: You may recall from theFeature cross exercisesin theCategorical data module,
that the following classification problem is nonlinear: ""Nonlinear"" means that you can't accurately predict a label with a
model of the form \(b + w_1x_1 + w_2x_2\). In other words, the
""decision surface"" is not a line. However, if we perform a feature cross on our features $x_1$ and $x_2$, we can
then represent the nonlinear relationship between the two features using alinear model:
$b + w_1x_1 + w_2x_2 + w_3x_3$ where $x_3$ is the feature cross between
$x_1$ and $x_2$: Now consider the following dataset: You may also recall from theFeature cross exercisesthat determining the correct feature crosses to fit a linear model to this data
took a bit more effort and experimentation. But what if you didn't have to do all that experimentation yourself?Neural networksare a family
of model architectures designed to findnonlinearpatterns in data. During training of a neural network, themodelautomatically
learns the optimal feature crosses to perform on the input data to minimize
loss. In the following sections, we'll take a closer look at how neural networks work. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-10-09 UTC."
https://www.techtarget.com/searchenterpriseai/definition/neural-network,"A neural network is a machine learning (ML) model designed to process data in a way that mimics the function and structure of the human brain. Neural networks are intricate networks of interconnected nodes, or artificial neurons, that collaborate to tackle complicated problems. Also referred to as artificial neural networks (ANNs), neural nets or deep neural networks, neural networks represent a type ofdeep learningtechnology that's classified under the broader field of artificial intelligence (AI). Neural networks are widely used in a variety of applications, includingimage recognition, predictive modeling, decision-making and natural language processing (NLP). Examples of significant commercial applications over the past 25 years include handwriting recognition for check processing, speech-to-text transcription, oil exploration data analysis, weather prediction andfacial recognition. An ANN usually involves manyprocessorsoperating in parallel and arranged in tiers or layers. There are typically three layers in a neural network: an input layer, an output layer and several hidden layers. The first tier -- analogous to optic nerves in human visual processing -- receives the raw input information. Each successive tier receives the output from the tier preceding it rather than the raw input, the same way biological neurons further from the optic nerve receive signals from those closer to it. The last tier produces the system's output. This article is part of Each processing node has its own small sphere of knowledge, including what it has seen and any rules it was originally programmed with or developed for itself. The tiers are highly interconnected, which means each node in TierNwill be connected to many nodes in TierN-1-- its inputs -- and in TierN+1, which provides input data for the TierN-1nodes. There could be one or more nodes in the output layer, from which the answer it produces can be read. ANNs are noted for beingadaptive, which means they modify themselves as they learn from initial training, and subsequent runs provide more information about the world. The most basic learning model is centered on weighting the input streams, which is how each node measures the importance of input data from each of its predecessors. Inputs that contribute to getting the right answers are weighted higher. Image recognition was one of the first areas in which neural networks were successfully applied. But the technologyuses of neural networkshave expanded to many additional areas, including the following: Prime uses involve any process that operates according to strict rules or patterns and has large amounts of data. If the data involved is too large for a human to make sense of in a reasonable amount of time, the process is likely a prime candidate for automation through artificial neural networks. Typically, an ANN is initially trained, or fed large amounts of data. Training consists of providing input and telling the network what the output should be. For example, to build a network that identifies the faces of actors, the initial training might be a series of pictures, including actors, non-actors, masks, statues and animal faces. Each input is accompanied by matching identification, such as actors' names or ""not actor"" or ""not human"" information. Providing the answers enables the model to adjust its internal weightings to do its job better. For example, if nodes David, Dianne and Dakota tell node Ernie that the current input image is a picture of Brad Pitt, but node Durango says it's George Clooney, and the training program confirms it's Pitt, Ernie decreases the weight it assigns to Durango's input and increases the weight it gives to David, Dianne and Dakota. In defining the rules and making determinations -- the decisions of each node on what to send to the next layer based on inputs from the previous tier -- neural networks use several principles. These include gradient-based training,fuzzy logic, genetic algorithms and Bayesian methods. They might be given some basic rules about object relationships in the data being modeled. For example, a facial recognition system might be instructed, ""Eyebrows are found above eyes,"" or ""Mustaches are below a nose. Mustaches are above and/or beside a mouth."" Preloading rules can make training faster and the model more powerful faster. But it also includes assumptions about the nature of the problem, which could prove to be either irrelevant and unhelpful, or incorrect and counterproductive, making the decision about what, if any, rules to build unimportant. Further, the assumptions people make when training algorithms cause neural networks to amplify cultural biases.Biased data sets are an ongoing challengein training systems that find answers on their own through pattern recognition in data. If the data feeding the algorithm isn't neutral -- and almost no data is -- the machine propagates bias. Neural networks are sometimes described in terms of their depth, including how many layers they have between input and output, or the model's so-called hidden layers. This is why the termneural networkis used almost synonymously withdeep learning. Neural networks can also be described by the number of hidden nodes the model has, or in terms of how many input layers and output layers each node has. Variations on the classic neural network design enable various forms of forward and backward propagation of information among tiers. Specific types of ANNs include the following: One of the simplest variants of neural networks, these pass information in one direction, through various input nodes, until it makes it to the output node. The network might or might not have hidden node layers, making their functioning more interpretable. It's prepared to process large amounts of noise. This type of ANN computational model is used in technologies such as facial recognition andcomputer vision. More complex in nature, recurrent neural networks (RNNs) save the output of processing nodes and feed the result back into the model. This is how the model learns to predict the outcome of a layer. Each node in the RNN model acts as a memory cell, continuing the computation and execution of operations. This neural network starts with the same front propagation as a feed-forward network, but then goes on to remember all processed information to reuse it in the future. If the network's prediction is incorrect, then the system self-learns and continues working toward the correct prediction duringbackpropagation. This type of ANN is frequently used in text-to-speech conversions. Convolutional neural networks (CNNs) are one of the most popular models used today. This computational model uses a variation of multilayerperceptronsand contains one or more convolutional layers that can be either entirely connected or pooled. These convolutional layers create feature maps that record a region of the image that's ultimately broken into rectangles and sent out for nonlinear processing. The CNN model is particularly popular in the realm of image recognition. It has been used in many of the most advanced applications of AI, including facial recognition, text digitization and NLP. Other use cases include paraphrase detection, signal processing andimage classification. Deconvolutional neural networksuse a reversed CNN learning process. They try to find lost features or signals that might have originally been considered unimportant to the CNN system's task. This network model can be used in image synthesis and analysis. These contain multiple neural networks working separately from one another. The networks don't communicate or interfere with each other's activities during the computation process. Consequently, complex or big computational processes can be performed more efficiently. These represent the most basic form of neural networks and were introduced in 1958 by Frank Rosenblatt, an American psychologist who's also considered to be the father of deep learning. The perceptron is specifically designed for binary classification tasks, enabling it to differentiate between two classes based on input data. Multilayer perceptron (MLP) networks consist of multiple layers of neurons, including an input layer, one or more hidden layers, and an output layer. Each layer is fully connected to the next, meaning that every neuron in one layer is connected to every neuron in the subsequent layer. This architecture enables MLPs to learn complex patterns and relationships in data, making them suitable for various classification andregression tasks. Radial basis function networks use radial basis functions as activation functions. They're typically used for function approximation, time series prediction and control systems. Transformer neural networks are reshaping NLPand other fields through a range of advancements. Introduced by Google in a 2017 paper, transformers are specifically designed to process sequential data, such as text, by effectively capturing relationships and dependencies between elements in the sequence, regardless of their distance from one another. Transformer neural networks have gained popularity as an alternative to CNNs and RNNs because their ""attention mechanism"" enables them to capture and process multiple elements in a sequence simultaneously, which is a distinct advantage over other neural network architectures. Generative adversarial networksconsist of two neural networks -- a generator and a discriminator -- that compete against each other. The generator creates fake data, while the discriminator evaluates its authenticity. These types of neural networks are widely used for generating realistic images and data augmentation processes. Artificial neural networks offer the following benefits: Along with their numerous benefits, neural networks also have some drawbacks, including the following: Thehistory of neural networksspans several decades and has seen considerable advancements. The following examines the important milestones and developments in the history of neural networks: Discover the process for building a machine learning model, including data collection, preparation, training, evaluation and iteration. Follow theseessential steps to kick-start your ML project. To help you find the right BI software for your analytics needs, here's a look at 20 top tools and the key features that ... Data preparation is a crucial but complex part of analytics applications. Don't let seven common challenges send your data prep ... A year after getting acquired by private equity firms amid declining revenue growth and a slow transition to the cloud, the ... President-elect Donald Trump has been vocal in his criticisms of big tech's content censorship power and President Joe Biden's ... Internal tech procurement can be a risky undertaking. Tech pilot programs can potentially reduce some of that risk, but IT ... Apart from President-elect Donald Trump's promise to take a strong stance on goods imported from China, collaboration might be ... Based on its AI development and expansion within data management, the record $10 billion the vendor raised shows it is viewed ... The data lakehouse pioneer has expanded into AI development and plans to use the funding to fuel further investments in AI, make ... The complementary partnership combines the data integration vendor's discovery and retrieval capabilities with the tech giant's ... The supply chain might be losing its seat at the executive table, but the work must continue to provide more resilience and ... Nothing is certain except death, taxes and supply chain disruptions. Companies should plan accordingly, especially as the ... Essential TMS features are real-time tracking, route optimization, carrier management, load planning, fleet management, automated... All Rights Reserved,Copyright 2018 - 2024, TechTargetPrivacy PolicyCookie PreferencesCookie PreferencesDo Not Sell or Share My Personal Information"
https://brilliant.org/wiki/artificial-neural-network/,"Reset passwordNew user?Sign up Existing user?Log in Already have an account?Log in here. A simple artificial neural network.  The first column of circles represents the ANN's inputs, the middle column represents computational units that act on that input, and the third column represents the ANN's output.  Lines connecting circles indicate dependencies.[1] Artificial neural networks(ANNs) are computational models inspired by the human brain.  They are comprised of a large number of connectednodes, each of which performs a simplemathematical operation.  Each node's output is determined by this operation, as well as a set of parameters that are specific to that node.  By connecting these nodes together and carefully setting their parameters, very complex functions can be learned and calculated. Artificial neural networks are responsible for many of the recent advances inartificial intelligence, including voice recognition, image recognition, androbotics.  For example, ANNs can perform image recognition on hand drawn digits.  An interactive example can be foundhere. With the advent of computers in the 1940s, computer scientists' attention turned towards developing intelligent systems that could learn to performprediction and decision making.  Of particular interest werealgorithmsthat could performonline learning, which is a learning method that can be applied to data points arriving sequentially.  This is in opposition tobatch learning, which requires that all of the data be present at the time of training. Online learning is especially useful in scenarios where training data is arriving sequentially over time, such as speech data or the movement of stock prices.  With a system capable of online learning, one doesn't have to wait until the system has received a ton of data before it can make a prediction or decision. If the human brain learned by batch learning, then human children would take 10 years before they could learn to speak, mostly just to gather enough speech data and grammatical rules to speak correctly. Instead, children learn to speak by observing the speech patterns of those around them and gradually incorporating that knowledge to improve their own speech, an example of online learning. Given that the brain is such a powerful online learner, it is natural to try to emulate it mathematically.  ANNs are one attempt at a model with the bare minimum level of complexity required to approximate the function of the human brain, and so are among the most powerful machine learning methods discovered thus far. The human brain is primarily comprised ofneurons, small cells that learn to fire electrical and chemical signals based on some function.  There are on the order of \(10^{11}\) neurons in the human brain, about \(15\) times the total number of people in the world.  Each neuron is, on average, connected to \(10000\) other neurons, so that there are a total of \(10^{15}\) connections between neurons. Neurons and microglial cells stained red and green respectively.[2] Since individual neurons aren't capable of very complicated calculations, it is thought that the huge number of neurons and connections are what gives the brain its computational power.  While there are in fact thousands of different types of neurons in the human brain, ANNs usually attempt to replicate only one type in an effort to simplify the model calculation and analysis. The electrical current for a neuron going from rest to firing to rest again.[3] Neurons function by firing when they receive enough input from the other neurons to which they're connected.  Typically, the output function is modeled as anactivation function, where inputs below a certain threshold don't cause the neuron to fire, and those above the threshold do.  Thus, a neuron exhibits what is known asall-or-nothingfiring, meaning it is either firing, or it is completely off and no output is produced. From the point of view of a particular neuron, its connections can generally be split into two classes, incoming connections and outgoing connections.  Incoming connections form the input to the neuron, while the output of the neuron flows through the outgoing connections.  Thus, neurons whose incoming connections are the outgoing connections of other neurons treat other neurons' outputs as inputs.  The repeated transformation of outputs of some neurons into inputs of other neurons gives rise to the power of the human brain, since thecompositionof activation functions can create highly complex functions. It turns out that incoming connections for a particular neuron are not considered equal.  Specifically, some incoming connections are stronger than others, and provide more input to a neuron than weak connections.  Since a neuron fires when it receives input above a certain threshold, these strong incoming connections contribute more to neural firing.  Neurons actually learn to make some connections stronger than others, in a process calledlong-term potentiation, allowing them to learn when to fire in response to the activities of neurons they're connected to.   Neurons can also make connections weaker through an analogous process calledlong-term depression. As discussed in the above sections, as well as the later section titledThe Universal Approximation Theorem, a good computational model of the brain will have three characteristics: Biologically-InspiredThe brain's computational power is derived from its neurons and the connections between them.  Thus, a good computational approximation of the brain will have individual computational units (a la neurons), as well as ways for those neurons to communicate (a la connections).  Specifically, the outputs of some computational units will be the inputs to other computational units.  Furthermore, each computational unit should calculate some function akin to the activation function of real neurons. FlexibleThe brain is flexible enough to learn seemingly endless types and forms of data.  For example, even though most teenagers under the age of 16 have never driven a car before, most learn very quickly to drive upon receiving their driver's license.  No person's brain is preprogrammed to learn how to drive, and yet almost anyone can do it given a small amount of training.  The brain's ability to learn to solve new tasks that it has no prior experience with is part of what makes it so powerful.  Thus, a good computational approximation of the brain should be able to learn many different types of functions without knowing the forms those functions will take beforehand. Capable of Online LearningThe brain doesn't need to learn everything at once, so neither should a good model of it.  Thus, a good computational approximation of the brain should be able to improve by online learning, meaning it gradually improves over time as it learns to correct past errors. By the first desideratum, the model will consist of many computational units connected in some way.  Each computational unit will perform a simple computation whose output will be passed as input to other units.  This process will repeat itself some number of times, so that outputs from some computational units are the inputs to others.  With any luck, connecting enough of these units together will give sufficient complexity to compute any function, satisfying the second desideratum.  However, what kind of function the model ends up computing will depend on the data it is exposed to, as well as alearning algorithmthat determines how the model learns that data.  Ideally, this algorithm will be able to perform online learning, the third desideratum. Thus, building a good computational approximation to the brain consists of three steps.  The first is to develop a computational model of the neuron and to connect those models together to replicate the way the brain performs computations.  This is covered in the sections titledA Computational Model of the Neuron,The Sigmoid Function, andPutting It All Together.  The second is to prove that this model is sufficiently complex to calculate any function and learn any type of data it is given, which is covered in the section titledThe Universal Approximation Theorem.  The third is to develop a learning algorithm that can learn to calculate a function, given a model and some data, in an online manner.  This is covered in the section titledTraining The Model. The step function.[4] As stated above, neurons fire above a certain threshold and do nothing below that threshold, so a model of the neuron requires a function exhibiting the same properties.  The simplest function that does this is thestep function. The step function is defined as:\(H(x) = \begin{cases}
  1 & \mbox{if } x \ge 0, \\
  0 & \mbox{if } x \lt 0. \\
\end{cases}\) In this simple neuron model, the input is a single number that must exceed the activation threshold in order to trigger firing.  However, neurons can (and should, if they're to do anything useful) have connections to multiple incoming neurons, so we need some way of ""integrating"" these incoming neuron's inputs into a single number.  The most common way of doing this is to take a weighted sum of the neuron's incoming inputs, so that the neuron fires when the weighted sum exceeds the threshold.  If the vector of outputs from the incoming neurons is represented by \(\vec{x}\), then the weighted sum of \(\vec{x}\) is thedot product\(\vec{w} \cdot \vec{x}\), where \(\vec{w}\) is called theweight vector. To further improve the modeling capacity of the neuron, we want to be able to set the threshold arbitrarily.  This can be achieved by adding ascalar(which may be positive or negative) to the weighted sum of the inputs.  Adding a scalar of \(-b\) will force the neuron's activation threshold to be set to \(b\), since the new step function \(H(x+(-b))\) at \(x = b\) equals \(0\), which is the threshold of the step function.  The value \(b\) is known as thebiassince it biases the step function away from the natural threshold at \(x = 0\). Thus, calculating the output of our neuron model is comprised of two steps:1) Calculate theintegration.  The integration, as defined above, is the sum \(\vec{w} \cdot \vec{x} + b\) for vectors \(\vec{w}\), \(\vec{x}\) and scalar \(b\). 2) Calculate theoutput.  The output is the activation function applied to the result of step 1.  Since the activation function in our model is the 
step function, the output of the neuron is \(H(\vec{w} \cdot \vec{x} + b)\), which is \(1\) when \(\vec{w} \cdot \vec{x} + b >= 0\) and \(0\) otherwise. A linear classifier, where squares evaluate to 1 and circles to 0.[5] Following from the description of step 2, our neuron model defines alinear classfier, i.e. a function that splits the inputs into two regions with a linear boundary.  In two dimensions, this is a line, while in higher dimensions the boundary is known as ahyperplane.  The weight vector \(\vec{w}\) defines the slope of the linear boundary while the bias \(b\) defines the intercept of the linear boundary.  The following diagram illustrates a neuron's output for two incoming connections (i.e. a two dimensional input vector \(\vec{x}\).  Note that the neuron inputs are clearly separated into values of \(0\) and \(1\) by a line (defined by \(\vec{w} \cdot \vec{x} + b = 0\)). By adjusting the values of \(\vec{w}\) and \(b\), the step function unit can adjust its linear boundary and learn to split its inputs into classes, \(0\) and \(1\), as shown in the previous image.  As a corollary, different values of \(\vec{w}\) and \(b\) for multiple step function units will yield multiple different linear classifiers.  Part of what makes ANNs so powerful is their ability to adjust \(\vec{w}\) and \(b\) for many units at the same time, effectively learning many linear classifiers simultaneously.  This learning is discussed in more depth in the section titledTraining the Model. Since the brain can calculate more than just linear functions by connecting many neurons together, this suggests that connecting many linear classifiers together should produce a nonlinear function.  In fact, it is proven that for certain activation functions and a very large number of neurons, ANNs can model any continuous, smooth function arbitrarily well, a result known as theuniversal approximation theorem. This is very convenient because, like the brain, an ANN should ideally be able to learn any function handed to it.  If ANNs could only learn one type of function (e.g. third degreepolynomials), this would severely limit the types of problems to which they could be applied.  Furthermore, learning often happens in an environment where the type of function to be learned is not known beforehand, so it is advantageous to have a model that does not depend on knowing a priori the form of the data it will be exposed to. Unfortunately, since the step function can only output two different values, \(0\) and \(1\), an ANN of step function neurons cannot be a universal approximator (generally speaking, continuous functions take on more than two values).   Luckily, there is a continuous function called the sigmoid function, described in the next section, that is very similar to the step function and can be used in universal approximators. The sigmoid function.[6] There is a continuous approximation of the step function called the logistic curve, orsigmoid function, denoted as \(\sigma(x)\).  This function's output ranges over all values between \(0\) and \(1\) and makes a transition from values near \(0\) to values near \(1\) at \(x = 0\), similar to the step function \(H(x)\). The sigmoid function is defined as:\(\sigma(x) = \frac{1}{1 + e^{-x}}\) So, for a computational unit that uses the sigmoid function, instead of firing \(0\) or \(1\) like a step function unit, it's output will be between \(0\) and \(1\), non-inclusive.  This changes slightly the interpretation of this unit as a model of a neuron, since it no longer exhibits all-or-nothing behavior since it will never take on the value of \(0\) (nothing) or \(1\) (all).  However, the sigmoid function is very close to \(0\) for \(x \lt 0\) and very close to \(1\) for \(x \gt 0\), so it can be interpreted as exhibiting practically all-or-nothing behavior on most (\(x \not\approx 0\)) inputs. The output for a  sigmoidal unit with weight vector \(\vec{w}\) and bias \(b\) on input \(\vec{x}\) is:\(\sigma(\vec{w} \cdot \vec{x} + b) = \left(1+\exp\left(-(\vec{w} \cdot \vec{x} + b)\right)\right)^{-1}\) Thus, a sigmoid unit is like a linear classifier with a boundary defined at \(\vec{w} \cdot \vec{x} + b = 0\).  The value of the sigmoid function at the boundary is \(\sigma(0) = .5\).  Inputs \(\vec{x}\) that are far from the linear boundary will be approximately \(0\) or \(1\), while those very close to the boundary will be closer to \(.5\). The sigmoid function turns out to be a member of the class of activation functions for universal approximators, so it imitates the behavior of real neurons (by approximating the step function) while also permitting the possibility of arbitrary function approximation.  These happen to be exactly the first twodesiderataspecified for a good mathematical model of the brain.  In fact, some ANNs use activation functions that are different from the sigmoidal function, because those functions are also proven to be in the class of functions for which universal approximators can be built.  Two well-known activation functions used in the same manner as the sigmoidal function are thehyperbolic tangentand therectifier.  The proof that these functions can be used to build ANN universal approximators is fairly advanced, so it is not covered here. Calculate the output of a sigmoidal neuron with weight vector \(\vec{w} = (.25, .75)\) and bias \(b = -.75\) for the following two inputs vectors: \(\vec{m} = (1, 2)\)\(\vec{n} = (1, -.5)\) Recalling that the output of a sigmoidal neuron with input \(\vec{x}\) is \(\sigma(\vec{w} \cdot \vec{x} + b)\), \(\begin{align*} 
d &= \vec{w} \cdot \vec{m} + b \\
&= w_1 \cdot m_1 + w_2 \cdot m_2 + b \\
&= .25 \cdot 1 + .75 \cdot 2 -.75 \\
&= 1
\end{align*}\) \(\begin{align*} 
s &= \sigma(d) \\
&=  \frac{1}{1 + e^{-d}} \\
&= \frac{1}{1+e^{-1}} \\
&= .73105857863
\end{align*}\) Thus, the output on \(\vec{m} = (1, 2)\) is \(.73105857863\).  The same reasoning applied to \(\vec{n} = (1, -.5)\)  yields \(.29421497216\).  Like the step function unit describe above, the sigmoid function unit's linear boundary can be adjusted by changing the values of \(\vec{w}\) and \(b\).  The weight vector defines the slope of the linear boundary while the bias defines the intercept of the linear boundary.  Since, like the brain, the final model will include many individual computational units (a la neurons), a learning algorithm that can learn, ortrain, many \(\vec{w}\) and \(b\) values simultaneously is required.  This algorithm is described in the section titledTraining the Model. Neurons are connected to one another, with each neuron's incoming connections made up of the outgoing connections of other neurons.  Thus, the ANN will need to connect the outputs of sigmoidal units to the inputs of other sigmoidal units. The diagram below shows a sigmoidal unit with three inputs \(\vec{x} = (x_1, x_2, x_3)\), one output \(y\), bias \(b\), and weight vector \(\vec{w} = (w_1, w_2, w_3)\).  Each of the inputs \(\vec{x} = (x_1, x_2, x_3)\) can be the output of another sigmoidal unit (though it could also be raw input, analogous to unprocessed sense data in the brain, such as sound), and the unit's output \(y\) can be the input to other sigmoidal units (though it could also be a final output, analogous to an action associated neuron in the brain, such as one that bends your left elbow).  Notice that each component \(w_i\) of the weight vector corresponds to each component \(x_i\) of the input vector.  Thus, the summation of the product of the individual \(w_i, x_i\) pairs is equivalent to the dot product, as discussed in the previous sections. A sigmoidal unit with three inputs \(\vec{x} = (x_1, x_2, x_3)\), weight vector \(\vec{w}\), and bias \(b\).[7] Artificial neural networks are most easily visualized in terms of adirected graph.  In the case of sigmoidal units,node\(s\) represents sigmoidal unit \(s\) (as in the diagram above) anddirected edge\(e = (u, v)\) indicates that one of sigmoidal unit \(v\)'s inputs is the output of sigmoidal unit \(u\). Thus, if the diagram above represents sigmoidal unit \(s\) and inputs \(x_1\), \(x_2\), and \(x_3\) are the outputs of sigmoidal units \(a\), \(b\), and \(c\), respectively, then a graph representation of the above sigmoidal unit will have nodes \(a\), \(b\), \(c\), and \(s\) with directed edges \((a, s)\), \((b, s)\), and \((c, s)\).  Furthermore, since each incoming directed edge is associated with a component of the weight vector for sigmoidal unit \(s\), each incoming edge will be labeled with its corresponding weight component.  Thus edge \((a, s)\) will have label \(w_1\), \((b, s)\) will have label \(w_2\), and \((c, s)\) will have label \(w_3\).  The corresponding graph is shown below, with the edges feeding into nodes \(a\), \(b\), and \(c\) representing inputs to those nodes. Directed graph representing ANN with sigmoidal units \(a\), \(b\), \(c\), and \(s\).  Unit \(s\)'s weight vector \(\vec{w}\) is \((w_1, w_2, w_3)\) While the above ANN is very simple, ANNs in general can have many more nodes (e.g. modern machine vision applications use ANNs with more than \(10^6\) nodes) in very complicated connection patterns (see the wiki aboutconvolutional neural networks). The outputs of sigmoidal units are the inputs of other sigmoidal units, indicated by directed edges, so computation follow the edges in the graph representation of the ANN.  Thus, in the example above, computation of \(s\)'s output is preceded by the computation of \(a\), \(b\), and \(c\)'s outputs.  If the graph above was modified so that's \(s\)'s output was an input of \(a\), a directed edge passing from \(s\) to \(a\) would be added, creating what is known as acycle.  This would mean that \(s\)'s output is dependent on itself.  Cyclic computation graphs greatly complicate computation and learning, so computation graphs are commonly restricted to bedirected acyclic graphs(or DAGs), which have no cycles.  ANNs with DAG computation graphs are known asfeedforward neural networks, while ANNs with cycles are known asrecurrent neural networks. Ultimately, ANNs are used to compute and learn functions.  This consists of giving the ANN a series of input-output pairs \(\vec{(x_i}, \vec{y_i})\), and training the model to approximate the function \(f\) such that \(f(\vec{x_i}) = \vec{y_i}\) for all pairs.  Thus, if \(\vec{x}\) is \(n\)-dimensional and \(\vec{y}\) is \(m\)-dimensional, the final sigmoidal ANN graph will consist of \(n\) input nodes (i.e. raw input, not coming from other sigmoidal units) representing \(\vec{x} = (x_1, \dots, x_n)\), \(k\) sigmoidal units (some of which will be connected to the input nodes), and \(m\) output nodes (i.e. final output, not fed into other sigmoidal units) representing \(\vec{y} = (y_1, \dots, y_m)\). Like sigmoidal units, output nodes have multiple incoming connections and output one value.  This necessitates an integration scheme and an activation function, as defined in the section titledThe Step Function.  Sometimes, output nodes use the same integration and activation as sigmoidal units, while other times they may use more complicated functions, such as thesoftmax function, which is heavily used in classification problems.  Often, the choice of integration and activation functions is dependent on the form of the output.  For example, since sigmoidal units can only output values in the range \((0, 1)\), they are ill-suited to problems where the expected value of \(y\) lies outside that range. An example graph for an ANN computing a two dimensional output \(\vec{y}\) on a three dimensional input \(\vec{x}\) using five sigmoidal units \(s_1, \dots, s_5\) is shown below.  An edge labeled with weight \(w_{ab}\) represents the component of the weight vector for node \(b\) that corresponds to the input coming from node \(a\).  Note that this graph, because it has no cycles, is a feedforward neural network. ANN for three dimensional input, two dimensional output, and five sigmoidal units Thus, the above ANN would start by computing the outputs of nodes \(s_1\) and \(s_2\) given \(x_1\), \(x_2\), and \(x_3\).  Once that was complete, the ANN would next compute the outputs of nodes \(s_3\), \(s_4\), and \(s_5\), dependent on the outputs of \(s_1\) and \(s_2\).  Once that was complete, the ANN would do the final calculation of nodes \(y_1\) and \(y_2\), dependent on the outputs of nodes \(s_3\), \(s_4\), and \(s_5\). It is obvious from this computational flow that certain sets of nodes tend to be computed at the same time, since a different set of nodes uses their outputs as inputs.  For example, set \(\{s_3, s_4, s_5\}\) depends on set \(\{s_1, s_2\}\).  These sets of nodes that are computed together are known aslayers, and ANNs are generally thought of a series of such layers, with each layer \(l_i\) dependent on previous layer \(l_{i-1}\)  Thus, the above graph is composed of four layers.  The first layer \(l_0\) is called theinput layer(which does not need to be computed, since it is given), while the final layer \(l_3\) is called theoutput layer.  The intermediate layers are known ashidden layers, which in this case are the layers \(l_1 = \{s_1, s_2\}\) and \(l_2 = \{s_3, s_4, s_5\}\), are usually numbered so that hidden layer \(h_i\) corresponds to layer \(l_i\).  Thus, hidden layer \(h_1=\{s_1, s_2\}\) and hidden layer \(h_2=\{s_3, s_4, s_5\}\).  The diagram below shows the example ANN with each node grouped into its appropriate layer. The same ANN grouped into layers ANN LayersThe image source:Artificial Neural Network The ANN can now calculate some function \(f_{\theta}(\vec{x})\) that depends on the values of the individual nodes' weight vectors and biases, which together are known as the ANN'sparameters\(\theta\).  The logical next step is to determine how to alter those biases and weight vectors so that the ANN computes known values of the function.  That is, given a series of input-output pairs \((\vec{x_i}, \vec{y_i})\), how can the weight vectors and biases be altered such that \(f_{\theta}(\vec{x_i}) \approx \vec{y_i}\) for all \(i\)? The typical way to do this is define an error function \(E\) over the set of pairs \(X = \{(\vec{x_1}, \vec{y_1}), \dots, (\vec{x_N},\vec{y_N})\}\) such that \(E(X, \theta)\) is small when \(f_{\theta}(\vec{x_i}) \approx \vec{y_i}\) for all \(i\).  Common choices for \(E\) are themean squared error(MSE) in the case ofregressionproblems and thecross entropyin the case ofclassificationproblems.  Thus, training the ANN reduces to minimizing the error \(E(X, \theta)\) with respect to the parameters (since \(X\) is fixed).  For example, for the mean square error function, given two input-output pairs \(X= \{(\vec{x_1}, \vec{y_1}), (\vec{x_2}, \vec{y_2})\}\) and an ANN with parameters \(\theta\) that outputs \( f_{\theta}(\vec{x})\) for input \(\vec{x}\), the error function \(E(X, \theta)\) is \[E(X, \theta)=\frac{(y_1 - f_{\theta}(\vec{x_1}))^2}{2} + \frac{(y_2 - f_{\theta}(\vec{x_2}))^2}{2}\] Since the error function \(E(X, \theta)\) defines a fairly complex function (it is a function of the output of the ANN, which is a composition of many nonlinear functions), finding the minimum analytically is generally impossible.  Luckily, there exists a general method for minimizingdifferentiable functionscalledgradient descent.  Basically, gradient descent finds thegradientof a function \(f\) at a particular value \(x\) (for an ANN, that value will be the parameters \(\theta\)) and then updates that value by moving (or stepping) in the direction of the negative of the gradient.  Generally speaking (it depends on the size of the step \(\eta\)), this will find a nearby value \(x^{\prime} = x - \eta \nabla f(x)\) for which \(f(x^{\prime}) \lt f(x)\).  This process repeats until alocal minimumis found, or the gradient sufficiently converges (i.e. becomes smaller than some threshold).  Learning for an ANN typically starts with a random initialization of the parameters (the weight vectors and biases) followed by successive updates to those parameters based on gradient descent until the error function \(E(X, \theta)\) converges. A major advantage of gradient descent is that it can be used foronline learning, since the parameters are not solved in one calculation but are instead gradually improved by moving in the direction of the negative gradient.  Thus, if input-output pairs are arriving in a sequential fashion, the ANN can perform gradient descent on one input-output pair for a certain number of steps, and then do the same once the next input-output pair arrives.  For an appropriate choice of step size \(\eta\), this approach can yield results similar to gradient descent on the entire dataset \(X\) (known asbatch learning). Because gradient descent is a local method (the step direction is determined by the gradient at a single point), it can only find local minima.  While this is generally a significant problem for most optimization applications, recent research has suggested that finding local minima is not actually an issue for ANNs, since the vast majority of local minima are evenly distributed and similar in magnitude for large ANNs. For a long time, calculating the gradient for ANNs was thought to be mathematically intractable, since ANNs can have large numbers of nodes and very many layers, making the error function \(E(X, \theta)\) highly nonlinear.  However, in the mid-1980s, computer scientists were able to derive a method for calculating the gradient with respect to an ANN's parameters, known asbackpropagation, or ""backpropagation by errors"".  The method works for bothfeedforward neural networks(for which it was originally designed) as well as forrecurrent neural networks, in which case it is calledbackpropagation through time, or BPTT.  The discovery of this method brought about a renaissance in artificial neural network research, as training non-trivial ANNs had finally become feasible. , D.Neuralnetwork.
    Retrieved
    June 4, 2005,
    fromhttps://commons.wikimedia.org/wiki/File:Neuralnetwork.png, G.Microglia_and_neurons.
    Retrieved
    July 25, 2005,
    fromhttps://commons.wikimedia.org/wiki/File:Microglia_and_neurons.jpg, B.Current_Clamp_recording_of_Neuron.
    Retrieved
    October 6, 2006,
    fromhttps://commons.wikimedia.org/wiki/File:Current_Clamp_recording_of_Neuron.GIF, L.Heaviside.
    Retrieved
    August 25, 2007,
    fromhttps://commons.wikimedia.org/wiki/File:Heaviside.svg, M.Linearna_separovatelnost_v_prikladovom_priestore.
    Retrieved
    December 13, 2013,
    fromhttps://commons.wikimedia.org/wiki/File:Linearna_separovatelnost_v_prikladovom_priestore.png, Q.Logistic-curve.
    Retrieved
    July 2, 2008,
    fromhttps://commons.wikimedia.org/wiki/File:Logistic-curve.svg, C.ArtificialNeuronModel.
    Retrieved
    July 14, 2005,
    fromhttps://commons.wikimedia.org/wiki/File:ArtificialNeuronModel.png Reset passwordNew user?Sign up Existing user?Log in Problem Loading... Note Loading... Set Loading..."
http://neuralnetworksanddeeplearning.com/,"Neural Networks and Deep Learning What this book is about On the exercises and problems Using neural nets to recognize handwritten digitsPerceptronsSigmoid neuronsThe architecture of neural networksA simple network to classify handwritten digitsLearning with gradient descentImplementing our network to classify digitsToward deep learningHow the backpropagation algorithm worksWarm up: a fast matrix-based approach to computing the output  from a neural networkThe two assumptions we need about the cost functionThe Hadamard product, $s \odot t$The four fundamental equations behind backpropagationProof of the four fundamental equations (optional)The backpropagation algorithmThe code for backpropagationIn what sense is backpropagation a fast algorithm?Backpropagation: the big pictureImproving the way neural networks learnThe cross-entropy cost functionOverfitting and regularizationWeight initializationHandwriting recognition revisited: the codeHow to choose a neural network's hyper-parameters?Other techniquesA visual proof that neural nets can compute any functionTwo caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusionWhy are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 PerceptronsSigmoid neuronsThe architecture of neural networksA simple network to classify handwritten digitsLearning with gradient descentImplementing our network to classify digitsToward deep learning How the backpropagation algorithm worksWarm up: a fast matrix-based approach to computing the output  from a neural networkThe two assumptions we need about the cost functionThe Hadamard product, $s \odot t$The four fundamental equations behind backpropagationProof of the four fundamental equations (optional)The backpropagation algorithmThe code for backpropagationIn what sense is backpropagation a fast algorithm?Backpropagation: the big pictureImproving the way neural networks learnThe cross-entropy cost functionOverfitting and regularizationWeight initializationHandwriting recognition revisited: the codeHow to choose a neural network's hyper-parameters?Other techniquesA visual proof that neural nets can compute any functionTwo caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusionWhy are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 Warm up: a fast matrix-based approach to computing the output  from a neural networkThe two assumptions we need about the cost functionThe Hadamard product, $s \odot t$The four fundamental equations behind backpropagationProof of the four fundamental equations (optional)The backpropagation algorithmThe code for backpropagationIn what sense is backpropagation a fast algorithm?Backpropagation: the big picture Improving the way neural networks learnThe cross-entropy cost functionOverfitting and regularizationWeight initializationHandwriting recognition revisited: the codeHow to choose a neural network's hyper-parameters?Other techniquesA visual proof that neural nets can compute any functionTwo caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusionWhy are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 The cross-entropy cost functionOverfitting and regularizationWeight initializationHandwriting recognition revisited: the codeHow to choose a neural network's hyper-parameters?Other techniques A visual proof that neural nets can compute any functionTwo caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusionWhy are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 Two caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusion Why are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learning Deep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 Introducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networks Appendix: Is there asimplealgorithm for intelligence? Acknowledgements Frequently Asked Questions If you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount. Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAx Thanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame. Michael Nielsen on Twitter Book FAQ Code repository Michael Nielsen's project announcement mailing list Deep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courville cognitivemedium.com ByMichael Nielsen/ Dec 2019 Neural Networks and Deep Learningis a free online book.  The
book will teach you about:Neural networks, a beautiful biologically-inspired programming
paradigm which enables a computer to learn from observational dataDeep learning, a powerful set of techniques for learning in neural
networksNeural networks and deep learning currently provide the best solutions
to many problems in image recognition, speech recognition, and natural
language processing.  This book will teach you many of the core
concepts behind neural networks and deep learning. For more details about the approach taken in the
book,see here.  Or you can jump directly
toChapter 1and get started. "
https://link.springer.com/chapter/10.1007/978-3-319-94463-0_1,"Advertisement 437kAccesses 24Citations 3Altmetric Artificial neural networks are popular machine learning techniques that simulate the mechanism of learning in biological organisms. The human nervous system contains cells, which are referred to asneurons. The neurons are connected to one another with the use ofaxonsanddendrites, and the connecting regions between axons and dendrites are referred to assynapses. These connections are illustrated in Figure1.1(a). The strengths of synaptic connections often change in response to external stimuli. This change is how learning takes place in living organisms. â€œThou shalt not make a machine to counterfeit a human mind.â€â€”Frank Herbert This is a preview of subscription content,log in via an institutionto check access. Tax calculation will be finalised at checkout Purchases are for personal use only Institutional subscriptions The ReLU shows asymmetric saturation. Examples include TorchÂ [572], TheanoÂ [573], and TensorFlowÂ [574]. Weight decay is generally used with other loss functions in single-layer models and in all multi-layer models with a large number of parameters. This is an overloading of the terminology used in convolutional neural networks. The meaning of the word â€œdepthâ€ is inferred from the context in which it is used. C. Aggarwal. Data classification: Algorithms and applications,CRC Press, 2014. Google Scholar C. Aggarwal. Data mining: The textbook.Springer, 2015. Google Scholar C. Aggarwal. Machine learning for text.Springer, 2018. Google Scholar Y. Bengio. Learning deep architectures for AI.Foundations and Trends in Machine Learning, 2(1), pp.Â 1â€“127, 2009. ArticleGoogle Scholar Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.IEEE TPAMI, 35(8), pp.Â 1798â€“1828, 2013. ArticleGoogle Scholar Y. Bengio and O. Delalleau. On the expressive power of deep architectures.Algorithmic Learning Theory, pp.Â 18â€“36, 2011. Google Scholar J. BergstraetÂ al.Theano: A CPU and GPU math compiler in Python.Python in Science Conference, 2010. Google Scholar C. M. Bishop. Pattern recognition and machine learning.Springer, 2007. Google Scholar C. M. Bishop. Neural networks for pattern recognition.Oxford University Press, 1995. Google Scholar L. Breiman. Random forests.Journal Machine Learning archive, 45(1), pp. 5â€“32, 2001. ArticleGoogle Scholar A. Bryson. A gradient method for optimizing multi-stage allocation processes.Harvard University Symposium on Digital Computers and their Applications, 1961. Google Scholar D. Ciresan, U. Meier, L. Gambardella, and J. Schmidhuber. Deep, big, simple neural nets for handwritten digit recognition.Neural Computation, 22(12), pp.Â 3207â€“3220, 2010. ArticleGoogle Scholar T. Cover. Geometrical and statistical properties of systems of linear inequalities with applications to pattern recognition.IEEE Transactions on Electronic Computers, pp.Â 326â€“334, 1965. Google Scholar N. de Freitas. Machine Learning, University of Oxford (Course Video), 2013.https://www.youtube.com/watch?v=w2OtwL5T1ow&list=PLE6Wd9FREdyJ5lbFl8Uu-GjecvVw66F6 N. de Freitas. Deep Learning, University of Oxford (Course Video), 2015.https://www.youtube.com/watch?v=PlhFWT7vAEw&list=PLjK8ddCbDMphIMSXn-1IjyYpHU3DaUYw O.Â Delalleau and Y. Bengio. Shallow vs. deep sum-product networks.NIPS Conference, pp.Â 666â€“674, 2011. Google Scholar Y. Freund and R. Schapire. Large margin classification using the perceptron algorithm.Machine Learning, 37(3), pp.Â 277â€“296, 1999. ArticleGoogle Scholar K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position.Biological Cybernetics, 36(4), pp.Â 193â€“202, 1980. ArticleGoogle Scholar S. Gallant. Perceptron-based learning algorithms.IEEE Transactions on Neural Networks, 1(2), pp. 179â€“191, 1990. ArticleGoogle Scholar A. Ghodsi. STAT 946: Topics in Probability and Statistics: Deep Learning,University of Waterloo, Fall 2015.https://www.youtube.com/watch?v=fyAZszlPphs&list=PLehuLRPyt1Hyi78UOkMP-WCGRxGcA9NVOE X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.AISTATS, pp.Â 249â€“256, 2010. Google Scholar I. Goodfellow, Y. Bengio, and A. Courville. Deep learning.MIT Press, 2016. Google Scholar A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks.Acoustics, Speech and Signal Processing (ICASSP), pp.Â 6645â€“6649, 2013. Google Scholar A. Graves, G. Wayne, and I. Danihelka. Neural turing machines.arXiv:1410.5401, 2014.https://arxiv.org/abs/1410.5401 K. Greff, R. K. Srivastava, and J. Schmidhuber. Highway and residual networks learn unrolled iterative estimation.arXiv:1612.07771, 2016.https://arxiv.org/abs/1612.07771 D. Hassabis, D. Kumaran, C. Summerfield, and M. Botvinick. Neuroscience-inspired artificial intelligence.Neuron, 95(2), pp.Â 245â€“258, 2017. ArticleGoogle Scholar T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning.Springer, 2009. Google Scholar S. Haykin. Neural networks and learning machines.Pearson, 2008. Google Scholar K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.IEEE Conference on Computer Vision and Pattern Recognition, pp.Â 770â€“778, 2016. Google Scholar G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.Science, 313, (5766), pp.Â 504â€“507, 2006. Google Scholar S. Hochreiter and J. Schmidhuber. Long short-term memory.Neural Computation, 9(8), pp.Â 1735â€“1785, 1997. ArticleGoogle Scholar J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities.National Academy of Sciences of the USA, 79(8), pp.Â 2554â€“2558, 1982. ArticleMathSciNetGoogle Scholar K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators.Neural Networks, 2(5), pp.Â 359â€“366, 1989. ArticleGoogle Scholar D. Hubel and T. Wiesel. Receptive fields of single neurones in the catâ€™s striate cortex.The Journal of Physiology, 124(3), pp.Â 574â€“591, 1959. ArticleGoogle Scholar H. Kandel, J. Schwartz, T. Jessell, S. Siegelbaum, and A. Hudspeth. Principles of neural science.McGraw Hill, 2012. Google Scholar A. Karpathy, J. Johnson, and L. Fei-Fei. Stanford University Class CS321n: Convolutional neural networks for visual recognition, 2016.http://cs231n.github.io/ H. J. Kelley. Gradient theory of optimal flight paths.Ars Journal, 30(10), pp.Â 947â€“954, 1960. ArticleGoogle Scholar T. Kietzmann, P. McClure, and N. Kriegeskorte. Deep Neural Networks In Computational Neuroscience.bioRxiv, 133504, 2017.https://www.biorxiv.org/content/early/2017/05/04/133504 J. Kivinen and M. Warmuth. The perceptron algorithm vs. winnow: linear vs. logarithmic mistake bounds when few input variables are relevant.Computational Learning Theory, pp.Â 289â€“296, 1995. Google Scholar D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques.MIT Press, 2009. Google Scholar A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks.NIPS Conference, pp.Â 1097â€“1105. 2012. Google Scholar H. Larochelle. Neural Networks (Course). Universite de Sherbrooke, 2013.https://www.youtube.com/watch?v=SGZ6BttHMPw&list=PL6Xpj9I5qXYEcOhn7-TqghAJ6NAPrNmUBH H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation.ICML Confererence, pp.Â 473â€“480, 2007. Google Scholar Y. LeCun, Y. Bengio, and G. Hinton. Deep learning.Nature, 521(7553), pp.Â 436â€“444, 2015. ArticleGoogle Scholar Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.Proceedings of the IEEE, 86(11), pp.Â 2278â€“2324, 1998. ArticleGoogle Scholar Y. LeCun, C. Cortes, and C. Burges. The MNIST database of handwritten digits, 1998.http://yann.lecun.com/exdb/mnist/ C. Manning and R. Socher. CS224N: Natural language processing with deep learning.Stanford University School of Engineering, 2017.https://www.youtube.com/watch?v=OQQ-W_63UgQ W. S. McCulloch and W. H. Pitts. A logical calculus of the ideas immanent in nervous activity.The Bulletin of Mathematical Biophysics, 5(4), pp.Â 115â€“133, 1943. ArticleMathSciNetGoogle Scholar G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller. Introduction to WordNet: An on-line lexical database.International Journal of Lexicography, 3(4), pp.Â 235â€“312, 1990.https://wordnet.princeton.edu/ ArticleGoogle Scholar M. Minsky and S. Papert. Perceptrons. An Introduction to Computational Geometry,MIT Press, 1969. Google Scholar G. Montufar. Universal approximation depth and errors of narrow belief networks with discrete units.Neural Computation, 26(7), pp.Â 1386â€“1407, 2014. ArticleMathSciNetGoogle Scholar R. Miotto, F. Wang, S. Wang, X. Jiang, and J. T. Dudley. Deep learning for healthcare: review, opportunities and challenges.Briefings in Bioinformatics, pp.Â 1â€“11, 2017. Google Scholar H. Poon and P. Domingos. Sum-product networks: A new deep architecture.Computer Vision Workshops (ICCV Workshops), pp.Â 689â€“690, 2011. Google Scholar V. Romanuke. Parallel Computing Center (Khmelnitskiy, Ukraine) represents an ensemble of 5 convolutional neural networks which performs on MNIST at 0.21 percent error rate. Retrieved 24 November 2016. Google Scholar F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain.Psychological Review, 65(6), 386, 1958. Google Scholar D. Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating errors.Nature, 323 (6088), pp.Â 533â€“536, 1986. ArticleGoogle Scholar D. Rumelhart, G. Hinton, and R. Williams. Learning internal representations by back-propagating errors. InParallel Distributed Processing: Explorations in the Microstructure of Cognition, pp.Â 318â€“362, 1986. Google Scholar J. Schmidhuber. Deep learning in neural networks: An overview.Neural Networks, 61, pp.Â 85â€“117, 2015. ArticleGoogle Scholar H. Siegelmann and E. Sontag. On the computational power of neural nets.Journal of Computer and System Sciences, 50(1), pp.Â 132â€“150, 1995. ArticleMathSciNetGoogle Scholar S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for SVM.Mathematical Programming, 127(1), pp.Â 3â€“30, 2011. ArticleMathSciNetGoogle Scholar B. W. Silverman. Density Estimation for Statistics and Data Analysis.Chapman and Hall, 1986. Google Scholar S. Wang, C. Aggarwal, and H. Liu. Using a random forest to inspire a neural network and improving on it.SIAM Conference on Data Mining, 2017. Google Scholar A. Wendemuth. Learning the unlearnable.Journal of Physics A: Math. Gen., 28, pp.Â 5423â€“5436, 1995. ArticleMathSciNetGoogle Scholar P. Werbos. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.PhD thesis, Harvard University, 1974. Google Scholar P. Werbos. The roots of backpropagation: from ordered derivatives to neural networks and political forecasting (Vol. 1).John Wiley and Sons, 1994. Google Scholar P. Werbos. Backpropagation through time: what it does and how to do it.Proceedings of the IEEE, 78(10), pp.Â 1550â€“1560, 1990. ArticleGoogle Scholar J. Weston, S. Chopra, and A. Bordes. Memory networks.ICLR, 2015. Google Scholar B. Widrow and M. Hoff. Adaptive switching circuits.IRE WESCON Convention Record, 4(1), pp.Â 96â€“104, 1960. Google Scholar http://caffe.berkeleyvision.org/ http://torch.ch/ http://deeplearning.net/software/theano/ https://www.tensorflow.org/ https://keras.io/ https://lasagne.readthedocs.io/en/latest/ http://www.image-net.org/ http://www.image-net.org/challenges/LSVRC/ https://deeplearning4j.org/ https://www.wikipedia.org/ https://science.education.nih.gov/supplements/webversions/BrainAddiction/guide/lesson2-1.html https://www.ibm.com/us-en/marketplace/deep-learning-platform https://www.coursera.org/learn/neural-networks https://archive.ics.uci.edu/ml/datasets.html https://www.youtube.com/watch?v=2pWv7GOvuf0 Download references IBM T. J. Watson Research Center, International Business Machines, Yorktown Heights, NY, USA Charu C. Aggarwal You can also search for this author inPubMedGoogle Scholar Reprints and permissions Â© 2018 Springer International Publishing AG, part of Springer Nature Aggarwal, C.C. (2018).  An Introduction to Neural Networks.

                     In:  Neural Networks and Deep Learning. Springer, Cham. https://doi.org/10.1007/978-3-319-94463-0_1 DOI:https://doi.org/10.1007/978-3-319-94463-0_1 Published:26 August 2018 Publisher Name:Springer, Cham Print ISBN:978-3-319-94462-3 Online ISBN:978-3-319-94463-0 eBook Packages:Computer ScienceComputer Science (R0) Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article.  Provided by the Springer Nature SharedIt content-sharing initiative Policies and ethics Tax calculation will be finalised at checkout Purchases are for personal use only Institutional subscriptions 112.134.157.129 Not affiliated Â© 2024 Springer Nature"
https://builtin.com/machine-learning/nn-models,"A neural network is a series of algorithms that identifies patterns and relationships in data, similar to the way the brain operates. Here's how they work. A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In that sense, neural networks refer to systems of neurons, either organic or artificial in nature. Neural networks can adapt to a changing input, so the network generates the best possible result without needing to redesign the output criteria.  Why do we need yet another learning algorithm? The question is relevant. We already have a lot of learning algorithms like linear regression,logistic regression,decision treesandrandom forests, etc. Iâ€™ll show you a situation in which we have to deal with a complex, non-linear hypothesis. The above plot is obtained from the Breast Cancer Wisconsindata set. I have plotted two of the features, mean radius and mean texture, to gain some information about whether the tumor is malignant (M, represented by blue dots) or benign (B, represented by an orange x). This is a supervised learning classification problem. If we are to applylogistic regressionto this problem, the hypothesis would look like this: We can see that there are a lot of non-linear features. In this equation,gis thesigmoid function. If we perform logistic regression with such a hypothesis, then we might get a boundary â€” an extremely curvy one â€” separating the malignant and benign tumors. But this is effective only when weâ€™re considering two features. But this data set contains 30 features. If we were to include only the quadratic terms in the hypothesis, we will still have hundreds of non-linear features. The number of quadratic features generated will be to the order ofO(nÂ²), wherenis the number of features (30). We may end up overfitting the data set. Itâ€™s also computationally expensive to work with that many features. This is if weâ€™re only using quadratic terms. Donâ€™t even try to imagine the number of cubic terms generated in a similar manner. Enter neural networks. The NN that is implemented in the computer is called an artificial neural network (ANN), as they simulate the neurons present in the brain. The neurons are responsible for all of the actions voluntarily and involuntarily happening in our bodies. They transmit signals to and from the brain. More on Machine LearningK-Nearest Neighbor Algorithm: An Introduction First, we will model a neuron as a simple logistic unit. Herex1,x2andx3are the input nodes in blue color. The orange-colored node is the output node outputtingh(x). Hereâ€™s what I mean byh(x): In that equation,xandÎ¸are: This is a simplified model for the vast range of computations that the neuron completes. It gets inputx1,x2andx3and outputs a valueh(x). Thex0node, which is called the bias unit or bias neuron, is not usually represented pictorially. The above neuron network has a sigmoid (logistic) activation function. The term activation function refers to the non-linearityg(z):  The above model represents a single neuron. A neural network is a group of these neurons strung together. We have inputsx1,x2andx3as inputs andx0as a bias unit. We also have three neurons in the next layer:a1,a2anda3witha0as the bias unit. The last layer has only one neuron, which gives the output. Layer one is called the input layer, layer three is called the output layer and layer two is called the hidden layer. Any layer that isnâ€™t the input layer or the output layer is called the hidden layer. Letâ€™s delve into the computational steps represented by this diagram: Activation stands for the value computed by, or outputted by, a specific neuron. A neural network is parameterized by the matrix of weights. The corresponding weights form the weight matrix, which are multiplied with the inputs and then given as an input to the activation function, here, the sigmoid function, to get the output of the specific neuron. There are three outputs from the three neurons in the hidden layer, and one output from the neuron in the output layer. If a network hasnunits in layerj,munits in layerj+1, then the weight matrix corresponding to the layersjandj+1will be of the dimensionsm X(n+1). The entire process of multiplying the weights with the inputs and applying the activations function to get the outputs is calledforward propagation. The notations can be further simplified:  Instead of representing the above model with individual equations for the outputs of each neuron, we can represent them in the form of avector. xis the input vector and includes three inputsx1,x2,x3, and a bias unitx0whose value is mostly equal to one.zis the vector containing the linear products of input with the weight matrix. Forward propagation can be written as: The activation functiongapplies the sigmoid function element-wise to each of the elements in vectorz. We can also denote the input vector as the output of the first layer. In the above model, there is a bias unit present in the hidden layer. So, we need to add the bias to the output vector of the hidden layer. The final output of the output layer is given by: This process is called forward propagation because we start with the input layer and compute the outputs of the hidden layer and then we forward propagate that to compute the activations of the final output layer. There is no limit to the number of neurons in each layer or the number of layers. We can model a neural network according to our needs and then map the inputs and outputs with suitable weights and activation functions. If there is more than one hidden layer in a neural network, then itâ€™s called adeep neural network. Now that we know how a neural network calculates its output, the question becomes: How do we train a NN to output the desired values? We had thegradient descent algorithminlinearand logistic regression, which modifies the values of the parameters in order to get the desired output. How does one do that in a neural network? More on Machine LearningWhat Is Machine Learning and How Does It Work?  We use thebackpropagation algorithmin a NN to compute the gradient, which will allow us to modify the weight matrices discussed above to get the desired output. This is the most important part of NN, and itâ€™s where the model gets trained on the given data. We will discuss the backpropagation algorithm using the same model used above. The cost function of logistic regression is: The cost function of the above NN has a sigmoid activation function similar to that of logistic regression.h(x)would be the output of the neuron in the output layer.yis the desired output. The first step of backpropagation would be to calculate the total cost from the cost function. Our main objective is to change the corresponding weights so that we can minimize this cost. For this, we need to find the gradient of cost with respect to each of the weight matrices. Since we have two weight matrices, the gradients will be: After identifying the gradients, we will update the weight matrices as: Now to find the gradients, we will use the chain rule of differentiation. Now we have to find the individual terms in the chain rule. Multiplying all of them together gives us the gradient of cost with respect to the corresponding weight matrix. The gradient of cost with respect to the first weight matrix can also be further calculated through this equation: Therefore, the gradient will be: Individual values are calculated in a similar way. The first two terms are already calculated. Multiplying them all together gives us the gradient of cost with respect to the weight matrix. Thus, updating both the weight matrices simultaneously, weâ€™ll get an equation that looks like this: This completes one iteration of our backpropagation. A set of forward propagation and backpropagation is called an epoch. One epoch is when an entire data set is passed forward and backwards through the neural network once. Now, letâ€™s discuss the specifics of a neural network.  When it comes to training a neural network, weight matrices are the most important part. We need to initialize the weight matrices to a value to perform forward propagation, and the backpropagation to update the initialized weights. What if we initialized all the weights in the NN to zero? All the activations of the second layer will be equal. Since the gradients of cost with respect to the weights are dependent on the activations, they will be equal, too. While updating the weights after an epoch, the weights will remain the same, as they are initially equal and the gradients are also equal. This will result in all the neurons computing the same features, thereby outputting a redundant value and preventing the NN from learning. Therefore, random initialization is done to the parameters.  The following are the steps involved in modeling and training a neural network. Become a Machine Learning ExpertWhat Is Machine Learning?  There are various libraries available for modeling and training a neural network, but to understand the exact working mechanism of it, we mustbuild itfrom scratch at-least once. However, we will be using libraries from TensorFlow-Keras (tf.keras) andscikit-learn (sklearn)to cross-verify our model. I will useÂ  thebreast cancer data setfrom the University of California, Irvine, Machine Learning Repository. After importing the necessary libraries and data into a pandas DataFrame, youâ€™ll see that there are 32 features and 569 data points in each. Each of the features contains information about a tumor found in a patient. The task is to classify the tumors into â€œmalignantâ€ or â€œbenignâ€ based on these features. The target variable is found to bediagnosis. Since the featureidhas unique entries for all 569 cases, it was set as the index of the data frame. While also searching for missing values, youâ€™ll see that a feature namedUnnamed: 32had 100 percent missing values, causing this feature to be dropped. All the other features were found to be numeric features. On checking for the unique values in the target variable,array(['M', 'B'], dtype=object), there are two classes in the output: M for malignant and B for benign. These were replaced with 0 for M and 1 for B. Now that we have cleaned our data set, we are ready to divide it intotrain and test data. The sklearnâ€™strain_test_splitis used to divide 80 percent of the data into train data, and the remaining 20 percent into test data. After the division, both the train and test inputs are scaled using theStandardScalerfrom sklearn. Take care to fit the scaler only on the train data and not the test data. Then transform both train and test data using the same classifier to avoid data leakage. Next weâ€™ll model our neural network. We are creating a class calledNeuralNetthat has all the required functions. I am pasting the code here, as it is the most important part of our discussion. There are a total of three layers in the model. The first layer is the input layer and has 30 neurons for each of the 30 inputs. The second layer is the hidden layer, and it contains 14 neurons by default. The third layer is the output layer, and since we have two classes, 0 and 1, we require only one neuron in the output layer. The default learning rate is set as 0.001, and the number of iterations or epochs is 100. Remember, there is a huge difference between the terms epoch and iterations. Consider a dataset of 2,000 data points. We are dividing the data into batches of 500 data points and then training the model on each batch. The number of batches to be trained for the entire data set to be trained once is called iterations. Here, the number of iterations is four. The number of times the entire data set undergoes a cycle of forward propagation and backpropagation is called epochs. Since the data above is not divided into batches, iteration and epochs will be the same. The weights are initialized randomly. The bias weight is not added with the main input weights, it is maintained separately. Then the sigmoid activation function andcost functionof the neural network are defined. The cost function takes in the predicted output and the actual output as input, and calculates the cost. The forward propagation function is then defined. The activations of the input layer is calculated and passed on as input to the output layer. All the parameters are stored in a dictionary with suitable labels. The backpropagation function is defined. It takes in the predicted output to perform backpropagation using the stored parameter values. The gradients are calculated as we discussed above and the weights are updated in the end. The fit function takes in the input x and desired outputy. It calls the weight initialization, forward propagation and backpropagation function in that order and trains the model. The predict function is used to predict the output of the test set. The accuracy function can be used to test the performance of the model. There is also a function available for plotting the cost function vs epochs. We first train the model by fitting the train data that we extracted from the entire data set. We can see that the total cost is exponentially decreasing as we recursively train the model. The train and test accuracy is found to beTrain accuracy: [97.14285714] Test accuracy: [97.36842105]. The number of features available ensures we get such a high rate of accuracy. As more information regarding the target variable is available, the model accuracy increases. The above plot and metrics correspond to the default values. Now, letâ€™s change the default values a bit. The number of neurons in the hidden layer is increased from 14 to 20. The learning rate is decreased from 0.001 to 0.01. The number of iterations has increased from 100 to 500. We can see that the cost decreased to somewhere around 0.1 and then reduced gradually to almost zero. The final cost value after 500 iterations is less than the previous case. The train and test accuracies areTrain accuracy: [100.] Test accuracy: [97.36842105]. We can see that the train accuracy reached 100 percent and the test accuracy remained the same.  For further verification, weâ€™ll use two of the libraries associated with neural networks  We will be using sklearnâ€™sMLPClassifierfor modeling a neural network, training and testing it. The same parameters used above are being used here. There is one hidden layer consisting of 14 neurons. The learning rate is set as 0.001 and number of iterations as 100. The train and test accuracies are: We can see that the train accuracy has increased a bit, and the test accuracy has remained the same. This conveys that our model is in line with the sklearn model.  We will be modeling a sequential model using tf.keras. It contains two dense layers apart from the input layer. The hidden dense layer consists of 14 neurons, and the output dense layer consists of one neuron. The learning rate is set as 0.001 and binary cross-entropy loss is used. The number of epochs is set as 100. The train and test accuracies are: We can see that both the train and test accuracies have increased a bit. The reason for this might be a well-optimized backpropagation algorithm, which helps the model achieve higher accuracies in a fewerÂ  number of iterations. The tf.keras and sklearn models excels our model in the case of training time. When inputting data that has millions of data-points, the model that we built may take a lot of time to converge or reach acceptable accuracy levels. Whereas, due to the optimization techniques employed in the tf.keras and sklearn models, they may converge faster."
https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf,"Sign up Sign in Sign up Sign in Arthur Arnx Follow Towards Data Science -- 19 Listen Share So you want to create your first artificial neural network, or simply discover this subject, but have no idea where to begin ? Follow this quick guide to understand all the steps ! Based on nature, neural networks are the usual representation we make of the brain : neurons interconnected to other neurons which forms a network. A simple information transits in a lot of them before becoming an actual thing, like â€œmove the hand to pick up this pencilâ€. The operation of a complete neural network is straightforward : one enter variables as inputs (for example an image if the neural network is supposed to tell what is on an image), and after some calculations, an output is returned (following the first example, giving an image of a cat should return the word â€œcatâ€). Now, you should know that artificial neural network are usually put on columns, so that a neuron of the columnncan only be connected to neurons from columnsn-1andn+1.There are few types of networks that use a different architecture, but we will focus on the simplest for now. So, we can represent an artificial neural network like that : Neural networks can usually be read from left to right. Here, the first layer is the layer in which inputs are entered. There are 2 internals layers (called hidden layers) that do some math, and one last layer that contains all the possible outputs. Donâ€™t bother with the â€œ+1â€s at the bottom of every columns. It is something called â€œbiasâ€ and weâ€™ll talk about that later. By the way, the term â€œdeep learningâ€ comes from neural networks that contains several hidden layers, also called â€œdeep neural networksâ€ . The Figure 1 can be considered as one. The operations done by each neurons are pretty simple : First, it adds up the value of every neurons from the previous column it is connected to. On the Figure 2, there are 3 inputs (x1, x2, x3) coming to the neuron, so 3 neurons of the previous column are connected to our neuron. This value is multiplied, before being added, by another variable called â€œweightâ€ (w1, w2, w3) which determines the connection between the two neurons. Each connection of neurons has its own weight, and those are the only values that will be modified during the learning process. Moreover, a bias value may be added to the total value calculated. It is not a value coming from a specific neuron and is chosen before the learning phase, but can be useful for the network. After all those summations, the neuron finally applies a function called â€œactivation functionâ€ to the obtained value. The so-called activation function usually serves to turn the total value calculated before to a number between 0 and 1 (done for example by a sigmoid function shown by Figure 3). Other function exist and may change the limits of our function, but keeps the same aim of limiting the value. Thatâ€™s all a neuron does ! Take all values from connected neurons multiplied by their respective weight, add them, and apply an activation function. Then, the neuron is ready to send its new value to other neurons. After every neurons of a column did it, the neural network passes to the next column. In the end, the last values obtained should be one usable to determine the desired output. Now that we understand what a neuron does, we could possibly create any network we want. However, there are other operations to implement to make a neural network learn. Yep, creating variables and making them interact with each other is great, but that is not enough to make the whole neural network learn by itself. We need to prepare a lot of data to give to our network. Those data include the inputs and the output expected from the neural network. Letâ€™s take a look at how the learning process works : First of all, remember that when an input is given to the neural network, it returns an output. On the first try, it canâ€™t get the right output by its own (except with luck) and that is why, during the learning phase, every inputs come with its label, explaining what output the neural network should have guessed. If the choice is the good one, actual parameters are kept and the next input is given. However, if the obtained output doesnâ€™t match the label, weights are changed. Those are the only variables that can be changed during the learning phase. This process may be imagined as multiple buttons, that are turned into different possibilities every times an input isnâ€™t guessed correctly. To determine which weight is better to modify, a particular process, called â€œbackpropagationâ€ is done. We wonâ€™t linger too much on that, since the neural network we will build doesnâ€™t use this exact process, but it consists on going back on the neural network and inspect every connection to check how the output would behave according to a change on the weight. Finally, there is a last parameter to know to be able to control the way the neural network learns : the â€œlearning rateâ€. The name says it all, this new value determines on what speed the neural network will learn, or more specifically how it will modify a weight, little by little or by bigger steps. 1 is generally a good value for that parameter. Okay, we know the basics, letâ€™s check about the neural network we will create. The one explained here is called a Perceptron and is the first neural network ever created. It consists on 2 neurons in the inputs column and 1 neuron in the output column. This configuration allows to create a simple classifier to distinguish 2 groups. To better understand the possibilities and the limitations, letâ€™s see a quick example (which doesnâ€™t have much interest except to understand) : Letâ€™s say you want your neural network to be able to return outputs according to the rules of the â€œinclusive orâ€. Reminder : If you replace the â€œtrueâ€s by 1 and the â€œfalseâ€s by 0 and put the 4 possibilities as points with coordinates on a plan, then you realize the two final groups â€œfalseâ€ and â€œtrueâ€ may be separated by a single line. This is what a Perceptron can do. On the other hand, if we check the case of the â€œexclusive orâ€ (in which the case â€œtrue or trueâ€ (the point (1,1)) is false), then we can see that a simple line cannot separate the two groups, and a Perceptron isnâ€™t able to deal with this problem. So, the Perceptron is indeed not a very efficient neural network, but it is simple to create and may still be useful as a classifier. Letâ€™s create a neural network from scratch with Python (3.x in the example below). The beginning of the program just defines libraries and the values of the parameters, and creates a list which contains the values of the weights that will be modified (those are generated randomly). Here we create a function which defines the work of the output neuron. It takes 3 parameters (the 2 values of the neurons and the expected output). â€œoutputPâ€ is the variable corresponding to the output given by the Perceptron. Then we calculate the error, used to modify the weights of every connections to the output neuron right after. We create a loop that makes the neural network repeat every situation several times. This part is the learning phase. The number of iteration is chosen according to the precision we want. However, be aware that too much iterations could lead the network to over-fitting, which causes it to focus too much on the treated examples, so it couldnâ€™t get a right output on case it didnâ€™t see during its learning phase. However, our case here is a bit special, since there are only 4 possibilities, and we give the neural network all of them during its learning phase. A Perceptron is supposed to give a correct output without having ever seen the case it is treating. Finally, we can ask the user to enter himself the values to check if the Perceptron is working. This is the testing phase. The activation function Heaviside is interesting to use in this case, since it takes back all values to exactly 0 or 1, since we are looking for a false or true result. We could try with a sigmoid function and obtain a decimal number between 0 and 1, normally very close to one of those limits. We could also save the weights that the neural network just calculated in a file, to use it later without making another learning phase. It is done for way bigger project, in which that phase can last days or weeks. Thatâ€™s it ! Youâ€™ve done your own complete neural network. You created it, made it learn, and checked its capacities. Your Perceptron can now be modified to use it on another problem. Just change the points given during the iterations, adjust the number of loop if your case is more complex, and just let your Perceptron do the classification. Do you want to list 2 types of trees in the nearest forest and be able to determine if a new tree is type A or B ? Chose 2 features that can dissociate both types (for example height and width), and create some points for the Perceptron to place on the plan. Let it deduct a way to separate the 2 groups, and enter any new treeâ€™s point to know which type it is. You could later expand your knowledge and see about bigger and deeper neural network, that are very powerful ! There are multiple aspects we didnâ€™t treat, or just enough for you to get the basics, so donâ€™t hesitate to go further. I would love to write about more complex neural networks so stay tuned ! Thanks for reading ! I hope this little guide was useful, if you have any question and/or suggestion, let me know in the comments. -- -- 19 Your home for data science and AI. The worldâ€™s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals. Software Engineer, Paris, France Help Status About Careers Press Blog Privacy Terms Text to speech Teams"
https://de.wikipedia.org/wiki/K%C3%BCnstliches_neuronales_Netz,"KÃ¼nstliche neuronale Netze, auchkÃ¼nstliche neuronale Netzwerke, kurz:KNN(englischartificial neural network, ANN), sindNetzeauskÃ¼nstlichen Neuronen, die von den Netzwerken inspiriert wurden, die biologischeNeuroneninGehirnenbilden. Ein KNN wird vonkÃ¼nstlichen Neuronengebildet, die miteinander verbunden sind und in der Regel in Schichten organisiert werden. KNN werden beimMaschinellen Lerneneingesetzt. Damit kÃ¶nnenComputerProbleme lÃ¶sen, die zu kompliziert sind, um sie mit Regeln zu beschreiben, zu denen es aber vieleDatengibt, die als Beispiele fÃ¼r die gewÃ¼nschte LÃ¶sung dienen kÃ¶nnen. KNN bilden die Basis fÃ¼rDeep Learning, das ab 2006 erhebliche Fortschritte bei der Analyse von groÃŸen Datenmengen erlaubte. Erfolgreiche Anwendungen des Deep Learning sind beispielsweiseBilderkennungundSpracherkennung.[1]:11 KNNs sind Forschungsgegenstand sowohl des Maschinellen Lernens, welches ein Teilbereich derkÃ¼nstlichen Intelligenzist, als auch der interdisziplinÃ¤renNeuroinformatik. Das Nachbilden eines biologischenneuronalen NetzesvonNeuronenist eher Gegenstand derComputational Neuroscience. KNN werden beimmaschinellen Lerneneingesetzt. Damit kÃ¶nnenComputerProbleme lÃ¶sen, die zu kompliziert sind, um sie mit Regeln zu beschreiben, zu denen es aber vieleDatengibt, die als Beispiele fÃ¼r die gewÃ¼nschte LÃ¶sung dienen kÃ¶nnen. EinAlgorithmuspasst das zunÃ¤chst unwissende Netz so an die Beispieldaten an, dass es von ihnen auf neue FÃ¤lle verallgemeinern kann. Dieser Vorgang wird Training genannt. Das trainierte Netz kann fÃ¼r neue Daten Vorhersagen treffen oder Empfehlungen und Entscheidungen erzeugen.[1]:8 KNN wurden von den Netzwerken inspiriert, die biologischeNeuroneninGehirnenbilden. Biologische Neuronen sind miteinander vernetzt und in Schichten organisiert. Sie kÃ¶nnen mehrere Eingangssignale aufsummieren und geben nur dann ein Signal an andere Neuronen weiter, wenn die Summe der Eingangssignale einen Schwellenwert erreicht.[2]:37 Ein KNN wird vonkÃ¼nstlichen Neuronengebildet. KÃ¼nstliche Neuronen bilden ausgewÃ¤hlte Eigenschaften von biologischen Neuronen mit mathematischen Mitteln nach. Ein kÃ¼nstliches Neuron kann mehrere Eingangssignale aufsummieren. Jedes Eingangssignal wird gewichtet und kann dadurch die Summe der Eingangssignale unterschiedlich stark erhÃ¶hen oder reduzieren. DieAktivierungsfunktioneines kÃ¼nstlichen Neurons sorgt dafÃ¼r, dass es nur dann ein Ausgangssignal ausgibt, wenn die Summe aller Eingangssignale einen Schwellenwert Ã¼berschreitet. Wenn die Summe aller Eingangssignale unter dem Schwellenwert liegt, gibt das kÃ¼nstliche Neuron kein Ausgangssignal aus.[3][2]:37 In der Regel besteht ein KNN aus mehreren Schichten von  kÃ¼nstlichen Neuronen. Die Signale wandern von der ersten Schicht (der Eingabeschicht) zur letzten Schicht (der Ausgabeschicht) und durchlaufen dabei mÃ¶glicherweise mehrere Zwischenschichten (versteckte Schichten). Jede Schicht kann die Signale an ihren EingÃ¤ngen unterschiedlich transformieren. Ein Netz mit vielen verborgenen Schichten kann eine komplizierte Aufgabe in mehrere einfachere Aufgaben zerlegen, die jeweils in verschiedenen Schichten des Modells ausgefÃ¼hrt werden.[3]Ein solches KNN wird auch als tiefes neuronales Netz bezeichnet. Darauf bezieht sich der Begriff â€žDeep Learningâ€œ.[4] Zu Beginn desTrainingsstehen alle Schwellenwerte und Gewichte auf Zufallswerten. WÃ¤hrend des Trainings passt ein Algorithmus schrittweise Schwellenwerte und Gewichte an die Daten an, mit denen das Netz trainiert wird, siehe auchOptimierung. Die resultierenden Gewichte werden alsParameter des KI-Modellsbezeichnet. Das Training wird beendet, wenn das Netz fÃ¼r alle Beispiele aus den Trainingsdaten eine mÃ¶glichst korrekte Ausgabe erzeugt.[3] Das Erstellen eines geeigneten Trainingsdatensatzes kann sehr schwierig sein, da man verhindern muss, dass die Daten Muster aufweisen, die das Netz nicht zur Entscheidung heranziehen soll. Das ForschungsgebietExplainable Artificial Intelligencearbeitet an Verfahren, die erklÃ¤ren kÃ¶nnen, wie ein trainiertes KNN zu einer Entscheidung kommt. Beispielsweise wurde untersucht, welchen Teil eines Bildes ein KNN betrachtet, um ein Bild zu klassifizieren. Dabei entdeckten die Forscher z.Â B. ein KNN, das EisenbahnzÃ¼ge erkennen sollte und dazu nur auf Schienen achtete.[2]:54 KÃ¼nstliche neuronale Netze basieren meist auf der Vernetzung vielerMcCulloch-Pitts-Neuronenoder leichter Abwandlungen davon. DieTopologieeines Netzes (die Zuordnung von Verbindungen zu Knoten) muss im Hinblick auf seine Aufgabe gut durchdacht sein. Nach der Konstruktion eines Netzes folgt die Trainingsphase, in der das Netz â€žlerntâ€œ. Theoretisch kann ein Netz durch folgende Methoden lernen: AuÃŸerdem verÃ¤ndert sich das Lernverhalten bei VerÃ¤nderung derAktivierungsfunktionder Neuronen oder der Lernrate des Netzes. Praktisch gesehen â€žlerntâ€œ ein Netz hauptsÃ¤chlich durch Modifikation der Gewichte der Verbindungen zwischen den Neuronen. Eine Anpassung des Schwellenwertes kann hierbei durch einon-Neuronmiterledigt werden. Dadurch sind KNNs in der Lage, kompliziertenichtlineareFunktionenÃ¼ber einen â€žLernâ€œ-Algorithmus, der durchiterativeoderrekursiveVorgehensweise aus vorhandenen Ein- und gewÃ¼nschten Ausgangswerten alleParameterder Funktion zu bestimmen versucht, zu erlernen. KNNs sind dabei eine Realisierung deskonnektionistischenParadigmas, da die Funktion aus vielen einfachen gleichartigen Teilen besteht. Erst in ihrer Summe kann deren Verhalten im Zusammenspiel sehr vieler beteiligter Teile komplex werden. Ein neuronales Netz, das deterministisch beschrieben wird und RÃ¼ckkopplungen erlaubt, stellt unter dem Gesichtspunkt derBerechenbarkeitein Ã¤quivalentes Modell zurTuringmaschinedar.[5]D.h. zu jedem Netz gibt es mindestens eine Turingmaschine und zu jeder Turingmaschine gibt es mindestens ein Netz mit RÃ¼ckkopplung. Bei einer stochastischen Beschreibung ist dies nicht der Fall. Rekurrente Netze sind damit die ausdrucksstÃ¤rkste Form (Typ 0 in derChomsky-Hierarchie). Das Interesse fÃ¼r kÃ¼nstliche neuronale Netze setzte bereits in den frÃ¼hen1940erJahren ein, also etwa gleichzeitig mit dem Einsatz programmierbarerComputerin angewandter Mathematik.[6] Die AnfÃ¤nge gehen aufWarren McCullochundWalter PittszurÃ¼ck. Sie beschrieben 1943 VerknÃ¼pfungen von elementaren Einheiten zu einem Netz Ã¤hnlich dem der Neuronen im Gehirn, mit dem sich praktisch jede logische oder arithmetische Funktion berechnen lassen kÃ¶nnte.[7]1947 wiesen sie darauf hin, dass ein solches Netz beispielsweise zur rÃ¤umlichen Mustererkennung eingesetzt werden kann. 1949 formulierteDonald O. HebbseineHebbsche Lernregel, die in ihrer allgemeinen Form die meisten der kÃ¼nstlichen neuronalen Lernverfahren wiedergibt.Karl Lashleyfand 1950, dass der Prozess der Informationsspeicherung im Gehirn verteilt auf verschiedene Untereinheiten realisiert wird.[8] Im anschlieÃŸenden Jahr, 1951, gelingtMarvin Minskymit seiner Dissertationsarbeit der Bau des NeurocomputersSnarc, der seine Gewichte automatisch justieren kann, jedoch nicht praktisch einsetzbar ist.[8]1956 treffen sich Wissenschaftler und Studenten auf derDartmouth Conference. Diese Konferenz gilt als Geburtsstunde der KÃ¼nstlichen Intelligenz als akademisches Fachgebiet.[9]Von 1957 bis 1958 entwickelnFrank RosenblattundCharles Wightmanden ersten erfolgreichen Neurocomputer, mit dem NamenMark I Perceptron. Der Computer konnte mit seinem 20 Ã— 20 Pixel groÃŸen Bildsensor bereits einfache Ziffern erkennen. Im nachfolgenden Jahr formuliert Rosenblatt dasPerceptron-Konvergenz-Theorem. 1960 stellenBernard WidrowundMarcian E. HoffdasADALINE(ADAptive LInear NEuron) vor.[10]Dieses Netz erreichte als erstes weite kommerzielle Verbreitung. Anwendung fand es in Analogtelefonen zur Echtzeit-Echofilterung. Das neuronale Netz lernte mit derDeltaregel. 1961 stellteKarl SteinbuchTechniken der assoziativen Speicherung vor. 1969 gaben Marvin Minsky undSeymour Paperteine genaue mathematische Analyse desPerceptrons.[11]Sie zeigten auf, dass wichtige Probleme nicht gelÃ¶st werden kÃ¶nnen. So sind unter anderemXOR-Operatorennicht auflÃ¶sbar und es gibt Probleme in derlinearen Separierbarkeit. Die Folge war ein vorlÃ¤ufiges Ende der Forschungen auf dem Gebiet der neuronalen Netze, da die meisten Forschungsgelder gestrichen wurden. 1972 prÃ¤sentiertTeuvo Kohonendenlinearen Assoziator, ein Modell des Assoziativspeichers.[12]James A. Andersonbeschreibt das Modell unabhÃ¤ngig von Kohonen aus neuropsychologischer Sicht im selben Jahr.[13]1973 benutztChristoph von der MalsburgeinNeuronenmodell, das nichtlinear ist. Bereits 1974 entwickeltPaul WerbosfÃ¼r seine Dissertation dieBackpropagationbzw. die FehlerrÃ¼ckfÃ¼hrung. Das Modell bekam aber erst spÃ¤ter eine grÃ¶ÃŸere Bedeutung. Ab 1976 entwickeltStephen Grossbergmathematisch fundierte Modelle neuronaler Netze. Zusammen mitGail Carpenterwidmet er sich auch dem Problem, ein neuronales Netz lernfÃ¤hig zu halten, ohne bereits Gelerntes zu zerstÃ¶ren. Sie formulieren ein Architekturkonzept fÃ¼r neuronale Netze, dieAdaptive Resonanztheorie. 1982 beschreibt Teuvo Kohonen die nach ihm benanntenselbstorganisierenden Karten. Im selben Jahr beschreibtJohn Hopfielddas Modell derHopfield-Netze. 1983 wird vonKunihiko Fukushima, S. Miyake und T. Ito das neuronale ModellNeocognitronvorgestellt. Das Modell war eine Weiterentwicklung des 1975 entwickeltenCognitronsund diente zur Erkennung handgeschriebener Zeichen. 1985 verÃ¶ffentlichtJohn Hopfieldeine LÃ¶sung desTravelling Salesman Problemsdurch einHopfield-Netz. 1985 wird das LernverfahrenBackpropagation of Errorals Verallgemeinerung derDelta-Regeldurch dieParallel-Distributed-Processing-Gruppe separat entwickelt. Somit werden nichtlinear separierbare Problemedurch mehrschichtigePerceptronslÃ¶sbar.MinskysAuffassung war also widerlegt. In jÃ¼ngster Zeit erlebten neuronale Netze eine Wiedergeburt, da sie bei herausfordernden Anwendungen oft bessere Ergebnisse als konkurrierende Lernverfahren liefern. Zwischen 2009 und 2012 gewannen dierekurrentenbzw. tiefen vorwÃ¤rtsgerichteten neuronalen Netze der Forschungsgruppe vonJÃ¼rgen SchmidhuberamSchweizer KI Labor IDSIAeine Serie von acht internationalen Wettbewerben in den BereichenMustererkennungundmaschinelles Lernen.[14]Insbesondere gewannen ihre rekurrentenLSTM-Netze[15][16]drei Wettbewerbe zur verbundenen Handschrifterkennung bei derIntl. Conf. on Document Analysis and Recognition (ICDAR)2009 â€“ obwohl kein A-priori-Wissen Ã¼ber die drei verschiedenen zu lernenden Sprachen in die Programmierung der Modelle einbezogen wurde. Die LSTM-Netze erlernten gleichzeitigeSegmentierungundErkennung. Dies waren die ersten internationalen Wettbewerbe, die durchDeep Learning[17][18]oder durch rekurrente Netze gewonnen wurden. Tiefe vorwÃ¤rtsgerichtete Netze wie Kunihiko FukushimasKonvolutionsnetzder 80er Jahre[19]haben wieder an Bedeutung gewonnen. Sie verfÃ¼gen Ã¼ber alternierendeKonvolutionslagen(convolutional layers) und Lagen von Neuronen, die mehrere Aktivierungen zusammenfassen (pooling layers[20]), um die rÃ¤umlicheDimensionzu reduzieren. Abgeschlossen wird ein solches Konvolutionsnetz in der Regel durch mehrerevollstÃ¤ndig verbundene Schichten(englischfully connected layers).Yann LeCunsTeam von derNew York Universitywandte den 1989 schon gut bekanntenBackpropagation-Algorithmus auf solche Netze an.[21]Moderne Varianten verwenden sogenanntesmax-poolingfÃ¼r die Zusammenfassung der Aktivierungen, das stets der stÃ¤rksten Aktivierung den Vorzug gibt.[22]SchnelleGrafikprozessor (GPU)-Implementierungen dieser Kombination wurden 2011 durch Dan Ciresan und Kollegen in Schmidhubers Gruppe eingefÃ¼hrt.[23]Sie gewannen seither zahlreiche Wettbewerbe, u.Â a. die â€žISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks Challengeâ€œ[24]und den â€žICPR 2012 Contest on Mitosis Detection in Breast Cancer Histological Imagesâ€œ.[25]Derartige Modelle erzielten auch die bisher besten Ergebnisse auf demImageNetBenchmark.[26][27]GPU-basiertemax-pooling-Konvolutionsnetze waren auch die ersten kÃ¼nstlichen Mustererkenner mit Ã¼bermenschlicher Performanz[28]in Wettbewerben wie der â€žIJCNN 2011 Traffic Sign Recognition Competitionâ€œ.[29]In den letzten Jahren fand auch die Theorie derZufallsmatrizenvermehrt Anwendung in der Erforschung von neuronalen Netzen.[30]NeuronaleOperatorensind Verallgemeinerungen von kÃ¼nstlichen neuronalen Netzwerken auf unendlichdimensionaleFunktionenrÃ¤ume. Die primÃ¤re Anwendung von neuronalen Operatoren liegt darin LÃ¶sungsoperatoren vonpartiellen Differentialgleichungenzu erlernen.[31] In kÃ¼nstlichen neuronalen Netzen bezeichnet die Topologie die Struktur des Netzes. Damit ist im Allgemeinen gemeint, wie vielekÃ¼nstliche Neuronensich auf wie vielen Schichten befinden, und wie diese miteinander verbunden sind. KÃ¼nstliche Neuronen kÃ¶nnen auf vielfÃ¤ltige Weise zu einem kÃ¼nstlichen neuronalen Netz verbunden werden. Dabei werden Neuronen bei vielen Modellen in hintereinander liegenden Schichten (englischlayers) angeordnet; bei einem Netz mit nur einer trainierbaren Neuronenschicht spricht man von einemeinschichtigen Netz. Unter Verwendung einesGraphenkÃ¶nnen die Neuronen alsKnotenund ihre Verbindungen alsKantendargestellt werden. Die Eingaben werden gelegentlich auch als Knoten dargestellt, was hilft, den Informationsfluss durch das Netzwerk zu visualisieren. Die hinterste Schicht des Netzes, deren Neuronenausgaben meist als einzige auÃŸerhalb des Netzes sichtbar sind, wirdAusgabeschicht(englischoutput layer) genannt. Davorliegende Schichten werden entsprechend alsversteckte Schicht(englischhidden layer) bezeichnet. Die Struktur eines Netzes hÃ¤ngt unmittelbar mit dem verwendeten Lernverfahren zusammen und umgekehrt; so kann mit derDelta-Regelnur ein einschichtiges Netz trainiert werden, bei mehreren Schichten ist eine leichte Abwandlung vonnÃ¶ten. Dabei mÃ¼ssen Netze nicht zwingend homogen sein: es existieren auch Kombinationen aus verschiedenen Modellen, um so unterschiedliche Vorteile zu kombinieren. Es gibt reinefeedforward-Netze, bei denen eine Schicht immer nur mit der nÃ¤chsthÃ¶heren Schicht verbunden ist. DarÃ¼ber hinaus gibt es Netze, in denen Verbindungen in beiden Richtungen erlaubt sind. Die passende Netzstruktur wird meist nach der Methode vonVersuch und Irrtumgefunden, was durchevolutionÃ¤re Algorithmenund eineFehlerrÃ¼ckfÃ¼hrungunterstÃ¼tzt werden kann. Einschichtige Netze mit derfeedforward-Eigenschaft (englisch fÃ¼rvorwÃ¤rts) sind die einfachsten Strukturen kÃ¼nstlicher neuronaler Netze. Sie besitzen lediglich eine Ausgabeschicht. Diefeedforward-Eigenschaft besagt, dass Neuronenausgaben nur in Verarbeitungsrichtung geleitet werden und nicht durch eine rekurrente Kante zurÃ¼ckgefÃ¼hrt werden kÃ¶nnen (azyklischer,gerichteter Graph). Mehrschichtige Netze besitzen neben der Ausgabeschicht auch verdeckte Schichten, deren Ausgabe wie beschrieben, auÃŸerhalb des Netzes nicht sichtbar sind. Verdeckte Schichten verbessern die Abstraktion solcher Netze. So kann erst das mehrschichtigePerzeptrondasXOR-Problem lÃ¶sen. Rekurrente Netzebesitzen im Gegensatz dazu auch rÃ¼ckgerichtete (rekurrente) Kanten (englischfeedback loops) und enthalten somit eineRÃ¼ckkopplung. Solche Kanten werden dann immer mit einer ZeitverzÃ¶gerung (in der Systemtheorie als VerzÃ¶gerungsglied bezeichnet) versehen, sodass bei einer schrittweisen Verarbeitung die Neuronenausgaben der vergangenen Einheit wieder als Eingaben angelegt werden kÃ¶nnen. Diese RÃ¼ckkopplungen ermÃ¶glichen einem Netz ein dynamisches Verhalten und statten es mit einemGedÃ¤chtnisaus. Dynamische neuronale Netze passen die Struktur und/oder Parameter dynamisch je nach Input bei der Inferenz an.[32] Neuronale Netze mit GedÃ¤chtnis verfÃ¼gen Ã¼ber einen statischen Speicher, der bei der Inferenz dynamisch abgefragt werden kann. KÃ¼nstliche neuronale Netze dienen als universelle Funktionsapproximatoren. Werte werden dabei von der Eingabe- bis zur Ausgabeschicht propagiert, wobei eine Aktivierungsfunktion fÃ¼r NichtlinearitÃ¤t sorgt. Beim Trainieren wird ein Fehler bestimmt; mit Hilfe von FehlerrÃ¼ckfÃ¼hrung und einem Optimierungsverfahren werden dabei die Gewichte schichtweise angepasst.[33] Ein kÃ¼nstliches Neuron erhÃ¤lt Eingaben von anderen Neuronen (oder von auÃŸen), wie auf dem Bild rechts zu erkennen ist. Der Wert einer Eingabe hÃ¤ngt vom AktivitÃ¤tslevel des sendenden Neurons und vom Gewichtwij{\displaystyle w_{ij}}der Verbindung zwischen Sender- und EmpfÃ¤nger-Neuron ab. DiePropagierungsfunktion(Ãœbertragungsfunktion) errechnet aus den einzelnen Eingaben und ihren Gewichten die Gesamteingabenetj{\displaystyle net_{j}}fÃ¼r das Neuron, dieNetz-Eingabeoder Netz-Input genannt wird. Am hÃ¤ufigsten wird die gewichtete Summe (Linearkombination) verwendet: netj=âˆ‘i=1nxiwij.{\displaystyle {\mbox{net}}_{j}=\sum \limits _{i=1}^{n}x_{i}w_{ij}.} Welches AktivitÃ¤tsleveloj{\displaystyle o_{j}}das Neuron annimmt, wird nicht direkt durch die Netz-Eingabe bestimmt, sondern durch dieAktivierungsfunktion. Dieses AktivitÃ¤tslevel bestimmt dann die Ausgabe des Neurons an Neuronen der nÃ¤chsten Schicht. oj=Ï†(netj){\displaystyle o_{j}=\varphi ({\mbox{net}}_{j})} Die Aktivierungsfunktion ermÃ¶glicht die EinfÃ¼hrung von NichtlinearitÃ¤t ins neuronale Netz, denn nicht alle Aufgaben neuronaler Netze lassen sich mit linearen Funktionen abbilden. Es gibt verschiedene Aktivierungsfunktionen. In den verdeckten Schichten wird meist dieRectifier-Funktionverwendet, in der Ausgabeschicht eineSigmoidfunktion. Hierbei ist Es existieren auch Ausgabefunktionen, meist wird jedoch einfach das Ergebnis der Aktivierungsfunktion zurÃ¼ckgegeben. Mit Hilfe von verbundenen Neuronen, die die Propagierungs- und Aktivierungsfunktionen anwenden, gibt das neuronale Netz einen Zahlenvektor aus. Inwieweit das Ergebnis aus dem KNN von dem Zielwert abweicht, wird mit Hilfe einer Fehlerfunktion bestimmt. Es gibt verschiedene Arten von Fehlerfunktionen. Eine davon ist der mittlere quadratische Fehler (MQF): Dabei ist Der Faktor12{\displaystyle {\tfrac {1}{2}}}wird dabei lediglich zur Vereinfachung bei der Ableitung hinzugenommen. Der MQF eignet sich, wenn die RÃ¼ckgabe des Netzes ein einzelner Wert ist. Mit Hilfe des Fehlers lassen sich die Gewichte anpassen. Dies geschieht in zwei Schritten: Im ersten Schritt werden mit Hilfe der FehlerrÃ¼ckfÃ¼hrung die Gradienten bestimmt. Im zweiten Schritt werden die Gradienten mit einem Optimierungsverfahren verwendet, um die Gewichte zu aktualisieren. In diesem Abschnitt geht es um die FehlerrÃ¼ckfÃ¼hrung. Die Idee hinter der FehlerrÃ¼ckfÃ¼hrung ist, die Gradienten schichtweise zu berechnen. Dies geschieht Ã¼ber die Kettenregel: âˆ‚Eâˆ‚wij=âˆ‚Eâˆ‚ojâˆ‚ojâˆ‚netjâˆ‚netjâˆ‚wij.{\displaystyle {\dfrac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\mbox{net}}_{j}}}{\frac {\partial {\mbox{net}}_{j}}{\partial w_{ij}}}.} Es ist wichtig, das Training mit zufÃ¤lligen Anfangswerten zu beginnen. Bei Ã¼berall gleichen Anfangswerten hÃ¤tte beispielsweise jeder Knoten im KNN das gleiche Gewicht. Wenn dann Gewichte durch die FehlerrÃ¼ckfÃ¼hrung angepasst werden, wÃ¼rde der Fehler auf alle Knoten gleich verteilt und es wÃ¼rden auch alle Gewichte gleichmÃ¤ÃŸig verÃ¤ndert. Da das Problem in der Regel nicht symmetrisch ist, dÃ¼rfen auch die Anfangswerte keine Symmetrien aufweisen, sonst kann der Lernalgorithmus das KNN nicht gut an die Trainingsdaten anpassen.[34]:190 Mit der FehlerrÃ¼ckfÃ¼hrung wurden Fehler und Gewichte in einer Funktion abgebildet. Das Lernen korrespondiert nun zu einer Minimierung der Fehlerfunktion, indem die Gewichte angepasst werden. Das aus der Schule bekannte OptimalitÃ¤tskriterium 1. Ordnung, das Nullsetzen der Ableitung, ist bei Neuronalen Netzen praktisch jedoch ungeeignet. Stattdessen wird mit Gradientenverfahren gearbeitet, um ein lokales Minimum der Fehlerfunktion zu finden. Der Gradient ist die Richtung des steilsten Anstieges einer Funktion, eine Bewegung entgegen den Gradienten ermÃ¶glicht also einen Abstieg auf dem Graphen der Fehlerfunktion. Eine Iteration des naiven Gradientverfahrens oder Gradientenabstieges ist also die Berechnung des Gradienten fÃ¼r die aktuelle Wahl der Gewichte, anschlieÃŸend wird von den Gewichten der Gradient abgezogen und so neue Gewichte mit niedrigerem Fehler erhalten. Das Netz hat somit also â€žgelerntâ€œ. Dieses Update bzw. dieser Abstiegsschritt wird durch folgende Zuweisung notiert: wi=wiâˆ’Î±âˆ‚Eâˆ‚wi.{\displaystyle w_{i}=w_{i}-\alpha {\frac {\partial E}{\partial w_{i}}}.}Dabei beschreibt der WertÎ±{\displaystyle \alpha }die Lernrate. Dieser gibt an, wie groÃŸ die Schritte sind, die das Verfahren in Richtung des Minimums nehmen soll. Zur Berechnung des Gradienten wird die FehlerrÃ¼ckfÃ¼hrung (engl. Backpropagation) verwendet. Das Verfahren wird solange wiederholt, bis ein Abbruchkriterium erfÃ¼llt ist, idealerweise durch Konvergenz zu einem lokalen Minimum. Neben dem hier dargestellten naiven Gradientenverfahren werden in der Praxis meist besser entwickelte und leistungsfÃ¤higere Variationen des Gradientenabstieges verwendet, z.Â B. der stochastische Gradientenabstieg oder das ADAM-Verfahren. Seine besonderen Eigenschaften machen das KNN bei allen Anwendungen interessant, bei denen kein oder nur geringesexplizites (systematisches) WissenÃ¼ber das zu lÃ¶sende Problem vorliegt. Dies sind z.Â B. dieTexterkennung,Spracherkennung,BilderkennungundGesichtserkennung, bei denen einige Hunderttausend bis MillionenBildpunktein eine im Vergleich dazu geringe Anzahl von erlaubten Ergebnissen Ã¼berfÃ¼hrt werden mÃ¼ssen. Auch in derRegelungstechnikkommen KNN zum Einsatz, um herkÃ¶mmlicheReglerzu ersetzen oder ihnenSollwertevorzugeben, die das Netz aus einer selbst entwickeltenPrognoseÃ¼ber denProzessverlaufermittelt hat. So kÃ¶nnen auchFuzzy-Systeme durch eine bidirektionale Umwandlung in neuronale Netze lernfÃ¤hig gestaltet werden. Die AnwendungsmÃ¶glichkeiten sind aber nicht auf techniknahe Gebiete begrenzt: Bei der Vorhersage von VerÃ¤nderungen in komplexen Systemen werden KNNs unterstÃ¼tzend hinzugezogen, z.Â B. zur FrÃ¼herkennung sich abzeichnenderTornadosoder aber auch zur AbschÃ¤tzung der weiteren Entwicklung wirtschaftlicher Prozesse. Zu den Anwendungsgebieten von KNNs gehÃ¶ren insbesondere: Trotz dieser sehr groÃŸen Spanne an Anwendungsgebieten gibt es Bereiche, die KNNs aufgrund ihrer Natur nicht abdecken kÃ¶nnen, beispielsweise:[35] WÃ¤hrend dasGehirnzur massiven Parallelverarbeitung in der Lage ist, arbeiten die meisten heutigen Computersysteme nur sequentiell (bzw. partiell parallel eines Rechners). Es gibt jedoch auch erste Prototypen neuronaler Rechnerarchitekturen, sozusagen den neuronalen Chip, fÃ¼r die das Forschungsgebiet der kÃ¼nstlichen neuronalen Netze die theoretischen Grundlagen bereitstellt. Dabei werden diephysiologischen VorgÃ¤ngeim Gehirn jedoch nicht nachgebildet, sondern nur die Architektur der massiv parallelen Analog-Addierer in Silizium nachgebaut, was gegenÃ¼ber einer Software-Emulation eine bessere Leistung verspricht. GrundsÃ¤tzlich unterscheiden sich die Klassen der Netze vorwiegend durch die unterschiedlichen Netztopologien und Verbindungsarten, so zum Beispiel einschichtige, mehrschichtige, Feedforward- oder Feedback-Netze. Jede verdeckte Schicht und die Ausgabeschicht bzw. deren Neuronen verfÃ¼gen Ã¼ber eine (eigene) Aktivierungsfunktion. Diese kÃ¶nnen linear oder nicht-linear sein. Nicht-lineare Aktivierungsfunktionen machen das Netz besonders mÃ¤chtig.[36] Lernverfahren dienen dazu, ein neuronales Netz so zu modifizieren, dass es fÃ¼r bestimmte Eingangsmuster zugehÃ¶rige Ausgabemuster erzeugt. Dies geschieht grundsÃ¤tzlich auf drei verschiedenen Wegen. Beim Ãœberwachten Lernen wird dem KNN ein Eingangsmuster gegeben und die Ausgabe, die das neuronale Netz in seinem aktuellen Zustand produziert, mit dem Wert verglichen, den es eigentlich ausgeben soll. Durch Vergleich von Soll- und Istausgabe kann auf die vorzunehmenden Ã„nderungen der Netzkonfiguration geschlossen werden. Bei einlagigen Perzeptrons kann dieDelta-Regel(auch Perzeptron-Lernregel) angewendet werden. Mehrlagige Perzeptrons werden in der Regel mitBackpropagationtrainiert, was eine Verallgemeinerung der Delta-Regel darstellt. Das UnÃ¼berwachte Lernen erfolgt ausschlieÃŸlich durch Eingabe der zu lernenden Muster. Das neuronale Netz verÃ¤ndert sich entsprechend den Eingabemustern von selbst. Hierbei gibt es folgende Lernregeln: Es ist nicht immer mÃ¶glich, zu jedem Eingabedatensatz den passenden Ausgabedatensatz zum Trainieren zur VerfÃ¼gung zu haben. Zum Beispiel kann man einem Agenten, der sich in einer fremden Umgebung zurechtfinden muss â€“ etwa einem Roboter auf dem Mars â€“ nicht zu jedem Zeitpunkt sagen, welche Aktion jeweils die beste ist. Aber man kann dem Agenten eine Aufgabe stellen, die dieser selbststÃ¤ndig lÃ¶sen soll. Nach einem Testlauf, der aus mehreren Zeitschritten besteht, kann der Agent bewertet werden. Aufgrund dieser Bewertung kann eine Agentenfunktion gelernt werden. Der Lernschritt kann durch eine Vielzahl von Techniken vollzogen werden. Unter anderem kÃ¶nnen hier auch kÃ¼nstliche neuronale Netze zum Einsatz kommen. DieHauptnachteilevon KNN sind gegenwÃ¤rtig: Der Text ist unter der Lizenzâ€žCreative-Commons Namensnennung â€“ Weitergabe unter gleichen Bedingungenâ€œverfÃ¼gbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) kÃ¶nnen im Regelfall durch Anklicken dieser abgerufen werden. MÃ¶glicherweise unterliegen die Inhalte jeweils zusÃ¤tzlichen Bedingungen. Durch die Nutzung dieser Website erklÃ¤ren Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden."
https://www.coursera.org/articles/how-do-neural-networks-work,"Learn how neural networks work and what makes them foundational for deep learning and artificial intelligence. Neural networks are a foundational deep learning and artificial intelligence (AI) element. Sometimes called artificial neural networks (ANNs), they aim to function similarly to how the human brain processes information and learns. Neural networks form the foundation of deep learning, a type of machine learning that uses deep neural networks. Artificial neural networks were first introduced in the early 1940s when Warren McCulloch and Walter Pitts studied how neurons work in the human brain, creating a simple binary device to show their findings. However, it wasnâ€™t until computer hardware developments in the 1980s and later in the 2010s that the ANNs in deep learning you see today were possible. Research continues on ANNs because they are vital to developing better AI. How do neural networks work? Continue reading to learn the answer to that question and get details about how ANNs function, their structure, how theyâ€™re trained, and how they work with AI. Artificial neural networks are computational processing systems containing many simple processing units called nodes that interact to perform tasks. Each node in the neural network focuses on one aspect of the problem, interacting like human neurons by each sharing their findings. Unlike computational algorithms, in which a programmer tells the computer how to process input data, neural networks use input and output data to discover what factors lead to generating the output data. It creates a machine learning algorithm that makes predictions when fed new input data. ANNs train on new data, attempting to make each prediction more accurate by continually training each node. One way to understand how ANNs work is to examine how neural networks work in the human brain. The history of ANNs comes from biological inspiration and extensive study on how the brain works to process information. An individual neuron is a cell with an input and output structure. The input structure of a neuron is formed by dendrites, which receive signals from other nerve cells. The output structure is an axon that branches out from the cell body, connecting to the dendrites of another neuron via a synapse. Neurons communicate using electrochemical signals. Neurons only fire an output signal if the input signal meets a certain threshold in a specified amount of time. ANNs operate similarly. They receive input signals that reach a threshold using sigmoid functions, process the information, and then generate an output signal. Like human neurons, ANNs receive multiple inputs, add them up, and then process the sum with a sigmoid function. If the sum fed into the sigmoid function produces a value that works, that value becomes the output of the ANN. This is the structure of an individual neuron in an ANN, but networks have multiple layers and neurons that create the network. The structure of an entire artificial neural network consists of:  Input layer:takes in the input data and transfers it to the second (hidden) layer of neurons using synapses. An input layer has as many nodes as features or columns of data in the matrix.  Hidden layer:takes data from the input layer to categorize or detect desired aspects of the data. Nodes in the hidden layer send the data to more hidden layers or, finally, to the output layer. The hidden layer of an ANN is a â€œblack boxâ€ because researchers cannot determine its results.  Output layer:takes data from the hidden layer and outputs the results. It has as many nodes as the model desires.  Synapses:connect nodes in layers and in between layers.  Deep neural networks, which are used in deep learning, have a similar structure to a basic neural network, except they use multiple hidden layers and require significantly more time and data to train. ANNs require training to produce accurate output values. Training begins with the network processing large data samples with already known outputs. ANNs undergo supervised learning using labeled data sets with known answers. Once the neural network builds a knowledge base, it tries to produce a correct answer from an unknown piece of data. ANNs use a â€œweight,â€ which is the strength of the connection between nodes in the network. During training, ANNs assign a high or low weight, strengthening the signal as the weight between nodes increases. The weight adjusts as it learns through a gradient descent method that calculates an error between the actual value and the predicted value. Throughout training, the error becomes smaller as the weight between connections increases. Neural networks vary in type based on how they process information and how many hidden layers they contain. Three types of neural networks include the following:  Feed-forward neural networks Backpropagation neural networks Convolution neural networks  Letâ€™s take a closer look at how each neural network type works. These neural networks constitute the most basic form of an artificial neural network. They send data in one forward direction from the input node to the output node in the next layer. They do not require hidden layers but sometimes contain them for more complicated processes. They learn over time through feedback processes. Facial recognition is an example of a feed-forward network. Backpropagation neural networks work continuously by having each node remember its output value and run it back through the network to create predictions in each layer. This allows for the network to learn and improve predictions continuously. These networks create a feedback loop called backpropagation. It starts like a feed-forward ANN, and if an answer is correct, it adds more weight to the pathway. If it is wrong, the network re-attempts the prediction until it becomes closer to the right answer. An example of this ANN is in speech-to-text-to-speech algorithms. Convolution neural networks use hidden layers to perform mathematical functions to create feature maps of image regions that are easier to classify. Each hidden layer gets a specific portion of the image to break down for further analysis, eventually leading to a prediction of what the image is. An example of convolution neural networks is AI image recognition. Well-trained, accurate neural networks are a key component of AI because of the speed at which they interact with data. If the ultimate goal of AI is an artificial intelligence of human capabilities, ANNs are an essential step in that process. Understanding how neural networks operate helps you understand how AI works since neural networks are foundational to AI's learning and predictive algorithms. Artificial neural networks are vital to creating AI and deep learning algorithms. Learn more about how neural networks work with online courses. For example, you can gain skills in developing, training, and building neural networks. Consider exploring theDeep Learning Specializationfrom DeepLearning.AI on Coursera.                   Editorial Team Courseraâ€™s editorial team is comprised of highly experienced professional editors, writers, and fact... This content has been made available for informational purposes only. Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals.   Coursera Footer Get Started with AI Popular Career Certificates Popular Subjects Popular Resources Coursera Community More"
https://nttuan8.com/bai-3-neural-network/,Error: 406 Client Error: Not Acceptable for url: https://nttuan8.com/bai-3-neural-network/
https://zh.wikipedia.org/zh-tw/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C,é¡žç¥žç¶“ç¶²è·¯ï¼ˆè‹±èªžï¼šartificial neural networkï¼ŒANNsï¼‰åˆç¨±äººå·¥ç¥žç¶“ç¶²çµ¡ï¼Œç°¡ç¨±ç¥žç¶“ç¶²è·¯ï¼ˆneural networkï¼ŒNNsï¼‰ï¼Œåœ¨æ©Ÿå™¨å­¸ç¿’å’ŒèªçŸ¥ç§‘å­¸é ˜åŸŸï¼Œæ˜¯ä¸€ç¨®æ¨¡ä»¿ç”Ÿç‰©ç¥žç¶“ç¶²è·¯ï¼ˆå‹•ç‰©çš„ä¸­æ¨žç¥žç¶“ç³»çµ±ï¼Œç‰¹åˆ¥æ˜¯å¤§è…¦ï¼‰çš„çµæ§‹å’ŒåŠŸèƒ½çš„æ•¸å­¸æ¨¡åž‹æˆ–è¨ˆç®—æ¨¡åž‹ï¼Œç”¨æ–¼å°å‡½å¼é€²è¡Œä¼°è¨ˆæˆ–è¿‘ä¼¼ã€‚ç¥žç¶“ç¶²è·¯ç”±å¤§é‡çš„é¡žç¥žç¶“å…ƒè¯çµé€²è¡Œè¨ˆç®—ã€‚å¤§å¤šæ•¸æƒ…æ³ä¸‹é¡žç¥žç¶“ç¶²è·¯èƒ½åœ¨å¤–ç•Œè³‡è¨Šçš„åŸºç¤Žä¸Šæ”¹è®Šå…§éƒ¨çµæ§‹ï¼Œæ˜¯ä¸€ç¨®è‡ªé©æ‡‰ç³»çµ±(adaptive system)ï¼Œé€šä¿—åœ°è¬›å°±æ˜¯å…·å‚™å­¸ç¿’åŠŸèƒ½ã€‚ç¾ä»£ç¥žç¶“ç¶²è·¯æ˜¯ä¸€ç¨®éžç·šæ€§çµ±è¨ˆæ€§è³‡æ–™å»ºæ¨¡(æ¦‚çŽ‡æ¨¡åž‹)å·¥å…·ï¼Œç¥žç¶“ç¶²è·¯é€šå¸¸æ˜¯é€éŽä¸€å€‹åŸºæ–¼æ•¸å­¸çµ±è¨ˆå­¸é¡žåž‹çš„å­¸ç¿’æ–¹æ³•ï¼ˆlearning methodï¼‰å¾—ä»¥æœ€ä½³åŒ–ï¼Œæ‰€ä»¥ä¹Ÿæ˜¯æ•¸å­¸çµ±è¨ˆå­¸æ–¹æ³•çš„ä¸€ç¨®å¯¦éš›æ‡‰ç”¨ï¼Œé€éŽçµ±è¨ˆå­¸çš„æ¨™æº–æ•¸å­¸æ–¹æ³•æˆ‘å€‘èƒ½å¤ å¾—åˆ°å¤§é‡çš„å¯ä»¥ç”¨å‡½å¼ä¾†è¡¨é”çš„å±€éƒ¨çµæ§‹ç©ºé–“ï¼Œå¦ä¸€æ–¹é¢åœ¨äººå·¥æ™ºæ…§å­¸çš„äººå·¥æ„ŸçŸ¥é ˜åŸŸï¼Œæˆ‘å€‘é€éŽæ•¸å­¸çµ±è¨ˆå­¸çš„æ‡‰ç”¨å¯ä»¥ä¾†åšäººå·¥æ„ŸçŸ¥æ–¹é¢çš„æ±ºå®šå•é¡Œï¼ˆä¹Ÿå°±æ˜¯èªªé€éŽçµ±è¨ˆå­¸çš„æ–¹æ³•ï¼Œé¡žç¥žç¶“ç¶²è·¯èƒ½å¤ é¡žä¼¼äººä¸€æ¨£å…·æœ‰ç°¡å–®çš„æ±ºå®šèƒ½åŠ›å’Œç°¡å–®çš„åˆ¤æ–·èƒ½åŠ›ï¼‰ï¼Œé€™ç¨®æ–¹æ³•æ¯”èµ·æ­£å¼çš„é‚è¼¯å­¸æŽ¨ç†æ¼”ç®—æ›´å…·æœ‰å„ªå‹¢ã€‚ å’Œå…¶ä»–æ©Ÿå™¨å­¸ç¿’æ–¹æ³•ä¸€æ¨£ï¼Œç¥žç¶“ç¶²è·¯å·²ç¶“è¢«ç”¨æ–¼è§£æ±ºå„ç¨®å„æ¨£çš„å•é¡Œï¼Œä¾‹å¦‚æ©Ÿå™¨è¦–è¦ºå’ŒèªžéŸ³è¾¨è­˜ã€‚é€™äº›å•é¡Œéƒ½æ˜¯å¾ˆé›£è¢«å‚³çµ±åŸºæ–¼è¦å‰‡çš„ç·¨ç¨‹æ‰€è§£æ±ºçš„ã€‚ å°äººé¡žä¸­æ¨žç¥žç¶“ç³»çµ±çš„è§€å¯Ÿå•Ÿç™¼äº†é¡žç¥žç¶“ç¶²è·¯é€™å€‹æ¦‚å¿µã€‚åœ¨é¡žç¥žç¶“ç¶²è·¯ä¸­ï¼Œç¯€é»žï¼ˆnodeï¼‰æ˜¯æ§‹æˆé¡žç¥žç¶“ç¶²è·¯çš„æ•¸ä½åŒ–å…ƒç´ ï¼Œæ˜¯ç¶²è·¯ä¸­ä»»ä½•çš„é€£æŽ¥é»žï¼›é¡žç¥žç¶“å…ƒï¼ˆneuronï¼‰å±¬æ–¼ä¸€ç¨®ç¯€é»žï¼Œç‰¹æŒ‡é¡žç¥žç¶“ç¶²è·¯ä¸­åŸºæœ¬çš„è¨ˆç®—æˆ–è™•ç†å–®å…ƒï¼Œé€£æŽ¥åœ¨ä¸€èµ·å½¢æˆä¸€å€‹é¡žä¼¼ç”Ÿç‰©ç¥žç¶“ç¶²è·¯çš„ç¶²ç‹€çµæ§‹ã€‚ é¡žç¥žç¶“ç¶²è·¯ç›®å‰æ²’æœ‰ä¸€å€‹çµ±ä¸€çš„æ­£å¼å®šç¾©ã€‚ä¸éŽï¼Œå…·æœ‰ä¸‹åˆ—ç‰¹é»žçš„çµ±è¨ˆæ¨¡åž‹å¯ä»¥è¢«ç¨±ä½œæ˜¯ã€Œç¥žç¶“åŒ–ã€çš„ï¼š é€™äº›å¯èª¿ç¯€çš„æ¬Šé‡å¯ä»¥è¢«çœ‹åšç¥žç¶“å…ƒä¹‹é–“çš„é€£æŽ¥å¼·åº¦ã€‚ é¡žç¥žç¶“ç¶²è·¯èˆ‡ç”Ÿç‰©ç¥žç¶“ç¶²è·¯çš„ç›¸ä¼¼ä¹‹è™•åœ¨æ–¼ï¼Œå®ƒå¯ä»¥é›†é«”åœ°ã€ä¸¦åˆ—åœ°è¨ˆç®—å‡½å¼çš„å„å€‹éƒ¨åˆ†ï¼Œè€Œä¸éœ€è¦æè¿°æ¯ä¸€å€‹å–®å…ƒçš„ç‰¹å®šä»»å‹™ã€‚ç¥žç¶“ç¶²è·¯é€™å€‹è©žä¸€èˆ¬æŒ‡çµ±è¨ˆå­¸ã€èªçŸ¥å¿ƒç†å­¸å’Œäººå·¥æ™ºæ…§é ˜åŸŸä½¿ç”¨çš„æ¨¡åž‹ï¼Œè€ŒæŽ§åˆ¶ä¸­å¤®ç¥žç¶“ç³»çµ±çš„ç¥žç¶“ç¶²è·¯å±¬æ–¼è¨ˆç®—ç¥žç¶“ç§‘å­¸ã€‚[1] åœ¨ç¥žç¶“ç¶²è·¯çš„ç¾ä»£è»Ÿé«”å¯¦ç¾ä¸­ï¼Œç”±ç”Ÿç‰©å­¸å•Ÿç™¼çš„æ–¹æ³•å·²ç¶“æœ‰äº†å¾ˆé‡å¤§çš„å»¶ä¼¸ï¼Œç¾åœ¨ä¸»æµçš„æ˜¯åŸºæ–¼çµ±è¨ˆå­¸å’Œè¨Šè™Ÿè™•ç†çš„æ›´åŠ å¯¦ç”¨çš„æ–¹æ³•ã€‚åœ¨ä¸€äº›è»Ÿé«”ç³»çµ±ä¸­ï¼Œç¥žç¶“ç¶²è·¯æˆ–è€…ç¥žç¶“ç¶²è·¯çš„ä¸€éƒ¨åˆ†ï¼ˆä¾‹å¦‚é¡žç¥žç¶“å…ƒï¼‰æ˜¯å¤§åž‹ç³»çµ±ä¸­çš„ä¸€å€‹éƒ¨åˆ†ã€‚é€™äº›ç³»çµ±çµåˆäº†é©æ‡‰æ€§çš„å’Œéžé©æ‡‰æ€§çš„å…ƒç´ ã€‚é›–ç„¶é€™ç¨®ç³»çµ±ä½¿ç”¨çš„é€™ç¨®æ›´åŠ æ™®éçš„æ–¹æ³•æ›´é©å®œè§£æ±ºç¾å¯¦ä¸­çš„å•é¡Œï¼Œä½†æ˜¯é€™å’Œå‚³çµ±çš„é€£æŽ¥ä¸»ç¾©äººå·¥æ™ºæ…§å·²ç¶“æ²’æœ‰ä»€éº¼é—œè¯äº†ã€‚ä¸éŽå®ƒå€‘é‚„æœ‰ä¸€äº›å…±åŒé»žï¼šéžç·šæ€§ã€åˆ†æ•£å¼ã€ä¸¦åˆ—åŒ–ï¼Œå±€éƒ¨æ€§è¨ˆç®—ä»¥åŠé©æ‡‰æ€§ã€‚å¾žæ­·å²çš„è§’åº¦è¬›ï¼Œç¥žç¶“ç¶²è·¯æ¨¡åž‹çš„æ‡‰ç”¨æ¨™èªŒè‘—äºŒåä¸–ç´€å…«åå¹´ä»£å¾ŒæœŸå¾žé«˜åº¦ç¬¦è™ŸåŒ–çš„äººå·¥æ™ºæ…§ï¼ˆä»¥ç”¨æ¢ä»¶è¦å‰‡è¡¨é”çŸ¥è­˜çš„å°ˆå®¶ç³»çµ±ç‚ºä»£è¡¨ï¼‰å‘ä½Žç¬¦è™ŸåŒ–çš„æ©Ÿå™¨å­¸ç¿’ï¼ˆä»¥ç”¨å‹•åŠ›ç³»çµ±çš„åƒæ•¸åˆ—é”çŸ¥è­˜ç‚ºä»£è¡¨ï¼‰çš„è½‰è®Šã€‚ æ²ƒå€«Â·éº¥å¡æ´›å…‹å’Œæ²ƒçˆ¾ç‰¹Â·çš®èŒ¨ï¼ˆ1943ï¼‰[2]åŸºæ–¼æ•¸å­¸å’Œä¸€ç¨®ç¨±ç‚ºé–¾å€¼é‚è¼¯çš„æ¼”ç®—æ³•å‰µé€ äº†ä¸€ç¨®ç¥žç¶“ç¶²è·¯çš„è¨ˆç®—æ¨¡åž‹ã€‚é€™ç¨®æ¨¡åž‹ä½¿å¾—ç¥žç¶“ç¶²è·¯çš„ç ”ç©¶åˆ†è£‚ç‚ºå…©ç¨®ä¸åŒç ”ç©¶æ€è·¯ã€‚ä¸€ç¨®ä¸»è¦é—œæ³¨å¤§è…¦ä¸­çš„ç”Ÿç‰©å­¸éŽç¨‹ï¼Œå¦ä¸€ç¨®ä¸»è¦é—œæ³¨ç¥žç¶“ç¶²è·¯åœ¨äººå·¥æ™ºæ…§è£¡çš„æ‡‰ç”¨ã€‚ äºŒåä¸–ç´€40å¹´ä»£å¾ŒæœŸï¼Œå¿ƒç†å­¸å®¶å”ç´å¾·Â·èµ«å¸ƒæ ¹æ“šç¥žç¶“å¯å¡‘æ€§çš„æ©Ÿåˆ¶å‰µé€ äº†ä¸€ç¨®å°å­¸ç¿’çš„å‡èªªï¼Œç¾åœ¨ç¨±ä½œèµ«å¸ƒåž‹å­¸ç¿’ã€‚èµ«å¸ƒåž‹å­¸ç¿’è¢«èªç‚ºæ˜¯ä¸€ç¨®å…¸åž‹çš„éžç›£ç£å¼å­¸ç¿’è¦å‰‡ï¼Œå®ƒå¾Œä¾†çš„è®Šç¨®æ˜¯é•·æœŸå¢žå¼·ä½œç”¨çš„æ—©æœŸæ¨¡åž‹ã€‚å¾ž1948å¹´é–‹å§‹ï¼Œç ”ç©¶äººå“¡å°‡é€™ç¨®è¨ˆç®—æ¨¡åž‹çš„æ€æƒ³æ‡‰ç”¨åˆ°Båž‹åœ–éˆæ©Ÿä¸Šã€‚ æ³•åˆ©å’ŒéŸ‹æ–¯åˆ©Â·AÂ·å…‹æ‹‰å…‹(Wesley Allison Clark )ï¼ˆ1954ï¼‰[3]é¦–æ¬¡ä½¿ç”¨é›»è…¦ï¼Œç•¶æ™‚ç¨±ä½œè¨ˆç®—æ©Ÿï¼Œåœ¨MITé¡žæ¯”äº†ä¸€å€‹èµ«å¸ƒç¶²è·¯ã€‚ç´æ’’å°¼çˆ¾Â·ç¾…åˆ‡æ–¯ç‰¹ï¼ˆ1956ï¼‰ç­‰äºº[4]é¡žæ¯”äº†ä¸€å° IBM 704é›»è…¦ä¸Šçš„æŠ½è±¡ç¥žç¶“ç¶²è·¯çš„è¡Œç‚ºã€‚ å¼—è˜­å…‹Â·ç¾…æ£®å¸ƒæ‹‰ç‰¹ï¼ˆè‹±èªžï¼šFrank Rosenblattï¼‰å‰µé€ äº†æ„ŸçŸ¥æ©Ÿ[5]ã€‚é€™æ˜¯ä¸€ç¨®åœ–åž‹è­˜åˆ¥æ¼”ç®—æ³•ï¼Œç”¨ç°¡å–®çš„åŠ æ¸›æ³•å¯¦ç¾äº†å…©å±¤çš„é›»è…¦å­¸ç¿’ç¶²è·¯ã€‚ç¾…æ£®å¸ƒæ‹‰ç‰¹ä¹Ÿç”¨æ•¸å­¸ç¬¦è™Ÿæè¿°äº†åŸºæœ¬æ„ŸçŸ¥æ©Ÿé‡Œæ²’æœ‰çš„è¿´è·¯ï¼Œä¾‹å¦‚äº’æ–¥æˆ–è¿´è·¯ã€‚é€™ç¨®è¿´è·¯ä¸€ç›´ç„¡æ³•è¢«ç¥žç¶“ç¶²è·¯è™•ç†ï¼Œç›´åˆ°ä¿ç¾…Â·éŸ‹ä¼¯æ–¯ï¼ˆè‹±èªžï¼šPaul Werbosï¼‰(1975)å‰µé€ äº†åå‘å‚³æ’­æ¼”ç®—æ³•ã€‚ åœ¨é¦¬æ–‡Â·æ˜Žæ–¯åŸºå’Œè¥¿æ‘©çˆ¾Â·æ´¾æ™®ç‰¹ï¼ˆ1969ï¼‰ç™¼è¡¨äº†ä¸€é …é—œæ–¼æ©Ÿå™¨å­¸ç¿’çš„ç ”ç©¶ä»¥å¾Œï¼Œç¥žç¶“ç¶²è·¯çš„ç ”ç©¶åœæ»¯ä¸å‰ã€‚ä»–å€‘ç™¼ç¾äº†ç¥žç¶“ç¶²è·¯çš„å…©å€‹é—œéµå•é¡Œã€‚ç¬¬ä¸€æ˜¯åŸºæœ¬æ„ŸçŸ¥æ©Ÿç„¡æ³•è™•ç†äº’æ–¥æˆ–è¿´è·¯ã€‚ç¬¬äºŒå€‹é‡è¦çš„å•é¡Œæ˜¯é›»è…¦æ²’æœ‰è¶³å¤ çš„èƒ½åŠ›ä¾†è™•ç†å¤§åž‹ç¥žç¶“ç¶²è·¯æ‰€éœ€è¦çš„å¾ˆé•·çš„è¨ˆç®—æ™‚é–“ã€‚ç›´åˆ°é›»è…¦å…·æœ‰æ›´å¼·çš„è¨ˆç®—èƒ½åŠ›ä¹‹å‰ï¼Œç¥žç¶“ç¶²è·¯çš„ç ”ç©¶é€²å±•ç·©æ…¢ã€‚ å¾Œä¾†å‡ºç¾çš„ä¸€å€‹é—œéµçš„é€²å±•æ˜¯ä¿ç¾…Â·éŸ‹ä¼¯æ–¯ç™¼æ˜Žçš„åå‘å‚³æ’­æ¼”ç®—æ³•ï¼ˆWerbos 1975ï¼‰ã€‚é€™å€‹æ¼”ç®—æ³•æœ‰æ•ˆåœ°è§£æ±ºäº†äº’æ–¥æˆ–çš„å•é¡Œï¼Œé‚„æœ‰æ›´æ™®éçš„è¨“ç·´å¤šå±¤ç¥žç¶“ç¶²è·¯çš„å•é¡Œã€‚ åœ¨äºŒåä¸–ç´€80å¹´ä»£ä¸­æœŸï¼Œåˆ†æ•£å¼ä¸¦åˆ—è™•ç†ï¼ˆç•¶æ™‚ç¨±ä½œè¯çµä¸»ç¾©ï¼‰æµè¡Œèµ·ä¾†ã€‚æˆ´ç¶­Â·é­¯å§†å“ˆç‰¹ï¼ˆè‹±èªžï¼šDavid E. Rumelhartï¼‰å’Œè©¹å§†æ–¯Â·éº¥å…‹é‡Œè˜­å¾·ï¼ˆè‹±èªžï¼šJames McClellandï¼‰çš„æ•™æå°æ–¼è¯çµä¸»ç¾©åœ¨é›»è…¦é¡žæ¯”ç¥žç¶“æ´»å‹•ä¸­çš„æ‡‰ç”¨æä¾›äº†å…¨é¢çš„è«–è¿°ã€‚ ç¥žç¶“ç¶²è·¯å‚³çµ±ä¸Šè¢«èªç‚ºæ˜¯å¤§è…¦ä¸­çš„ç¥žç¶“æ´»å‹•çš„ç°¡åŒ–æ¨¡åž‹ï¼Œé›–ç„¶é€™å€‹æ¨¡åž‹å’Œå¤§è…¦çš„ç”Ÿç†çµæ§‹ä¹‹é–“çš„é—œè¯å­˜åœ¨çˆ­è­°ã€‚äººå€‘ä¸æ¸…æ¥šé¡žç¥žç¶“ç¶²è·¯èƒ½å¤šå¤§ç¨‹åº¦åœ°åæ˜ å¤§è…¦çš„åŠŸèƒ½ã€‚ æ”¯æŒå‘é‡æ©Ÿå’Œå…¶ä»–æ›´ç°¡å–®çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ç·šæ€§åˆ†é¡žå™¨ï¼‰åœ¨æ©Ÿå™¨å­¸ç¿’é ˜åŸŸçš„æµè¡Œåº¦é€æ¼¸è¶…éŽäº†ç¥žç¶“ç¶²è·¯ï¼Œä½†æ˜¯åœ¨2000å¹´ä»£å¾ŒæœŸå‡ºç¾çš„æ·±åº¦å­¸ç¿’é‡æ–°æ¿€ç™¼äº†äººå€‘å°ç¥žç¶“ç¶²è·¯çš„èˆˆè¶£ã€‚ äººå€‘ç”¨CMOSå‰µé€ äº†ç”¨æ–¼ç”Ÿç‰©ç‰©ç†é¡žæ¯”å’Œç¥žç¶“å½¢æ…‹è¨ˆç®—çš„è¨ˆç®—è£ç½®ã€‚æœ€æ–°çš„ç ”ç©¶é¡¯ç¤ºäº†ç”¨æ–¼å¤§åž‹ä¸»æˆåˆ†åˆ†æžå’Œå·ç©ç¥žç¶“ç¶²è·¯çš„å¥ˆç±³è£ç½®[6]å…·æœ‰è‰¯å¥½çš„å‰æ™¯ã€‚å¦‚æžœæˆåŠŸçš„è©±ï¼Œé€™æœƒå‰µé€ å‡ºä¸€ç¨®æ–°çš„ç¥žç¶“è¨ˆç®—è£ç½®[7]ï¼Œå› ç‚ºå®ƒä¾è³´æ–¼å­¸ç¿’è€Œä¸æ˜¯ç·¨ç¨‹ï¼Œä¸¦ä¸”å®ƒå¾žæ ¹æœ¬ä¸Šå°±æ˜¯é¡žæ¯”çš„è€Œä¸æ˜¯æ•¸ä½åŒ–çš„ï¼Œé›–ç„¶å®ƒçš„ç¬¬ä¸€å€‹å¯¦ä¾‹å¯èƒ½æ˜¯æ•¸ä½åŒ–çš„CMOSè£ç½®ã€‚ åœ¨2009åˆ°2012å¹´ä¹‹é–“ï¼ŒäºŽçˆ¾æ ¹Â·æ–½å¯†å¾·èƒ¡ä¼¯åœ¨ç‘žå£«ç ”ç©¶å°çµ„(Dalle Molle Institute for Artificial Intelligence)çš„ç ”ç™¼çš„è¿´åœˆç¥žç¶“ç¶²è·¯å’Œæ·±å‰é¥‹ç¥žç¶“ç¶²è·¯è´å¾—äº†8é …é—œæ–¼åœ–åž‹è­˜åˆ¥å’Œæ©Ÿå™¨å­¸ç¿’çš„åœ‹éš›æ¯”è³½ã€‚[8][9]ä¾‹å¦‚ï¼Œè‰¾åŠ›å…‹æ–¯Â·æ ¼é›·å¤«æ–¯çš„é›™å‘ã€å¤šç¶­çš„LSTMè´å¾—äº†2009å¹´ICDARçš„3é …é—œæ–¼é€£ç­†å­—è¾¨è­˜çš„æ¯”è³½ï¼Œè€Œä¸”ä¹‹å‰ä¸¦ä¸çŸ¥é“é—œæ–¼å°‡è¦å­¸ç¿’çš„3ç¨®èªžè¨€çš„è³‡è¨Šã€‚[10][11][12][13] IDSIAçš„ä¸¹Â·å¥‡çˆ¾æ¡‘ (Dan Ciresan)å’ŒåŒäº‹æ ¹æ“šé€™å€‹æ–¹æ³•ç·¨å¯«çš„åŸºæ–¼GPUçš„å¯¦ç¾è´å¾—äº†å¤šé …åœ–åž‹è­˜åˆ¥çš„æ¯”è³½ï¼ŒåŒ…æ‹¬IJCNN 2011äº¤é€šæ¨™èªŒè¾¨è­˜æ¯”è³½ç­‰ç­‰ã€‚[14][15]ä»–å€‘çš„ç¥žç¶“ç¶²è·¯ä¹Ÿæ˜¯ç¬¬ä¸€å€‹åœ¨é‡è¦çš„åŸºæº–æ¸¬è©¦ä¸­ï¼ˆä¾‹å¦‚IJCNN 2012äº¤é€šæ¨™èªŒè¾¨è­˜å’ŒNYUçš„æ¥Šç«‹æ˜†çš„MNISTæ‰‹å¯«æ•¸å­—å•é¡Œï¼‰èƒ½é”åˆ°æˆ–è¶…éŽäººé¡žæ°´å¹³çš„äººå·¥åœ–åž‹è­˜åˆ¥å™¨ã€‚ é¡žä¼¼1980å¹´ç¦å³¶é‚¦å½¥ï¼ˆæ—¥èªžï¼šç¦å³¶é‚¦å½¦ï¼‰(Kunihiko Fukushima )ç™¼æ˜Žçš„neocognitron[16]å’Œè¦–è¦ºæ¨™æº–çµæ§‹[17]ï¼ˆç”±David H. Hubelå’ŒTorsten Wieselåœ¨åˆç´šè¦–çš®å±¤ä¸­ç™¼ç¾çš„é‚£äº›ç°¡å–®è€Œåˆè¤‡é›œçš„ç´°èƒžå•Ÿç™¼ï¼‰é‚£æ¨£æœ‰æ·±åº¦çš„ã€é«˜åº¦éžç·šæ€§çš„ç¥žç¶“çµæ§‹å¯ä»¥è¢«å¤šå€«å¤šå¤§å­¸å‚‘å¼—é‡ŒÂ·è¾›é “å¯¦é©—å®¤çš„éžç›£ç£å¼å­¸ç¿’æ–¹æ³•æ‰€è¨“ç·´ã€‚[18][19][20]2012å¹´ï¼Œç¥žç¶“ç¶²è·¯å‡ºç¾äº†å¿«é€Ÿçš„ç™¼å±•ï¼Œä¸»è¦åŽŸå› åœ¨æ–¼è¨ˆç®—æŠ€è¡“çš„æé«˜ï¼Œä½¿å¾—å¾ˆå¤šè¤‡é›œçš„é‹ç®—è®Šå¾—æˆæœ¬ä½Žå»‰ã€‚ä»¥AlexNetç‚ºæ¨™èªŒï¼Œå¤§é‡çš„æ·±åº¦ç¶²è·¯é–‹å§‹å‡ºç¾ã€‚ 2014å¹´å‡ºç¾äº†æ®˜å·®ç¥žç¶“ç¶²è·¯ï¼Œè©²ç¶²è·¯æ¥µå¤§è§£æ”¾äº†ç¥žç¶“ç¶²è·¯çš„æ·±åº¦é™åˆ¶ï¼Œå‡ºç¾äº†æ·±åº¦å­¸ç¿’çš„æ¦‚å¿µã€‚ å…¸åž‹çš„é¡žç¥žç¶“ç¶²è·¯å…·æœ‰ä»¥ä¸‹ä¸‰å€‹éƒ¨åˆ†ï¼š ç¥žç¶“å…ƒç¤ºæ„åœ–ï¼š æ•¸å­¸è¡¨ç¤ºt=f(Wâ€²â†’Aâ†’+b){\displaystyle t=f({\vec {W'}}{\vec {A}}+b)} å¯è¦‹ï¼Œä¸€å€‹ç¥žç¶“å…ƒçš„åŠŸèƒ½æ˜¯æ±‚å¾—è¼¸å…¥å‘é‡èˆ‡æ¬Šå‘é‡çš„å…§ç©å¾Œï¼Œç¶“ä¸€å€‹éžç·šæ€§å‚³éžå‡½å¼å¾—åˆ°ä¸€å€‹ç´”é‡çµæžœã€‚ å–®å€‹ç¥žç¶“å…ƒçš„ä½œç”¨ï¼šæŠŠä¸€å€‹nç¶­å‘é‡ç©ºé–“ç”¨ä¸€å€‹è¶…å¹³é¢åˆ†å‰²æˆå…©éƒ¨åˆ†ï¼ˆç¨±ä¹‹ç‚ºåˆ¤æ–·é‚Šç•Œï¼‰ï¼Œçµ¦å®šä¸€å€‹è¼¸å…¥å‘é‡ï¼Œç¥žç¶“å…ƒå¯ä»¥åˆ¤æ–·å‡ºé€™å€‹å‘é‡ä½æ–¼è¶…å¹³é¢çš„å“ªä¸€é‚Šã€‚ è©²è¶…å¹³é¢çš„æ–¹ç¨‹ï¼šWâ€²â†’pâ†’+b=0{\displaystyle {\vec {W'}}{\vec {p}}+b=0} æ˜¯æœ€åŸºæœ¬çš„ç¥žç¶“å…ƒç¶²è·¯å½¢å¼ï¼Œç”±æœ‰é™å€‹ç¥žç¶“å…ƒæ§‹æˆï¼Œæ‰€æœ‰ç¥žç¶“å…ƒçš„è¼¸å…¥å‘é‡éƒ½æ˜¯åŒä¸€å€‹å‘é‡ã€‚ç”±æ–¼æ¯ä¸€å€‹ç¥žç¶“å…ƒéƒ½æœƒç”¢ç”Ÿä¸€å€‹ç´”é‡çµæžœï¼Œæ‰€ä»¥å–®å±¤ç¥žç¶“å…ƒçš„è¼¸å‡ºæ˜¯ä¸€å€‹å‘é‡ï¼Œå‘é‡çš„ç¶­æ•¸ç­‰æ–¼ç¥žç¶“å…ƒçš„æ•¸ç›®ã€‚ ç¤ºæ„åœ–ï¼š é¡žç¥žç¶“ç¶²è·¯æ˜¯ä¸€å€‹èƒ½å¤ å­¸ç¿’ï¼Œèƒ½å¤ ç¸½çµæ­¸ç´çš„ç³»çµ±ï¼Œä¹Ÿå°±æ˜¯èªªå®ƒèƒ½å¤ é€éŽå·²çŸ¥è³‡æ–™çš„å¯¦é©—é‹ç”¨ä¾†å­¸ç¿’å’Œæ­¸ç´ç¸½çµã€‚é¡žç¥žç¶“ç¶²è·¯é€éŽå°å±€éƒ¨æƒ…æ³çš„å°ç…§æ¯”è¼ƒï¼ˆè€Œé€™äº›æ¯”è¼ƒæ˜¯åŸºæ–¼ä¸åŒæƒ…æ³ä¸‹çš„è‡ªå‹•å­¸ç¿’å’Œè¦å¯¦éš›è§£æ±ºå•é¡Œçš„è¤‡é›œæ€§æ‰€æ±ºå®šçš„ï¼‰ï¼Œå®ƒèƒ½å¤ æŽ¨ç†ç”¢ç”Ÿä¸€å€‹å¯ä»¥è‡ªå‹•è¾¨è­˜çš„ç³»çµ±ã€‚èˆ‡ä¹‹ä¸åŒçš„åŸºæ–¼ç¬¦è™Ÿç³»çµ±ä¸‹çš„å­¸ç¿’æ–¹æ³•ï¼Œå®ƒå€‘ä¹Ÿå…·æœ‰æŽ¨ç†åŠŸèƒ½ï¼Œåªæ˜¯å®ƒå€‘æ˜¯å»ºç«‹åœ¨é‚è¼¯æ¼”ç®—æ³•çš„åŸºç¤Žä¸Šï¼Œä¹Ÿå°±æ˜¯èªªå®ƒå€‘ä¹‹æ‰€ä»¥èƒ½å¤ æŽ¨ç†ï¼ŒåŸºç¤Žæ˜¯éœ€è¦æœ‰ä¸€å€‹æŽ¨ç†æ¼”ç®—æ³•å‰‡çš„é›†åˆã€‚ é€šå¸¸ä¾†èªªï¼Œä¸€å€‹é¡žç¥žç¶“å…ƒç¶²è·¯æ˜¯ç”±ä¸€å€‹å¤šå±¤ç¥žç¶“å…ƒçµæ§‹çµ„æˆï¼Œæ¯ä¸€å±¤ç¥žç¶“å…ƒæ“æœ‰è¼¸å…¥ï¼ˆå®ƒçš„è¼¸å…¥æ˜¯å‰ä¸€å±¤ç¥žç¶“å…ƒçš„è¼¸å‡ºï¼‰å’Œè¼¸å‡ºï¼Œæ¯ä¸€å±¤ï¼ˆæˆ‘å€‘ç”¨ç¬¦è™Ÿè¨˜åšï¼‰Layer(i)æ˜¯ç”±Ni(Niä»£è¡¨åœ¨ç¬¬iå±¤ä¸Šçš„N)å€‹ç¶²è·¯ç¥žç¶“å…ƒçµ„æˆï¼Œæ¯å€‹Niä¸Šçš„ç¶²è·¯ç¥žç¶“å…ƒæŠŠå°æ‡‰åœ¨Ni-1ä¸Šçš„ç¥žç¶“å…ƒè¼¸å‡ºåšç‚ºå®ƒçš„è¼¸å…¥ï¼Œæˆ‘å€‘æŠŠç¥žç¶“å…ƒå’Œèˆ‡ä¹‹å°æ‡‰çš„ç¥žç¶“å…ƒä¹‹é–“çš„é€£ç·šç”¨ç”Ÿç‰©å­¸çš„åç¨±ï¼Œå«åšçªè§¸ï¼ˆè‹±èªžï¼šSynapseï¼‰ï¼Œåœ¨æ•¸å­¸æ¨¡åž‹ä¸­æ¯å€‹çªè§¸æœ‰ä¸€å€‹åŠ æ¬Šæ•¸å€¼ï¼Œæˆ‘å€‘ç¨±åšæ¬Šé‡ï¼Œé‚£éº¼è¦è¨ˆç®—ç¬¬iå±¤ä¸Šçš„æŸå€‹ç¥žç¶“å…ƒæ‰€å¾—åˆ°çš„å‹¢èƒ½ç­‰æ–¼æ¯ä¸€å€‹æ¬Šé‡ä¹˜ä»¥ç¬¬i-1å±¤ä¸Šå°æ‡‰çš„ç¥žç¶“å…ƒçš„è¼¸å‡ºï¼Œç„¶å¾Œå…¨é«”æ±‚å’Œå¾—åˆ°äº†ç¬¬iå±¤ä¸Šçš„æŸå€‹ç¥žç¶“å…ƒæ‰€å¾—åˆ°çš„å‹¢èƒ½ï¼Œç„¶å¾Œå‹¢èƒ½æ•¸å€¼é€éŽè©²ç¥žç¶“å…ƒä¸Šçš„å•Ÿç”¨åŠŸèƒ½ï¼ˆactivation functionï¼Œå¸¸æ˜¯âˆ‘å‡½å¼ï¼ˆè‹±èªžï¼šSigmoid functionï¼‰ä»¥æŽ§åˆ¶è¼¸å‡ºå¤§å°ï¼Œå› ç‚ºå…¶å¯å¾®åˆ†ä¸”é€£çºŒï¼Œæ–¹ä¾¿å·®é‡è¦å‰‡ï¼ˆè‹±èªžï¼šDelta ruleï¼‰è™•ç†ã€‚ï¼‰ï¼Œæ±‚å‡ºè©²ç¥žç¶“å…ƒçš„è¼¸å‡ºï¼Œæ³¨æ„çš„æ˜¯è©²è¼¸å‡ºæ˜¯ä¸€å€‹éžç·šæ€§çš„æ•¸å€¼ï¼Œä¹Ÿå°±æ˜¯èªªé€éŽæ¿€å‹µå‡½å¼æ±‚çš„æ•¸å€¼æ ¹æ“šæ¥µé™å€¼ä¾†åˆ¤æ–·æ˜¯å¦è¦å•Ÿç”¨è©²ç¥žç¶“å…ƒï¼Œæ›å¥è©±èªªæˆ‘å€‘å°ä¸€å€‹ç¥žç¶“å…ƒç¶²è·¯çš„è¼¸å‡ºæ˜¯å¦ç·šæ€§ä¸æ„Ÿèˆˆè¶£ã€‚ ä¸€ç¨®å¸¸è¦‹çš„å¤šå±¤çµæ§‹çš„å‰é¥‹ç¶²è·¯ï¼ˆMultilayer Feedforward Networkï¼‰ç”±ä¸‰éƒ¨åˆ†çµ„æˆï¼Œ é€™ç¨®ç¶²è·¯ä¸€èˆ¬ç¨±ç‚ºæ„ŸçŸ¥å™¨ï¼ˆå°å–®éš±è—å±¤ï¼‰æˆ–å¤šå±¤æ„ŸçŸ¥å™¨ï¼ˆå°å¤šéš±è—å±¤ï¼‰ï¼Œç¥žç¶“ç¶²è·¯çš„é¡žåž‹å·²ç¶“æ¼”è®Šå‡ºå¾ˆå¤šç¨®ï¼Œé€™ç¨®åˆ†å±¤çš„çµæ§‹ä¹Ÿä¸¦ä¸æ˜¯å°æ‰€æœ‰çš„ç¥žç¶“ç¶²è·¯éƒ½é©ç”¨ã€‚ é€šéŽè¨“ç·´æ¨£æœ¬çš„æ ¡æ­£ï¼Œå°å„å€‹å±¤çš„æ¬Šé‡é€²è¡Œæ ¡æ­£ï¼ˆlearningï¼‰è€Œå»ºç«‹æ¨¡åž‹çš„éŽç¨‹ï¼Œç¨±ç‚ºè‡ªå‹•å­¸ç¿’éŽç¨‹ï¼ˆtraining algorithmï¼‰ã€‚å…·é«”çš„å­¸ç¿’æ–¹æ³•å‰‡å› ç¶²è·¯çµæ§‹å’Œæ¨¡åž‹ä¸åŒè€Œä¸åŒï¼Œå¸¸ç”¨åå‘å‚³æ’­æ¼”ç®—æ³•ï¼ˆBackpropagation/å€’å‚³éž/é€†å‚³æ’­ï¼Œä»¥outputåˆ©ç”¨ä¸€æ¬¡å¾®åˆ†Delta ruleï¼ˆè‹±èªžï¼šDelta ruleï¼‰ä¾†ä¿®æ­£weightï¼‰ä¾†é©—è­‰ã€‚ é¡žç¥žç¶“ç¶²çµ¡åˆ†é¡žç‚ºä»¥ä¸‹å…©ç¨®ï¼š 1.ä¾å­¸ç¿’ç­–ç•¥ï¼ˆAlgorithmï¼‰åˆ†é¡žä¸»è¦æœ‰ï¼š 2.ä¾ç¶²çµ¡æž¶æ§‹ï¼ˆConnectionismï¼‰åˆ†é¡žä¸»è¦æœ‰ï¼š å¤šå±¤æ„ŸçŸ¥å™¨ï¼ˆMultilayer Perceptronï¼Œç¸®å¯«MLPï¼‰æ˜¯ä¸€å€‹é€šç”¨çš„å‡½å¼é€¼è¿‘å™¨ï¼Œç”±Cybenkoå®šç†è­‰æ˜Žã€‚ç„¶è€Œï¼Œè­‰æ˜Žä¸ä¾è³´ç‰¹å®šçš„ç¥žç¶“å…ƒæ•¸é‡æˆ–æ¬Šé‡ã€‚Hava Siegelmannå’ŒEduardo D. Sontagçš„å·¥ä½œè­‰æ˜Žäº†ï¼Œä¸€å€‹å…·æœ‰æœ‰ç†æ•¸æ¬Šé‡å€¼çš„ç‰¹å®šéžè¿´çµæ§‹ï¼ˆèˆ‡å…¨ç²¾åº¦å¯¦æ•¸æ¬Šé‡å€¼ç›¸å°æ‡‰ï¼‰ç”±æœ‰é™å€‹ç¥žç¶“å…ƒå’Œæ¨™æº–çš„ç·šæ€§é—œä¿‚æ§‹æˆçš„ç¥žç¶“ç¶²è·¯ç›¸ç•¶æ–¼ä¸€å€‹é€šç”¨åœ–éˆæ©Ÿã€‚[22]ä»–å€‘é€²ä¸€æ­¥è¡¨æ˜Žï¼Œä½¿ç”¨ç„¡ç†æ•¸æ¬Šé‡å€¼æœƒç”¢ç”Ÿä¸€å€‹è¶…åœ–éˆæ©Ÿã€‚ é¡žç¥žç¶“ç¶²è·¯æ¨¡åž‹æœ‰ä¸€å€‹å±¬æ€§ï¼Œç¨±ç‚ºã€Œå®¹é‡ã€ï¼Œé€™å¤§è‡´ç›¸ç•¶æ–¼ä»–å€‘è¨˜ä½ï¼ˆè€Œéžæ­£ç¢ºåˆ†é¡žï¼‰è¼¸å…¥è³‡æ–™çš„èƒ½åŠ›ã€‚å®ƒèˆ‡ç¶²è·¯çš„åƒæ•¸ã€å’Œçµæ§‹æœ‰é—œã€‚è°·æ­Œåœ¨ç ”ç©¶[23]ä¸­ä½¿ç”¨æ‰“äº‚æ¨™ç±¤çš„æ–¹æ³•ï¼Œä¾†æ¸¬è©¦æ¨¡åž‹æ˜¯å¦èƒ½è¨˜ä½æ‰€æœ‰çš„è¼¸å‡ºã€‚é›–ç„¶å¾ˆæ˜Žé¡¯ï¼Œé€™æ¨£æ¨¡åž‹åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾å¹¾ä¹Žæ˜¯éš¨æ©ŸçŒœæ¸¬ï¼Œä½†æ˜¯æ¨¡åž‹èƒ½å¤ è¨˜ä½æ‰€æœ‰è¨“ç·´é›†çš„è¼¸å…¥è³‡æ–™ï¼Œå³è¨˜ä½ä»–å€‘è¢«æ‰“äº‚å¾Œçš„æ¨™ç±¤ã€‚è€Œè¨˜ä½æœ‰é™çš„æ¨£æœ¬çš„è³‡è¨Šï¼ˆExpressivityï¼‰ï¼Œéœ€è¦çš„æ¨¡åž‹çš„åƒæ•¸ï¼ˆæ¬Šé‡ï¼‰æ•¸é‡å­˜åœ¨ä¸‹é™ã€‚ æ¨¡åž‹ä¸¦ä¸ç¸½æ˜¯æ”¶æ–‚åˆ°å”¯ä¸€è§£ï¼Œå› ç‚ºå®ƒå–æ±ºæ–¼ä¸€äº›å› ç´ ã€‚é¦–å…ˆï¼Œå‡½å¼å¯èƒ½å­˜åœ¨è¨±å¤šå±€éƒ¨æ¥µå°å€¼ï¼Œé€™å–æ±ºæ–¼æˆæœ¬å‡½å¼å’Œæ¨¡åž‹ã€‚å…¶æ¬¡ï¼Œåœ¨é é›¢å±€éƒ¨æœ€å°å€¼æ™‚ï¼Œæœ€ä½³åŒ–æ–¹æ³•å¯èƒ½ç„¡æ³•ä¿è­‰æ”¶æ–‚ã€‚ç¬¬ä¸‰ï¼Œå°å¤§é‡çš„è³‡æ–™æˆ–åƒæ•¸ï¼Œä¸€äº›æ–¹æ³•è®Šå¾—ä¸åˆ‡å¯¦éš›ã€‚åœ¨ä¸€èˆ¬æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘ç™¼ç¾ï¼Œç†è«–ä¿è­‰çš„æ”¶æ–‚ä¸èƒ½æˆç‚ºå¯¦éš›æ‡‰ç”¨çš„ä¸€å€‹å¯é çš„æŒ‡å—ã€‚ åœ¨ç›®æ¨™æ˜¯å»ºç«‹ä¸€å€‹æ™®éç³»çµ±çš„æ‡‰ç”¨ç¨‹å¼ä¸­ï¼ŒéŽåº¦è¨“ç·´çš„å•é¡Œå‡ºç¾äº†ã€‚é€™å‡ºç¾åœ¨è¿´æ—‹æˆ–éŽåº¦å…·é«”çš„ç³»çµ±ä¸­ç•¶ç¶²è·¯çš„å®¹é‡å¤§å¤§è¶…éŽæ‰€éœ€çš„è‡ªç”±åƒæ•¸ã€‚ç‚ºäº†é¿å…é€™å€‹å•é¡Œï¼Œæœ‰å…©å€‹æ–¹å‘ï¼šç¬¬ä¸€å€‹æ˜¯ä½¿ç”¨äº¤å‰é©—è­‰å’Œé¡žä¼¼çš„æŠ€è¡“ä¾†æª¢æŸ¥éŽåº¦è¨“ç·´çš„å­˜åœ¨å’Œé¸æ“‡æœ€ä½³åƒæ•¸å¦‚æœ€å°åŒ–æ³›åŒ–èª¤å·®ã€‚äºŒæ˜¯ä½¿ç”¨æŸç¨®å½¢å¼çš„æ­£è¦åŒ–ã€‚é€™æ˜¯ä¸€å€‹åœ¨æ¦‚çŽ‡åŒ–ï¼ˆè²è‘‰æ–¯ï¼‰æ¡†æž¶é‡Œå‡ºç¾çš„æ¦‚å¿µï¼Œå…¶ä¸­çš„æ­£å‰‡åŒ–å¯ä»¥é€éŽç‚ºç°¡å–®æ¨¡åž‹é¸æ“‡ä¸€å€‹è¼ƒå¤§çš„å…ˆé©—æ¦‚çŽ‡æ¨¡åž‹é€²è¡Œï¼›è€Œä¸”åœ¨çµ±è¨ˆå­¸ç¿’ç†è«–ä¸­ï¼Œå…¶ç›®çš„æ˜¯æœ€å¤§é™åº¦åœ°æ¸›å°‘äº†å…©å€‹æ•¸é‡ï¼šã€Œé¢¨éšªã€å’Œã€Œçµæ§‹é¢¨éšªã€ï¼Œç›¸ç•¶æ–¼èª¤å·®åœ¨è¨“ç·´é›†å’Œç”±æ–¼éŽåº¦æ“¬åˆé€ æˆçš„é æ¸¬èª¤å·®ã€‚
https://pg-p.ctme.caltech.edu/blog/ai-ml/what-is-a-neural-network,"Caltech Bootcamp/Blog// Many of todayâ€™s information technologies aspire to mimic human behavior and thought processes as closely as possible. But do you realize that these efforts extend to imitating a human brain? The human brain is a marvel of organic engineering, and any attempt to create an artificial version will ultimately send the fields of Artificial Intelligence (AI) and Machine Learning (ML) to new heights. This article tackles the question, â€œWhat is a neural network?â€ We will define the term, outline the types of neural networks, compare the pros and cons, explore neural network applications, and finally, a way for you toupskill in AI and machine learning. So, before we explore the fantastic world of artificial neural networks and how they are poised to revolutionize what we know about AI, letâ€™s first establish a definition. So, what is a neural network anyway? A neural network is a method of artificial intelligence, a series of algorithms that teach computers to recognize underlying relationships in data sets and process the data in a way that imitates the human brain. Also, itâ€™s considered a type of machine learning process, usually called deep learning, that uses interconnected nodes or neurons in a layered structure, following the same pattern of neurons found in organic brains. This process creates an adaptive system that lets computers continuously learn from their mistakes and improve performance. Humans use artificial neural networks to solve complex problems, such as summarizing documents or recognizing faces, with greater accuracy. Neural networks are sometimes called artificial neural networks (ANN) to distinguish them from organic neural networks. After all, every person walking around today is equipped with a neural network. Neural networks interpret sensory data using a method of machine perception that labels or clusters raw input. The patterns that ANNs recognize are numerical and contained in vectors, translating all real-world data, including text, images, sound, or time series. Artificial neural networks form the basis of large-language models (LLMS) used by tools such as chatGPT, Googleâ€™s Bard, Microsoftâ€™s Bing, and Metaâ€™s Llama. Neural networks come in several types, listed below. Also Read:Is AI Engineering a Viable Career? Hereâ€™s a rundown of the types of neural networks available today. Using different neural network paths, ANN types are distinguished by how the data moves from input to output mode. This ANN is one of the least complex networks. Information passes through various input nodes in one direction until it reaches the output node. For example, computer vision and facial recognition use feed-forward networks. Recurrent neural networks are more complex than feed-forwards. They save processing node output and feed it into the model, a process that trains the network to predict a layerâ€™s outcome. Each RNN modelâ€™s node is a memory cell that continues computation and implements operations. For example, ANN is usually used in text-to-speech conversions. Convolution neural networks are one of todayâ€™s most popular ANN models. This model uses a different version of multilayer perceptrons, containing at least one convolutional layer that may be connected entirely or pooled. These layers generate feature maps that record an imageâ€™s region, are broken down into rectangles, and sent out. This ANN model is used primarily in image recognition in many of the more complex applications of Artificial Intelligence, like facial recognition, natural language processing, and text digitization. This type of neural network uses a reversed CNN model process that finds lost signals or features previously considered irrelevant to the CNN systemâ€™s operations. This model works well with image synthesis and analysis. Finally, modular neural networks have multiple neural networks that work separately from each other. These networks donâ€™t communicate or interfere with each otherâ€™s operations during the computing process. As a result, large or complex computational processes can be conducted more efficiently. Also Read:What is Machine Learning? A Comprehensive Guide for Beginners Neural network architecture emulates the human brain. Human brain cells, referred to as neurons, build a highly interconnected, complex network that transmits electrical signals to each other, helping us process information. Likewise, artificial neural networks consist of artificial neurons that work together to solve problems. Artificial neurons comprise software modules called nodes, and artificial neural networks consist of software programs or algorithms that ultimately use computing systems to tackle math calculations. Nodes are called perceptrons and are comparable to multiple linear regressions. Perceptrons feed the signal created by multiple linear regressions into an activation function that could be nonlinear. Hereâ€™s a look at basic neural network architecture. A deep neural network can theoretically map any input to the output type. However, the network also needs considerably more training than other machine learning methods. Consequently, deep neural networks need millions of training data examples instead of the hundreds or thousands a simpler network may require. Speaking of deep learning, letâ€™s explore the neural network machine learning concept. Standard machine learning methods need humans to input data for the machine learning software to work correctly. Then, data scientists determine the set of relevant features the software must analyze. This tedious process limits the softwareâ€™s ability. On the other hand, when dealing with deep learning, the data scientist only needs to give the software raw data. Then, the deep learning network extracts the relevant features by itself, thereby learning more independently. Moreover, it allows it to analyze unstructured data sets such as text documents, identify which data attributes need prioritization, and solve more challenging and complex problems. Also Read:AI ML Engineer Salary â€“ What You Can Expect To get a more in-depth answer to the question â€œWhat is a neural network?â€ itâ€™s super helpful to get an idea of the real-world applications theyâ€™re used for. Neural networks have countless uses, and as the technology improves, weâ€™ll see more of them in our everyday lives. Hereâ€™s a partial list of how neural networks are being used today. Letâ€™s start off the list with one of the most popular applications. Neural networks can analyze human speech despite disparate languages, speech patterns, pitch, tone, and accents. Virtual assistants such as Amazon Alexa and transcription software use speech recognition to: Computer vision lets computers extract insights and information from images and videos. Using neural networks, computers can distinguish and recognize images as humans can. Computer vision is used for: Natural language processing (NLP) is a computerâ€™s ability to process natural, human-made text. Neural networks aid computers in gathering insights and meaning from documents and other text data. NLP has many uses, including: If youâ€™ve ever ordered something online and later noticed that your social media newsfeed got flooded with recommendations for related products, congratulations! Youâ€™ve encountered a recommendation engine! Neural networks can track user activity and use the results to develop personalized recommendations. They can also analyze all aspects of a userâ€™s behavior and discover new products or services that could interest them. Pro tip: You can gain practical experience working on these applications in aninteractive AI/ML bootcamp. Also Read:What are Todayâ€™s Top Ten AI Technologies? Neural networks bring plenty of advantages to the table but also have downsides. So letâ€™s break things down into a list of pros and cons. Neural networks have a lot going for them, and as the technology gets better, they will only improve and offer more functionality. Unfortunately, itâ€™s not all sunshine and smooth sailing. Neural networks arenâ€™t perfect and have their drawbacks. Neural networks are gaining in popularity, so if youâ€™re interested in an exciting career in a technology thatâ€™s still in its infancy, consider taking anAI courseand setting your sights on an AI/ML position. This six-month course provides a high-engagement learning experience that teaches concepts and skills such as computer vision, deep learning, speech recognition, neural networks, NLP, and much more. The job website Glassdoor.com reports that an Artificial Intelligence Engineerâ€™s average yearly salary in the United States is $105,013. So, if youâ€™re ready to claim a good seat at the table of an industry thatâ€™s still new and growing, getting in at the ground floor of this exciting technology while enjoying excellent compensation, consider this bootcamp and get that ball rolling. The Future of AI: A Comprehensive Guide How Does AI Work? A Beginnerâ€™s Guide Machine Learning Engineer Salary: Trends in 2023 Your email address will not be published.Required fields are marked* Name* Email* Website Save my name, email, and website in this browser for the next time I comment.  Do you want to learn how to become a robotics engineer? Learn how to get into robotics engineering and what it takes to excel in this exciting field with our comprehensive guide. AI is now part of daily lives. This article explores the top AI technologies, including a brief definition of AI; its history, pros and cons, and a bit more about how it works for aspiring professionals in the field. This article contains the top machine learning interview questions and answers for 2024, broken down into introductory and experienced categories. This article defines artificial intelligence and gives examples of applications of AI in todayâ€™s commercial world. Want to learn the fundamentals of machine learning algorithms? Read this guide to understand ML algorithms, types, and popular ML models. Thereâ€™s a staggering demand for ML professionals across most industries today. If you want to get into this exciting field, check out this article explaining a typical machine learning engineer job description. Duration Learning Format California Institute of Technology"
https://www.ibm.com/jp-ja/topics/neural-networks,"Error: HTTPSConnectionPool(host='www.ibm.com', port=443): Max retries exceeded with url: /jp-ja/topics/neural-networks (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x760283e33c50>: Failed to establish a new connection: [Errno 101] Network is unreachable'))"
https://www.bmc.com/blogs/neural-network-introduction/,"Connected Digital Ops When you connect data and operations, you become an Autonomous Digital Enterprise able to optimize and accelerate innovation across your business. Featured Topics Technologies Industries Service Management Operations Management Workflow Orchestration Mainframe Partners Developers Integrations & Connectors Support Services & Consulting Education & Certification Why BMC? Engage with BMC We want to exploremachine learningon a deeper level by discussing neural networks. We will do that by explaining how you can useTensorFlowto recognize handwriting. But to do that we first must understand what are neural networks. We begin our discussion, based upon our knowledge of linear models, and draw some introductory material fromthis book writtenby Michael Nielsen. It isrecommendedby TensorFlow. To begin our discussion of how to use TensorFlow to work with neural networks, we first need to discuss whatneural networksare. Think of the linear regression problem we have look at several times here before. We have the concept of aloss function. A neural network hones in on the correct answer to a problem by minimizing the loss function. Suppose we have this simple linear equation:y = mx + b. This predicts some value of y given values of x. Predictive models are not always 100% correct. The measure of how incorrect it is is the loss. The goal of machine learning it to take a training set to minimize the loss function. That is true with linear regression, neural networks, and other ML algorithms. For example, suppose m = 2, x = 3, and b = 2. Then our predicted value of y = 2 * 3 + 2 = 8. But our actual observed value is 10. So the loss is 10 â€“ 8 = 2. In a neural network, we have the same basic principle, except the inputs are binary and the outputs are binary. The objects that do the calculations areperceptrons. They adjust themselves to minimize the loss function until the model is very accurate. For example, we can get handwriting analysis to be 99% accurate. Neural networks are designed to work just like the human brain does. In the case of recognizing handwriting or facial recognition, the brain very quickly makes some decisions. For example, in the case of facial recognition, the brain might start with â€œIt is female or male? Is it black or white? Is it old or young? Is there a scar?â€ and so forth. Michael Nielsen lays this out in his book like the diagram below. All of these inputs (x1, x2, x3) are fed into a perceptron. That then makes a yes or no decision and passes it onto the next perceptron for the next decision. This process repeats until the final perceptron. At which point we know what the handwriting is or whose face we are looking at.  Letâ€™s illustrate with a small example. This topic is complex, so we will present the first concept here and in the next post take it a step further. As we said, aperceptronis an object that takes binary inputs and outputs a binary output. It uses a weighted sum and a threshold to decide whether the outcome should be yes (1) or no (0). For example, suppose you want to go to France but only if: You represent this decision with this simple vector of possible inputs: (1,0), (0,1), (1,1), and (0,0). In the first case (1,0) the ticket is > 1,000 and your girlfriend or boyfriend cannot go with you. You put some weight on each of these two calculations. For example, if you are on a budget and cost is important, give it weight w1=4. And whether your partner can go or not is not as important. So give it a weight of w2=3. So you have this function forGo to France: (x1 * w1) + (x2 * w2) = (x1 * 4) + (x2 * 3) > some threshold, b, say, 4. We move b to the other side and write: If (x1 * 4) + (x2 * 3) -4 > 0 then Go to France (i.e., perceptron says 1)- Then feed vectors into the equation. Obviously if the ticket is > $1,000 and if your girlfriend cannot go (0,0) then you will not make the trip, because (0 * 3) + (0 * 4) â€“ 4 is obviously < 0. If the ticket is cheap but you are going alone then go anyway: (1 * 4) + (0 * 3) â€“ 4 = 0 which is not bigger than 0. Handwriting and facial recognition using neural networks does the same thing, meaning making a series of binary decisions. This is because any image can be broken down into its smallest object, the pixel. In the case of handwriting, like shown below, each pixel is either black (1) or white (meaning empty, or 0). Graphic Source Michael Neilson. Already we introduced the concept of perceptrons, which take inputs from simple linear equations and output 1 (true) or 0 (false). They are the left-hand side of the neural network.  But as Michael Nielsen explains, inhis book, perceptrons are not suitable for tasks like image recognition because small changes to the weights and biases product large changes to the output. After all, going to 0 to 1 is a large change. It would be better to go from, say, 0.6 to 0.65. Suppose have a simple neural network with two input variables x1 and x2 and a bias of 3 with weights of -2 and -3. The equation for that is: If -2Ã—1 + -3Ã—2 + 3 < 0 then 1 (true) otherwise 0 (false). (Thatâ€™s not exactly the correct way to express that in algebra, but it is close enough. The goal here is to keep the math to a minimum to make it easier to understand. Michaelâ€™s paper is difficult to understand for those without a math background.) Machine learning adjusts the weights and the biases until the resulting formula most accurately calculates the correct value. Remember from the last post, that this is the same as saying that adjusting the weights and biases reduces the lossfunctionto its minimum. Most ML problems work that way. For example, linear regression.  So how do we avoid the large change of going from 0 to 1, which would mess up our model? We allow inputs and output numbers between 0 and 1 instead of just 0 or 1. The simplest way to do that is to divide the equation into the number 1, by using a similar formula, as that used by logistic regression. And then we adopt the convention that if the final output value of the neural network has a threshold, say 0.5, then we can conclude that the outcome is 1. But isnâ€™t that just a roundabout way of calculating something that results in either 0 or 1? No. Because in a neural network there is not just the input initial values and the resulting output. In the middle, there are intermediate steps calledhidden layers. Those need not evaluate to 0 or 1. (You can play around with a neural network to add or remove hidden layers using thisonline tool.)  To illustrate, let z=x1w1 + x2w2 + b be the function above. Then we create a modified perception called asigmoid neuronfunction (Î´) like this.  Now we state that the values of x1 and x2 in function z do not have to be integers. They can be any value between 0 and 1, as a result of which the sigmoid neuron function Î´ will vary between 0 and 1. Remember that exp,the constant e = 2.714. Raising it to a negative power is the same as dividing it into 1, i.e. exp(-z) = 1 / exp(z). When the value of z is large then exp(-z) is small (close to zero). Because 1 divided by something large is small. In that case, the sigmoid neuron function is close to 1. Conversely, when z is small then 1/(1 + exp(-z) is close to 0. But for values that are neither large nor small, Î´ does not vary much. With artificial intelligence, wetrainthe neural network by varying the weights x1, x2, x3, â€¦ , xn and the bias b. That is to say, we vary the inputs to minimize the loss function. That is no different than simple linear regression. Remember that the loss function is just the difference between the predicted value and the observed value. When there is just 1 or 2 inputs that is easy. But with handwriting recognition there are hundreds or thousands of inputs. (For an image of 256 pixels there are 256 * 256 inputs in our neural network, it looks something like this, except that this has been made smaller so that you can visualize it. And this network only looks at digits and not the whole alphabet.)  With simple linear regression, the loss function is the distance between the observed value z and the predicted value p, or z â€“ p. With neural networks we use something more complicated called thestochastic gradient descent, which is not necessary to be understood.It will suffice to say that it is basically the same thing. But finding the minimum value in some function with thousands of input variables is hard to achieve, so thestochastic gradient descentfirst takes a guess and then works from there. Michael Nielsen gives this analogy. Below is a graph of a loss function f(x,y), i.e. a function with two inputs. If you drop a marble into that bowl then it will roll to the lowest point. Thestochastic gradient descentis an algorithm to find that point for a loss function with many input variables. (For those who know calculus, you might say why not just take the derivative of that function and find its minimum? The answer is that you cannot easily find the derivative for a function with thousands of variables.)  Anyway, letâ€™s now see how this works with handwriting recognition. Here is an image of the number â€œ0â€. The neural network looks at each pixel, and how dark the pixel is, to figure out which pixels are filled in. Then it matches that with handwriting samples known to represent the number 0. The MNIST training set takes handwriting samples from 250 people. This data takes the combination of pixels of each drawing and indicates whether it is a 0, 1, 2, â€¦, or 9.  The neural network is thentrained,based on this data, i.e., it adjusts the coefficients and bias until it most accurately determines what digit it is. Then you plug in handwriting samples from people who are not present in the training set. This new set of data is called thetestingset, which makes it possible to read what these people have written. This e-book teaches machine learning in the simplest way possible. This book is for managers, programmers, directors â€“ and anyone else who wants to learn machine learning. We start with very basic stats and algebra and build upon that. These postings are my own and do not necessarily represent BMC's position, strategies, or opinion. See an error or have a suggestion? Please let us know by emailing[emailÂ protected]. BMC empowers 86% of the Forbes Global 50 to accelerate business value faster than humanly possible. Our industry-leading portfolio unlocks human and machine potential to drive business growth, innovation, and sustainable success. BMC does this in a simple and optimized way by connecting people, systems, and data that power the worldâ€™s largest organizations so they can seize a competitive advantage.Learn more about BMC â€º Walker Rowe is an American freelancer tech writer and programmer living in Cyprus. He writes tutorials on analytics and big data and specializes in documenting SDKs and APIs. He is the founder of theHypatia Academy Cyprus, an online school to teach secondary school children programming. You can find Walkerhereandhere. ContactFree TrialsLegalPrivacy PolicyEmail Opt-OutTrust CenterÂ©Copyright 2005-2024 BMC Software, Inc. Use of this site signifies your acceptance of BMCâ€™sTerms of Use. BMC, the BMC logo, and other BMC marks are assets of BMC Software, Inc. These trademarks areregisteredandmay be registeredin the U.S. and in other countries."
https://www.javatpoint.com/artificial-neural-network,"Artificial Neural Network Tutorial provides basic and advanced concepts of ANNs. Our Artificial Neural Network tutorial is developed for beginners as well as professions. The term ""Artificial neural network"" refers to a biologically inspired sub-field of artificial intelligence modeled after the brain. An Artificial neural network is usually a computational network based on biological neural networks that construct the structure of the human brain. Similar to a human brain has neurons interconnected to each other, artificial neural networks also have neurons that are linked to each other in various layers of the networks. These neurons are known as nodes. Artificial neural network tutorial covers all the aspects related to the artificial neural network. In this tutorial, we will discuss ANNs, Adaptive resonance theory, Kohonen self-organizing map, Building blocks, unsupervised learning, Genetic algorithm, etc. The term ""Artificial Neural Network"" is derived from Biological neural networks that develop the structure of a human brain. Similar to the human brain that has neurons interconnected to one another, artificial neural networks also have neurons that are interconnected to one another in various layers of the networks. These neurons are known as nodes. The given figure illustrates the typical diagram of Biological Neural Network. The typical Artificial Neural Network looks something like the given figure. Dendrites from Biological Neural Network represent inputs in Artificial Neural Networks, cell nucleus represents Nodes, synapse represents Weights, and Axon represents Output. Relationship between Biological neural network and artificial neural network: AnArtificial Neural Networkin the field ofArtificial intelligencewhere it attempts to mimic the network of neurons makes up a human brain so that computers will have an option to understand things and make decisions in a human-like manner. The artificial neural network is designed by programming computers to behave simply like interconnected brain cells. There are around 1000 billion neurons in the human brain. Each neuron has an association point somewhere in the range of 1,000 and 100,000. In the human brain, data is stored in such a manner as to be distributed, and we can extract more than one piece of this data when necessary from our memory parallelly. We can say that the human brain is made up of incredibly amazing parallel processors. We can understand the artificial neural network with an example, consider an example of a digital logic gate that takes an input and gives an output. ""OR"" gate, which takes two inputs. If one or both the inputs are ""On,"" then we get ""On"" in output. If both the inputs are ""Off,"" then we get ""Off"" in output. Here the output depends upon input. Our brain does not perform the same task. The outputs to inputs relationship keep changing because of the neurons in our brain, which are ""learning."" To understand the concept of the architecture of an artificial neural network, we have to understand what a neural network consists of. In order to define a neural network that consists of a large number of artificial neurons, which are termed units arranged in a sequence of layers. Lets us look at various types of layers available in an artificial neural network. Artificial Neural Network primarily consists of three layers: Input Layer: As the name suggests, it accepts inputs in several different formats provided by the programmer. Hidden Layer: The hidden layer presents in-between input and output layers. It performs all the calculations to find hidden features and patterns. Output Layer: The input goes through a series of transformations using the hidden layer, which finally results in output that is conveyed using this layer. The artificial neural network takes input and computes the weighted sum of the inputs and includes a bias. This computation is represented in the form of a transfer function. It determines weighted total is passed as an input to an activation function to produce the output. Activation functions choose whether a node should fire or not. Only those who are fired make it to the output layer. There are distinctive activation functions available that can be applied upon the sort of task we are performing. Parallel processing capability: Artificial neural networks have a numerical value that can perform more than one task simultaneously. Storing data on the entire network: Data that is used in traditional programming is stored on the whole network, not on a database. The disappearance of a couple of pieces of data in one place doesn't prevent the network from working. Capability to work with incomplete knowledge: After ANN training, the information may produce output even with inadequate data. The loss of performance here relies upon the significance of missing data. Having a memory distribution: For ANN is to be able to adapt, it is important to determine the examples and to encourage the network according to the desired output by demonstrating these examples to the network. The succession of the network is directly proportional to the chosen instances, and if the event can't appear to the network in all its aspects, it can produce false output. Having fault tolerance: Extortion of one or more cells of ANN does not prohibit it from generating output, and this feature makes the network fault-tolerance. Assurance of proper network structure: There is no particular guideline for determining the structure of artificial neural networks. The appropriate network structure is accomplished through experience, trial, and error. Unrecognized behavior of the network: It is the most significant issue of ANN. When ANN produces a testing solution, it does not provide insight concerning why and how. It decreases trust in the network. Hardware dependence: Artificial neural networks need processors with parallel processing power, as per their structure. Therefore, the realization of the equipment is dependent. Difficulty of showing the issue to the network: ANNs can work with numerical data. Problems must be converted into numerical values before being introduced to ANN. The presentation mechanism to be resolved here will directly impact the performance of the network. It relies on the user's abilities. The duration of the network is unknown: The network is reduced to a specific value of the error, and this value does not give us optimum results. Artificial Neural Network can be best represented as a weighted directed graph, where the artificial neurons form the nodes. The association between the neurons outputs and neuron inputs can be viewed as the directed edges with weights. The Artificial Neural Network receives the input signal from the external source in the form of a pattern and image in the form of a vector. These inputs are then mathematically assigned by the notations x(n) for every n number of inputs. Afterward, each of the input is multiplied by its corresponding weights ( these weights are the details utilized by the artificial neural networks to solve a specific problem ). In general terms, these weights normally represent the strength of the interconnection between neurons inside the artificial neural network. All the weighted inputs are summarized inside the computing unit. If the weighted sum is equal to zero, then bias is added to make the output non-zero or something else to scale up to the system's response. Bias has the same input, and weight equals to 1. Here the total of weighted inputs can be in the range of 0 to positive infinity. Here, to keep the response in the limits of the desired value, a certain maximum value is benchmarked, and the total of weighted inputs is passed through the activation function. The activation function refers to the set of transfer functions used to achieve the desired output. There is a different kind of the activation function, but primarily either linear or non-linear sets of functions. Some of the commonly used sets of activation functions are the Binary, linear, and Tan hyperbolic sigmoidal activation functions. Let us take a look at each of them in details: In binary activation function, the output is either a one or a 0. Here, to accomplish this, there is a threshold value set up. If the net weighted input of neurons is more than 1, then the final output of the activation function is returned as one or else the output is returned as 0. The Sigmoidal Hyperbola function is generally seen as an ""S"" shaped curve. Here the tan hyperbolic function is used to approximate output from the actual net input. The function is defined as: F(x) = (1/1 + exp(-????x)) Where ???? is considered the Steepness parameter. There are various types of Artificial Neural Networks (ANN) depending upon the human brain neuron and network functions, an artificial neural network similarly performs tasks. The majority of the artificial neural networks will have some similarities with a more complex biological partner and are very effective at their expected tasks. For example, segmentation or classification. In this type of ANN, the output returns into the network to accomplish the best-evolved results internally. As per theUniversity of Massachusetts, Lowell Centre for Atmospheric Research. The feedback networks feed information back into itself and are well suited to solve optimization issues. The Internal system error corrections utilize feedback ANNs.  No specific expertise is needed as a prerequisite before starting this tutorial. Our Artificial Neural Network Tutorial is developed for beginners as well as professionals, to help them understand the basic concept of ANNs. We assure you that you will not find any problem in this Artificial Neural Network tutorial. But if there is any problem or mistake, please post the problem in the contact form so that we can further improve it. 1. The activation function of the artificial neural network serves what main purpose? Answer:b) To supply non-linearity into the network Explanation:They are applied to introduce non-linearity into the network so that the network can be able to learn various forms of patterns. They decide the output level of a neuron by using the weighted sum of the inputs passing through its control. 2.Which of the following is not an example of an artificial neural network? This model is: Answer:d) Logistic regression Explanation:Logistic regression is actually a model used in classification thus it cannot in anyway be considered as a neural network. 3. What is the function of an algorithm known as backpropagation in the construction of neural networks? Answer:c) For the weights of network applied corrections aimed at minimizing an error Explanation:Backpropagation is an algorithm that calculates the error from the input which has been processed through the network and then goes back through the connections to modify weights accordingly. 4. What layer of neural network take the input data? Answer:c) Input layer Explanation:The input layer is the outermost layer in a neural network and is responsible for taking the input data which is to be analysed. 5. The explicit bias term in a neuron is used hence to introduce an explicit bias to the activation function employed in the neuron. Answer:b) To replace the activation function Explanation:The bias term makes the activation function to be flexible in learning other patterns since it shifts the curve up or down. 6. Which activation function we use in the last layer for classification problem? There are four activation functions, including Answer:b) Sigmoid Explanation:The sigmoid activation function is utilized in the output layer of the feed forward networks particularly in classification problems since the output value ranges between 0 and 1 which may be seen as a probable distribution of the input patterns. 7. What can be considered as the primary distinction between supervised and unsupervised learning in neural nets? Answer:a) Supervised learning work on labelled data set while unsupervised learning does not work on labelled data set. Explanation:Supervised learning on the other hand, involves learning where the input values are well defined and pointed out while the output values. In case of unsupervised learning the network looks for different relationships inside the data and has no tags for it. 8. What is the architecture of the neural network for the aims of image recognition? Answer:b) Convolutional neural network Explanation:Once trained, CNNs are the best to use for special types of data such as images, for example when classifying an image or detecting an object in an image. 9. What is exactly the vanishing gradient problem in the deep neural networks? Answer:b) When the gradients become too small Explanation:This is because during the back propagation the gradients become very much small and thus leading to loss of gradient problem. 10. Which technique is favourable in avoiding overfitting in the case of neural network? Answer:d) All of the above Explanation:Quite a few methods are applied to control over fitting; among them, we have regularization techniques, dropout, and early stopping. 11. What is a hidden layer of a neural network and what does it look like? The decision rule: Answer:c) For the purpose of concept acquisition that involves learning of complex patterns and features. Explanation:In a neural network there are always one or more hidden layers which are responsible for extracting hard and comprehensive features from the input data as used in the predictions or classifications. 12. Which activation function do we often apply in the hidden layers of the artificial neural network? Answer:a) ReLU Explanation:The rectified linear unit (ReLU) non-linearity is used universally as the activation function in the hidden layer of neural networks for their mathematical efficiency and non-sensitive nature to the vanishing gradient issue. 13. What is the principal drawback of Neural Networks? It is for the following reasons: Answer:d) All the above. Explanation:Thatâ€™s why, training deep networks in neural networks can be a big computationally demanding. They also can be ambiguous, because it is rather complicated to understand the inner structure of the network. Also, neural networks are data intensive, and as such may demand big data to have an accurate training. We provides tutorials and interview questions of all technology like java tutorial, android, java frameworks G-13, 2nd Floor, Sec-3, Noida, UP, 201301, India [emailÂ protected]. Latest Post PRIVACY POLICY"
https://datasolut.com/neuronale-netzwerke-einfuehrung/,"KÃ¼nstliche Neuronale Netze (KNN)sind dem menschlichen Gehirn nachempfunden und werden fÃ¼r maschinelles Lernen und KÃ¼nstliche Intelligenz eingesetzt. Computerbasiert lassen sich damit diverse Problemstellungen lÃ¶sen, die fÃ¼r uns Menschen fast unmÃ¶glich wÃ¤ren. In diesem Artikel erklÃ¤re ich, wie kÃ¼nstliche neuronale Netze funktionieren, wie sie aufgebaut sind und wo sie eingesetzt werden. KÃ¼nstliche neuronale Netze sind Algorithmen, die dem menschlichen Gehirn nachempfunden sind. Dieses abstrahierte Modell miteinander verbundener kÃ¼nstlicher Neuronen ermÃ¶glicht es, komplexe Aufgaben aus den Bereichen Statistik, Informatik und Wirtschaft durch Computer zu lÃ¶sen. Neuronale Netze sind ein sehr aktives Forschungsgebiet und gelten als Grundlage der kÃ¼nstlichen Intelligenz. Neuronale Netze ermÃ¶glichen es, unterschiedliche Datenquellen wie Bilder, TÃ¶ne, Texte, Tabellen oder Zeitreihen zu interpretieren und Informationen oder Muster zu extrahieren, um diese auf unbekannte Daten anzuwenden. Auf diese Weise kÃ¶nnen datenbasierte Vorhersagen fÃ¼r die Zukunft getroffen werden. KÃ¼nstliche neuronale Netze kÃ¶nnen unterschiedlich komplex aufgebaut sein, haben aber im Wesentlichen die Struktur gerichteter Graphen. Weist ein kÃ¼nstliches neuronales Netz besonders tiefe Netzstrukturen auf, spricht man vonDeep Learning. Zum Thema KÃ¼nstliche Neuronale Netzwerke haben wir bereits ein Video: Vereinfacht kann man sich den Aufbau eines KNN wie folgt vorstellen: Das Modell des Neuronalen Netzes besteht aus Knoten, auch Neuronen genannt, die Informationen von anderen Neuronen oder von auÃŸen aufnehmen, modifizieren und als Ergebnis wieder ausgeben. Dies geschieht Ã¼ber drei verschiedene Schichten, denen jeweils ein Typ von Neuronen zugeordnet werden kann: solche fÃ¼r den Input (Eingabeschicht), solche fÃ¼r den Output (Ausgabeschicht) und so genannte Hidden Neuronen (verborgene Schichten). Die Information wird durch die Input-Neuronen aufgenommen und durch die Output-Neuronen ausgegeben. Die Hidden-Neuronen liegen dazwischen und bilden innere Informationsmuster ab. Die Neuronen sind miteinander Ã¼ber sogenannte Kanten verbunden. Je stÃ¤rker die Verbindung ist, desto grÃ¶ÃŸer die Einflussnahme auf das andere Neuron. Schauen wir uns die Schichten einmal genauer an: Tiefes Lernenist eine Hauptfunktion eines KNN und funktioniert wie folgt: Bei einer vorhandenen Netzstruktur bekommt jedes Neuron ein zufÃ¤lliges Anfangsgewicht zugeteilt. Dann werden die Eingangsdaten in das Netz gegeben und von jedem Neuron mit seinem individuellen Gewicht gewichtet. Das Ergebnis dieser Berechnung wird an die nÃ¤chsten Neuronen der nÃ¤chsten Schicht oder des nÃ¤chsten Layers weitergegeben, man spricht auch von einer â€žAktivierung der Neuronenâ€œ. Eine Berechnung des Gesamtergebnis geschieht am Outputlayer. NatÃ¼rlich sind, wie bei jedem maschinellen Lernverfahren, nicht alle Ergebnisse (Outputs) korrekt und es treten Fehler auf. Diese Fehler sind berechenbar, ebenso wie der Anteil eines einzelnen Neurons am Fehler. Im nÃ¤chsten Lerndurchgang wird das Gewicht jedes Neurons so verÃ¤ndert, dass der Fehler minimiert wird. Im nÃ¤chsten Durchlauf wird der Fehler erneut gemessen und angepasst. Auf diese Weise â€žlerntâ€œ das neuronale Netz von Mal zu Mal besser, von den Eingabedaten auf bekannte Ausgabedaten zu schlieÃŸen. Dieser Prozess ist dem menschlichen Entscheidungsprozess sehr Ã¤hnlich. Die Denkweise des Menschen ist Ã¤hnlich verschachtelt wie die eines neuronalen Netzes. Wo finden denn nun solche Netzwerke Anwendungen? Nun, da gibt es zahlreiche MÃ¶glichkeiten. Typischerweise sind sie prÃ¤destiniert fÃ¼r solche Bereiche, bei denen wenig systematisches Wissen vorliegt, aber eine groÃŸe Menge unprÃ¤ziser Eingabeinformationen (unstrukturierte Daten) verarbeitet werden mÃ¼ssen, um ein konkretes Ergebnis zu erhalten. Das kann zum Beispiel in der Spracherkennung, Mustererkennung, Gesichtserkennung oderBilderkennungder Fall sein. Weitere AnwendungsfÃ¤lle fÃ¼r neuronale Netze sind: Zahlreiche Produkte und Dienstleistungen, die auf kÃ¼nstlichen neuronalen Netzen basieren, haben bereits Einzug in unseren Alltag gehalten. Global agierende Konzerne wie Google, Facebook oder auch Amazon sind hier wichtige Vertreter und gelten mitunter als Vorreiter in der Entwicklung und Anwendung von Deep Learning und kÃ¼nstlicher Intelligenz. Neuronale Netze bilden die Grundlage der KÃ¼nstlichen Intelligenz und sind bereits heute in der Lage, durch gezieltes Training sehr spezifische Aufgaben zu Ã¼bernehmen (schwacheKÃ¼nstliche Intelligenz). Es gibt unzÃ¤hlig viele Typen von neuronalen Netzwerk-Architekturen. Wir zeigen hier die wichtigsten Arten von neuronalen Netzen: Das einfachste und Ã¤lteste neuronale Netz. Es nimmt die Eingabeparameter, addiert diese, wendet die Aktivierungsfunktion an und schickt das Ergebnis an die Ausgabeschicht. Das Ergebnis ist binÃ¤r, also entweder 0 oder 1 und damit vergleichbar mit einer Ja- oder Nein-Entscheidung. Die Entscheidung erfolgt, indem man den Wert der Aktivierungsfunktion mit einem Schwellwert vergleicht. Bei Ãœberschreitung des Schwellwertes, wird dem Ergebnis eine 1 zugeordnet, hingegen 0 wenn der Schwellwert unterschritten wird. Darauf aufbauend wurden weitere Neuronale Netzwerke und Aktivierungsfunktionen entwickelt, die es auch ermÃ¶glichen mehrere Ausgaben mit Werten zwischen 0 und 1 zu erhalten. Am bekanntesten ist die Sigmoid-Funktion, in dem Fall spricht man auch von Sigmoid-Neuronen. Der Ursprung dieser neuronalen Netze liegt in den 1950 Jahren. Sie zeichnen sich dadurch aus, dass die Schichten lediglich mit der nÃ¤chst hÃ¶heren Schicht verbunden sind. Es gibt keine zurÃ¼ckgerichteten Kanten. Der Trainingsprozess eines Feed Forward Neural Network (FF) lÃ¤uft dann in der Regel so ab: Wenn besonders viele Schichten zwischen Eingangs- und Ausgangsschicht sind, spricht man vonDeep Feed Forward Neural Networksâ€ž Faltende Neuronale Netze oder auch Convolutional Neural Networks (CNN), sind KÃ¼nstliche Neuronale Netzwerke, die besonders effizient mit 2D- oder 3D-Eingabedaten arbeiten kÃ¶nnen. FÃ¼r die Objektdetektion in Bildern verwendet man insbesondere CNNs. Der groÃŸe Unterschied zu den klassischen neuronalen Netzen liegt in der Architektur der CNNs, die auch den Namen â€žConvolutionâ€œ oder â€žFaltungâ€œ erklÃ¤rt. Bei CNNs basiert die verborgene Schicht auf einer Abfolge von Faltungs- und Poolingoperationen. Bei der Faltung wird ein sogenannter Kernel Ã¼ber die Daten geschoben und wÃ¤hrenddessen eine Faltung berechnet, was mit einer Multiplikation vergleichbar ist. Die Neuronen werden aktualisiert. Die anschlieÃŸende EinfÃ¼hrung einer Pooling-Schicht sorgt fÃ¼r eine Vereinfachung der Ergebnisse. Nur die wichtigen Informationen bleiben erhalten. Dies sorgt auch dafÃ¼r, dass die 2D- oder 3D-Eingangsdaten kleiner werden. Setzt man diesen Prozess fort, so erhÃ¤lt man am Ende in der Ausgabeschicht einen Vektor, den â€žfully connected layerâ€œ. Dieser hat vor allem in der Klassifikation eine besondere Bedeutung, da er ebenso viele Neuronen wie Klassen enthÃ¤lt und die entsprechende Zuordnung Ã¼ber eine Wahrscheinlichkeit bewertet. Recurrent Neural Networks (RNN) fÃ¼gen den KNN wiederkehrende Zellen hinzu, wodurch neuronale Netze ein GedÃ¤chtnis erhalten. Das erste kÃ¼nstliche, neuronale Netzwerk dieser Art war dasJordan-Netzwerk, bei dem jede versteckte Zelle ihre eigene Ausgabe mit fester VerzÃ¶gerung â€“ eine oder mehrere Iterationen â€“ erhielt. Ansonsten ist es vergleichbar mit den klassischen Feed Forward Netzen. NatÃ¼rlich gibt es viele Variationen, wie z.B. die Ãœbergabe des Status an die Eingangsknoten, variable VerzÃ¶gerungen usw., aber die Grundidee bleibt die gleiche. Diese Art von NN wird insbesondere dann verwendet, wenn der Kontext wichtig ist. In diesem Fall haben Entscheidungen aus frÃ¼heren Iterationen oder Stichproben einen signifikanten Einfluss auf die aktuellen Iterationen. Da rekurrente Netze jedoch den entscheidenden Nachteil haben, dass sie mit der Zeit instabil werden, ist es mittlerweile Ã¼blich, sogenannte Long Short-Term Memory Units (kurz: LSTMs) zu verwenden. Diese stabilisieren das RNN auch fÃ¼r AbhÃ¤ngigkeiten, die Ã¼ber einen lÃ¤ngeren Zeitraum bestehen. Das hÃ¤ufigste Beispiel fÃ¼r solche AbhÃ¤ngigkeiten ist die Textverarbeitung â€“ ein Wort kann nur im Zusammenhang mit vorhergehenden WÃ¶rtern oder SÃ¤tzen analysiert werden. Ein weiteres Beispiel ist die Verarbeitung von Videos, z.B. beim autonomen Fahren. Objekte in Bildsequenzen werden erkannt und Ã¼ber die Zeit verfolgt. Weitere Quellen: http://neuralnetworksanddeeplearning.com/ KÃ¼nstliche neuronale Netze Ã¤hneln der Funktionsweise des menschlichen Gehirns und eignen sich daher hervorragend fÃ¼r alle Bereiche des maschinellen Lernens und der kÃ¼nstlichen Intelligenz. Daraus ergibt sich fÃ¼r Unternehmen eine Vielzahl von MÃ¶glichkeiten, die Effizienz ihres Unternehmens zu steigern. Aufgrund der vielen Vorteile von kÃ¼nstlichen neuronalen Netzen kÃ¶nnen viele komplexe Probleme in den Bereichen Statistik, Informatik und Wirtschaft von Computern gelÃ¶st werden. Profitieren auch Sie von diesen Vorteilen! MÃ¶chten auch Sie die Vorteile KÃ¼nstlicher Neuronaler Netze nutzen oder haben Sie weitere Fragen zu diesem Thema?KontaktierenSie mich gerne. Ob und wie kÃ¼nstliche Intelligenz Ihnen weiterhelfen kann, kÃ¶nnen Sie in einem ersten, unverbindlichen GesprÃ¤ch mit uns herausfinden. In diesem GesprÃ¤ch erfahren Sie: Sie sehen gerade einen Platzhalterinhalt vonHubSpot. Um auf den eigentlichen Inhalt zuzugreifen, klicken Sie auf die SchaltflÃ¤che unten. Bitte beachten Sie, dass dabei Daten an Drittanbieter weitergegeben werden."
https://www.mathworks.com/discovery/neural-network.html,"Training Events Learning Resources Visit theHelp Centerto explore product documentation, engage with community forums, check release notes, and more. MATLAB and Simulink Videos Learn about products, watch demonstrations, and explore what's new. Company Careers Decarbonizing MathWorks See how MathWorks is protecting and restoring Earthâ€™s resources. Search A neural network (also called an artificial neural network or ANN) is an adaptive system that learns by using interconnected nodes or neurons in a layered structure that resembles a human brain. A neural network can learn from data, so it can be trained to recognize patterns, classify data, and forecast future events. A neural network breaks down the input into layers of abstraction. It can be trained using many examples to recognize patterns in speech or images just as the human brain does. The neural network behavior is defined by the way its individual elements are connected and by the strength, or weights, of those connections. These weights are automatically adjusted during training according to a specified learning rule until the artificial neural network performs the desired task correctly. Table of Contents Neural networks are a type of machine learning approach inspired by how neurons signal to each other in the human brain. Neural networks are especially suitable for modeling nonlinear relationships, and they are typically used to performpattern recognitionand classify objects or signals in speech, vision, and control systems. Neural networks, particularly deep neural networks, have become known for their proficiency at complex identification applications such as face recognition, text translation, and voice recognition. These approaches are a key technology driving innovation in advanced driver assistance systems and tasks, including lane classification and traffic sign recognition. Here are a few examples of how neural networks are used in machine learning applications: Inspired by biological nervous systems, a neural network combines several processing layers using simple elements operating in parallel. The network consists of an input layer, one or more hidden layers, and an output layer. In each layer there are several nodes, or neurons, and the nodes in each layer use the outputs of all nodes in the previous layer as inputs, such that all neurons interconnect with each other through the different layers. Each neuron is typically assigned a weight that is adjusted during the learning process. Decreases or increases in the weight change the strength of that neuronâ€™s signal. Typical neural network architecture. Like othermachine learning algorithms, neural networks can be used for classification or regression tasks. Model parameters are set by weighting the neural network throughlearningon training data, typically by optimizing weights to minimize prediction error. The first and simplest neural network was the perceptron, introduced by Frank Rosenblatt in 1958. It consisted of a single neuron and essentially a linear regression model with a sigmoid activation function. Since then, increasingly complex neural networks have been explored, leading up to todayâ€™s deep networks, which can contain hundreds of layers. Deep learningrefers to neural networks with many layers, whereas neural networks with only two or three layers of connected neurons are also known as shallow neural networks. Deep learning has become popular because it eliminates the need to extract features from images, which previously challenged the application of machine learning to image and signal processing. Although feature extraction can be omitted in image processing applications, some form of feature extraction is still commonly applied to signal processing tasks to improve model accuracy. There are three common types of neural networks used for engineering applications: Supervised Machine Learning Watch this short video with the specifics of CNNs, including layers, activations, and classification. Learn more about deep learning: UsingMATLABÂ®withDeep Learning Toolboxâ„¢andStatistics and Machine Learning Toolboxâ„¢, you can create deep and shallow neural networks for applications such ascomputer visionand automated driving. With just a few lines of code, you can create neural networks in MATLAB without being an expert. You can get started quickly, train and visualize neural network models, and integrate neural networks into your existing system and deploy them to servers, enterprise systems, clusters, clouds, and embedded devices. Developing AI applications, and specificallyÂ including neural networks, typically involves these steps: Learn about MATLAB support for deep learning. Get started with MATLAB for deep learning. Select a Web Site Choose a web site to get translated content where available and see local events and offers. Based on your location, we recommend that you select:.  You can also select a web site from the following list How to Get Best Site Performance Select the China site (in Chinese or English) for best site performance. Other MathWorks country sites are not optimized for visits from your location. Americas Europe Asia Pacific Contact your local office MathWorks Accelerating the pace of engineering and science MathWorks is the leading developer of mathematical computing software for engineers and scientists. Discoverâ€¦ Explore Products Try or Buy Learn to Use Get Support About MathWorks Â© 1994-2024 The MathWorks, Inc. "
https://zhuanlan.zhihu.com/p/404173054,Error: 403 Client Error: Forbidden for url: https://zhuanlan.zhihu.com/p/404173054
http://neuralnetworksanddeeplearning.com/,"Neural Networks and Deep Learning What this book is about On the exercises and problems Using neural nets to recognize handwritten digitsPerceptronsSigmoid neuronsThe architecture of neural networksA simple network to classify handwritten digitsLearning with gradient descentImplementing our network to classify digitsToward deep learningHow the backpropagation algorithm worksWarm up: a fast matrix-based approach to computing the output  from a neural networkThe two assumptions we need about the cost functionThe Hadamard product, $s \odot t$The four fundamental equations behind backpropagationProof of the four fundamental equations (optional)The backpropagation algorithmThe code for backpropagationIn what sense is backpropagation a fast algorithm?Backpropagation: the big pictureImproving the way neural networks learnThe cross-entropy cost functionOverfitting and regularizationWeight initializationHandwriting recognition revisited: the codeHow to choose a neural network's hyper-parameters?Other techniquesA visual proof that neural nets can compute any functionTwo caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusionWhy are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 PerceptronsSigmoid neuronsThe architecture of neural networksA simple network to classify handwritten digitsLearning with gradient descentImplementing our network to classify digitsToward deep learning How the backpropagation algorithm worksWarm up: a fast matrix-based approach to computing the output  from a neural networkThe two assumptions we need about the cost functionThe Hadamard product, $s \odot t$The four fundamental equations behind backpropagationProof of the four fundamental equations (optional)The backpropagation algorithmThe code for backpropagationIn what sense is backpropagation a fast algorithm?Backpropagation: the big pictureImproving the way neural networks learnThe cross-entropy cost functionOverfitting and regularizationWeight initializationHandwriting recognition revisited: the codeHow to choose a neural network's hyper-parameters?Other techniquesA visual proof that neural nets can compute any functionTwo caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusionWhy are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 Warm up: a fast matrix-based approach to computing the output  from a neural networkThe two assumptions we need about the cost functionThe Hadamard product, $s \odot t$The four fundamental equations behind backpropagationProof of the four fundamental equations (optional)The backpropagation algorithmThe code for backpropagationIn what sense is backpropagation a fast algorithm?Backpropagation: the big picture Improving the way neural networks learnThe cross-entropy cost functionOverfitting and regularizationWeight initializationHandwriting recognition revisited: the codeHow to choose a neural network's hyper-parameters?Other techniquesA visual proof that neural nets can compute any functionTwo caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusionWhy are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 The cross-entropy cost functionOverfitting and regularizationWeight initializationHandwriting recognition revisited: the codeHow to choose a neural network's hyper-parameters?Other techniques A visual proof that neural nets can compute any functionTwo caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusionWhy are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 Two caveatsUniversality with one input and one outputMany input variablesExtension beyond sigmoid neuronsFixing up the step functionsConclusion Why are deep neural networks hard to train?The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learningDeep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 The vanishing gradient problemWhat's causing the vanishing gradient problem?  Unstable gradients in deep neural netsUnstable gradients in more complex networksOther obstacles to deep learning Deep learningIntroducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networksAppendix: Is there asimplealgorithm for intelligence?AcknowledgementsFrequently Asked QuestionsIf you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount.Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAxSponsorsDeep Learning Workstations, Servers, and LaptopsThanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame.ResourcesMichael Nielsen on TwitterBook FAQCode repositoryMichael Nielsen's project announcement mailing listDeep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courvillecognitivemedium.comByMichael Nielsen/ Dec 2019 Introducing convolutional networksConvolutional neural networks in practiceThe code for our convolutional networksRecent progress in image recognitionOther approaches to deep neural netsOn the future of neural networks Appendix: Is there asimplealgorithm for intelligence? Acknowledgements Frequently Asked Questions If you benefit from the book, please make a small
donation.  I suggest $5, but you can choose the amount. Alternately, you can make a donation by sending me
Bitcoin, at address1Kd6tXH5SDAmiFb49J9hknG5pqj7KStSAx Thanks to all thesupporterswho made the book possible, with
especial thanks to Pavel Dudrenov.  Thanks also to all the
contributors to theBugfinder Hall of
Fame. Michael Nielsen on Twitter Book FAQ Code repository Michael Nielsen's project announcement mailing list Deep Learning, book by Ian
Goodfellow, Yoshua Bengio, and Aaron Courville cognitivemedium.com ByMichael Nielsen/ Dec 2019 Neural Networks and Deep Learningis a free online book.  The
book will teach you about:Neural networks, a beautiful biologically-inspired programming
paradigm which enables a computer to learn from observational dataDeep learning, a powerful set of techniques for learning in neural
networksNeural networks and deep learning currently provide the best solutions
to many problems in image recognition, speech recognition, and natural
language processing.  This book will teach you many of the core
concepts behind neural networks and deep learning. For more details about the approach taken in the
book,see here.  Or you can jump directly
toChapter 1and get started. "
https://wiki.pathmind.com/neural-network,"Contents Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated. Neural networks help us cluster and classify. You can think of them as a clustering and classification layer on top of the data you store and manage. They help to group unlabeled data according to similarities among the example inputs, and they classify data when they have alabeled dataset to train on. (Neural networks can also extract features that are fed to other algorithms for clustering and classification; so you can think of deep neural networks as components of larger machine-learning applications involving algorithms forreinforcement learning, classification andregression.) Artificial neural networks are the foundation of large-language models (LLMs) used bychatGPT,Microsoftâ€™s Bing, Googleâ€™s Bard andMetaâ€™s Llama, among others. What kind of problems does deep learning solve, and more importantly, can it solve yours? To know the answer, you need to ask a few questions: What outcomes do I care about?In a classification problem, those outcomes are labels that could be applied to data: for example,spamornot_spamin an email filter,good_guyorbad_guyin fraud detection,angry_customerorhappy_customerin customer relationship management. Other types of problems include anomaly detection (useful in fraud detection and predictive maintenance of manufacturing equipment), and clustering, which is useful in recommendation systems that surface similarities. Do I have the right data?For example, if you have a classification problem, youâ€™ll need labeled data. Is the dataset you need publicly available, or can you create it (with a data annotation service likeScaleor AWS Mechanical Turk)? In this example, spam emails would be labeled as spam, and the labels would enable the algorithm to map from inputs to the classifications you care about. You canâ€™t know that you have the right data until you get your hands on it. If you are a data scientist working on a problem, you canâ€™t trust anyone to tell you whether the data is good enough. Only direct exploration of the data will answer this question. Find Out How Page One Can Support You Deep learning maps inputs to outputs. It finds correlations. It is known as a â€œuniversal approximatorâ€, because it can learn to approximate an unknown functionf(x) = ybetween any inputxand any outputy, assuming they are related at all (by correlation or causation, for example). In the process of learning, a neural network finds the rightf, or the correct manner of transformingxintoy, whether that bef(x) = 3x + 12orf(x) = 9x - 0.1. Here are a few examples of what deep learning can do. All classification tasks depend upon labeled datasets; that is, humans must transfer their knowledge to the dataset in order for a neural network to learn the correlation between labels and data. This is known assupervised learning. Any labels that humans can generate, any outcomes that you care about and which correlate to data, can be used to train a neural network. Clustering or grouping is the detection of similarities. Deep learning does not require labels to detect similarities. Learning without labels is calledunsupervised learning. Unlabeled data is the majority of data in the world. One law of machine learning is: the more data an algorithm can train on, the more accurate it will be. Therefore, unsupervised learning has the potential to produce highly accurate models. With classification, deep learning is able to establish correlations between, say, pixels in an image and the name of a person. You might call this a static prediction. By the same token, exposed to enough of the right data, deep learning is able to establish correlations between present events and future events. It can run regression between the past and the future. The future event is like the label in a sense. Deep learning doesnâ€™t necessarily care about time, or the fact that something hasnâ€™t happened yet. Given a time series, deep learning may read a string of number and predict the number most likely to occur next. The better we can predict, the better we can prevent and pre-empt. As you can see, with neural networks, weâ€™re moving towards a world of fewer surprises. Not zero surprises, just marginally fewer. Weâ€™re also moving toward a world of smarter agents that combine neural networks with other algorithms likereinforcement learningto attain goals. With that brief overview of deep learning use cases, letâ€™s look at what neural nets are made of. Deep learning is the name we use for â€œstacked neural networksâ€; that is, networks composed of several layers. The layers are made ofnodes. A node is just a place where computation happens, loosely patterned on a neuron in the human brain, which fires when it encounters sufficient stimuli. A node combines input from the data with a set of coefficients, or weights, that either amplify or dampen that input, thereby assigning significance to inputs with regard to the task the algorithm is trying to learn; e.g. which input is most helpful is classifying data without error? These input-weight products are summed and then the sum is passed through a nodeâ€™s so-called activation function, to determine whether and to what extent that signal should progress further through the network to affect the ultimate outcome, say, an act of classification. If the signals passes through, the neuron has been â€œactivated.â€ Hereâ€™s a diagram of what one node might look like.  A node layer is a row of those neuron-like switches that turn on or off as the input is fed through the net. Each layerâ€™s output is simultaneously the subsequent layerâ€™s input, starting from an initial input layer receiving your data.  Pairing the modelâ€™s adjustable weights with input features is how we assign significance to those features with regard to how the neural network classifies and clusters input. Deep-learning networks are distinguished from the more commonplace single-hidden-layer neural networks by theirdepth; that is, the number of node layers through which data must pass in a multistep process of pattern recognition. Earlier versions of neural networks such as the firstperceptronswere shallow, composed of one input and one output layer, and at most one hidden layer in between. More than three layers (including input and output) qualifies as â€œdeepâ€ learning. Sodeepis not just a buzzword to make algorithms seem like they read Sartre and listen to bands you havenâ€™t heard of yet. It is a strictly defined term that means more than one hidden layer. In deep-learning networks, each layer of nodes trains on a distinct set of features based on the previous layerâ€™s output. The further you advance into the neural net, the more complex the features your nodes can recognize, since they aggregate and recombine features from the previous layer.  This is known asfeature hierarchy, and it is a hierarchy of increasing complexity and abstraction. It makes deep-learning networks capable of handling very large, high-dimensional data sets with billions of parameters that pass throughnonlinear functions. Above all, these neural nets are capable of discovering latent structures withinunlabeled, unstructured data, which is the vast majority of data in the world. Another word for unstructured data israw media; i.e. pictures, texts, video and audio recordings. Therefore, one of the problems deep learning solves best is in processing and clustering the worldâ€™s raw, unlabeled media, discerning similarities and anomalies in data that no human has organized in a relational database or ever put a name to. For example, deep learning can take a million images, and cluster them according to their similarities: cats in one corner, ice breakers in another, and in a third all the photos of your grandmother. This is the basis of so-called smart photo albums. Now apply that same idea to other data types: Deep learning might cluster raw text such as emails or news articles. Emails full of angry complaints might cluster in one corner of the vector space, while satisfied customers, or spambot messages, might cluster in others. This is the basis of various messaging filters, and can be used in customer-relationship management (CRM). The same applies to voice messages. With time series, data might cluster around normal/healthy behavior and anomalous/dangerous behavior. If the time series data is being generated by a smart phone, it will provide insight into usersâ€™ health and habits; if it is being generated by an autopart, it might be used to prevent catastrophic breakdowns. Deep-learning networks performautomatic feature extractionwithout human intervention, unlike most traditional machine-learning algorithms. Given that feature extraction is a task that can take teams of data scientists years to accomplish, deep learning is a way to circumvent the chokepoint of limited experts. It augments the powers of small data science teams, which by their nature do not scale. When training on unlabeled data, each node layer in a deep network learns features automatically by repeatedly trying to reconstruct the input from which it draws its samples, attempting to minimize the difference between the networkâ€™s guesses and the probability distribution of the input data itself. Restricted Boltzmann machines, for examples, create so-called reconstructions in this manner. In the process, these neural networks learn to recognize correlations between certain relevant features and optimal results â€“ they draw connections between feature signals and what those features represent, whether it be a full reconstruction, or with labeled data. A deep-learning network trained on labeled data can then be applied to unstructured data, giving it access to much more input than machine-learning nets. This is a recipe for higher performance: the more data a net can train on, the more accurate it is likely to be. (Bad algorithms trained on lots of data can outperform good algorithms trained on very little.) Deep learningâ€™s ability to process and learn from huge quantities of unlabeled data give it a distinct advantage over previous algorithms. Deep-learning networks end in an output layer: a logistic, or softmax, classifier that assigns a likelihood to a particular outcome or label. We call that predictive, but it is predictive in a broad sense. Given raw data in the form of an image, a deep-learning network may decide, for example, that the input data is 90 percent likely to represent a person. Our goal in using a neural net is to arrive at the point of least error as fast as possible. We are running a race, and the race is around a track, so we pass the same points repeatedly in a loop. The starting line for the race is the state in which our weights are initialized, and the finish line is the state of those parameters when they are capable of producing sufficiently accurate classifications and predictions. The race itself involves many steps, and each of those steps resembles the steps before and after. Just like a runner, we will engage in a repetitive act over and over to arrive at the finish. Each step for a neural network involves a guess, an error measurement and a slight update in its weights, an incremental adjustment to the coefficients, as it slowly learnsto pay attentionto the most important features. A collection of weights, whether they are in their start or end state, is also called a model, because it is an attempt to model dataâ€™s relationship to ground-truth labels, to grasp the dataâ€™s structure. Models normally start out bad and end up less bad, changing over time as the neural network updates its parameters. This is because a neural network is born in ignorance. It does not know which weights and biases will translate the input best to make the correct guesses. It has to start out with a guess, and then try to make better guesses sequentially as it learns from its mistakes. (You can think of a neural network as a miniature enactment of the scientific method, testing hypotheses and trying again â€“ only it is the scientific method with a blindfold on. Or like a child: they are born not knowing much, and through exposure to life experience, they slowly learn to solve problems in the world. For neural networks, data is the only experience.) Here is a simple explanation of what happens during learning with a feedforward neural network, the simplest architecture to explain. Input enters the network. The coefficients, or weights, map that input to a set of guesses the network makes at the end. Weighted input results in a guess about what that input is. The neural then takes its guess and compares it to a ground-truth about the data, effectively asking an expert â€œDid I get this right?â€ The difference between the networkâ€™s guess and the ground truth is itserror. The network measures that error, and walks the error back over its model, adjusting weights to the extent that they contributed to the error. The three pseudo-mathematical formulas above account for the three key functions of neural networks: scoring input, calculating loss and applying an update to the model â€“ to begin the three-step process over again. A neural network is a corrective feedback loop, rewarding weights that support its correct guesses, and punishing weights that lead it to err. Letâ€™s linger on the first step above. Despite their biologically inspired name, artificial neural networks are nothing more than math and code, like any other machine-learning algorithm. In fact, anyone who understandslinear regression, one of first methods you learn in statistics, can understand how a neural net works. In its simplest form, linear regression is expressed as whereY_hatis the estimated output,Xis the input,bis the slope andais the intercept of a line on the vertical axis of a two-dimensional graph. (To make this more concrete:Xcould be radiation exposure andYcould be the cancer risk;Xcould be daily pushups andY_hatcould be the total weight you can benchpress;Xthe amount of fertilizer andY_hatthe size of the crop.) You can imagine that every time you add a unit toX, the dependent variableY_hatincreases proportionally, no matter how far along you are on the X axis. That simple relation between two variables moving up or down together is a starting point. The next step is to imagine multiple linear regression, where you have many input variables producing an output variable. Itâ€™s typically expressed like this: (To extend the crop example above, you might add the amount of sunlight and rainfall in a growing season to the fertilizer variable, with all three affectingY_hat.) Now, that form of multiple linear regression is happening at every node of a neural network. For each node of a single layer, input from each node of the previous layer is recombined with input from every other node. That is, the inputs are mixed in different proportions, according to their coefficients, which are different leading into each node of the subsequent layer. In this way, a net tests which combination of input is significant as it tries to reduce error. Once you sum your node inputs to arrive atY_hat, itâ€™s passed through a non-linear function. Hereâ€™s why: If every node merely performed multiple linear regression,Y_hatwould increase linearly and without limit as the Xâ€™s increase, but that doesnâ€™t suit our purposes. What we are trying to build at each node is a switch (like a neuronâ€¦) that turns on and off, depending on whether or not it should let the signal of the input pass through to affect the ultimate decisions of the network. When you have a switch, you have a classification problem. Does the inputâ€™s signal indicate the node should classify it as enough, or not_enough, on or off? A binary decision can be expressed by 1 and 0, andlogistic regressionis a non-linear function that squashes input to translate it to a space between 0 and 1. The nonlinear transforms at each node are usually s-shaped functions similar to logistic regression. They go by the names of sigmoid (the Greek word for â€œSâ€), tanh, hard tanh, etc., and they shaping the output of each node. The output of all nodes, each squashed into an s-shaped space between 0 and 1, is then passed as input to the next layer in a feed forward neural network, and so on until the signal reaches the final layer of the net, where decisions are made. The name for one commonly used optimization function that adjusts weights according to the error they caused is called â€œgradient descent.â€ Gradient is another word for slope, and slope, in its typical form on an x-y graph, represents how two variables relate to each other: rise over run, the change in money over the change in time, etc. In this particular case, the slope we care about describes the relationship between the networkâ€™s error and a single weight; i.e. that is, how does the error vary as the weight is adjusted. To put a finer point on it, which weight will produce the least error? Which one correctly represents the signals contained in the input data, and translates them to a correct classification? Which one can hear â€œnoseâ€ in an input image, and know that should be labeled as a face and not a frying pan? As a neural network learns, it slowly adjusts many weights so that they can map signal to meaning correctly. The relationship between networkErrorand each of thoseweightsis a derivative,dE/dw, that measures the degree to which a slight change in a weight causes a slight change in the error. Each weight is just one factor in a deep network that involves many transforms; the signal of the weight passes through activations and sums over several layers, so we use thechain rule of calculusto march back through the networks activations and outputs and finally arrive at the weight in question, and its relationship to overall error. The chain rule in calculus states that  In a feedforward network, the relationship between the netâ€™s error and a single weight will look something like this:  That is, given two variables,Errorandweight, that are mediated by a third variable,activation, through which the weight is passed, you can calculate how a change inweightaffects a change inErrorby first calculating how a change inactivationaffects a change inError, and how a change inweightaffects a change inactivation. The essence of learning in deep learning is nothing more than that: adjusting a modelâ€™s weights in response to the error it produces, until you canâ€™t reduce the error any more. On a deep neural network of many layers, the final layer has a particular role. When dealing with labeled input, the output layer classifies each example, applying the most likely label. Each node on the output layer represents one label, and that node turns on or off according to the strength of the signal it receives from the previous layerâ€™s input and parameters. Each output node produces two possible outcomes, the binary output values 0 or 1, becausean input variable either deserves a label or it does not. After all, there is no such thing as a little pregnant. While neural networks working with labeled data produce binary output, the input they receive is often continuous. That is, the signals that the network receives as input will span a range of values and include any number of metrics, depending on the problem it seeks to solve. For example, a recommendation engine has to make a binary decision about whether to serve an ad or not. But the input it bases its decision on could include how much a customer has spent on Amazon in the last week, or how often that customer visits the site. So the output layer has to condense signals such as $67.59 spent on diapers, and 15 visits to a website, into a range between 0 and 1; i.e. a probability that a given input should be labeled or not. The mechanism we use to convert continuous signals into binary output is calledlogistic regression. The name is unfortunate, since logistic regression is used for classification rather than regression in the linear sense that most people are familiar with. It calculates the probability that a set of inputs match the label.  Letâ€™s examine this little formula. For continuous inputs to be expressed as probabilities, they must output positive results, since there is no such thing as a negative probability. Thatâ€™s why you see input as the exponent ofein the denominator â€“ because exponents force our results to be greater than zero. Now consider the relationship ofeâ€™s exponent to the fraction 1/1. One, as we know, is the ceiling of a probability, beyond which our results canâ€™t go without being absurd. (Weâ€™re 120% sure of that.) As the inputxthat triggers a label grows, the expressione to the xshrinks toward zero, leaving us with the fraction 1/1, or 100%, which means we approach (without ever quite reaching) absolute certainty that the label applies. Input that correlates negatively with your output will have its value flipped by the negative sign oneâ€™s exponent, and as that negative signal grows, the quantitye to the xbecomes larger, pushing the entire fraction ever closer to zero. Now imagine that, rather than havingxas the exponent, you have the sum of the products of all the weights and their corresponding inputs â€“ the total signal passing through your net. Thatâ€™s what youâ€™re feeding into the logistic regression layer at the output layer of a neural network classifier. With this layer, we can set a decision threshold above which an example is labeled 1, and below which it is not. You can set different thresholds as you prefer â€“ a low threshold will increase the number of false positives, and a higher one will increase the number of false negatives â€“ depending on which side you would like to err. In some circles, neural networks are synonymous withAI. In others, they are thought of as a â€œbrute forceâ€ technique, characterized by alackof intelligence, because they start with a blank slate, and they hammer their way through to an accurate model. By this interpretation,neural networks are effective, but inefficient in their approach to modeling, since they donâ€™t make assumptions about functional dependencies between output and input. For what itâ€™s worth, the foremost AI research groups are pushing the edge of the discipline by training larger and larger neural networks. Brute force works. It is a necessary, if not sufficient, condition to AI breakthroughs. OpenAIâ€™s pursuit of more general AI emphasizes a brute force approach, which has proven effective with well-known models such as GPT-3. Algorithms such as Hintonâ€™s capsule networks require far fewer instances of data to converge on an accurate model; that is, present research has the potential to resolve the brute force inefficiencies of deep learning. While neural networks are useful as a function approximator, mapping inputs to outputs in many tasks of perception, to achieve a more general intelligence, they can be combined with other AI methods to perform more complex tasks. For example,deep reinforcement learningembeds neural networks within a reinforcement learning framework, where they map actions to rewards in order to achieve goals. Deepmindâ€™s victories in video games and the board game of go are good examples. Chris V. Nicholson is a venture partner atPage One Ventures. He previously led Pathmind and Skymind. In a prior life, Chris spent a decade reporting on tech and finance for The New York Times, Businessweek and Bloomberg, among others. Copyright Â© 2023. Pathmind Inc.All rights reserved"
https://www.analyticsvidhya.com/blog/2022/01/introduction-to-neural-networks/,"Mastering Pythonâ€™s Set Difference: A Game-Changer for Data Wrangling Neural network is the fusion of artificial intelligence and brain-inspired design that reshapes modern computing. With intricate layers of interconnected artificial neurons, these networks emulate the intricate workings of the human brain, enabling remarkable feats in machine learning. There are different types of neural networks, from feedforward to recurrent and convolutional, each tailored for specific tasks. This article covers its real-world applications across industries like image recognition, natural language processing, and more. Read on to know everything about neural network in machine learning!These Learning algorithms will help you for optimization these adaptive networks while it is computer science or having different deep learning algorithms. This article was published as a part of theData Science Blogathon. Neural networks mimic the basicfunctioningof the human brain and draw inspiration from how the brain interprets information. They solve various real-time tasks due to their ability to perform computations quickly and respond rapidly. Artificial Neural Network has a huge number of interconnected processing elements, also known as Nodes. These nodes are connected with other nodes using a connection link. The connection link contains weights, these weights contain the information about the input signal. Each iteration and input in turn leads to updation of these weights. After inputting all the data instances from the training dataset, the final weights of the neural network, along with its architecture, form the trained neural network. This process is called training neural networks. These trained neural networks solve specific problems defined in the problem statement. Artificial neural networks can solve tasks such as classification problems, pattern matching, and data clustering. We useartificial neural networksbecause they learn very efficiently and adaptively. They have the capability to learn â€œhowâ€ to solve a specific problem from the training data it receives. After learning, the model can solve that specific problem very quickly and efficiently with high accuracy. Some real-life applications of neural networks include Air Traffic Control, Optical Character Recognition as used by some scanning apps like Google Lens, Voice Recognition, etc. Neural networks find applications across various domains for: Explore different kinds of neural networks in machine learning in this section: ANNalso goes by the name of artificial neural network. It functions as a feed-forward neural network because the inputs move in the forward direction. It can also contain hidden layers which can make the model even denser. They have a fixed length as specified by the programmer. It is used for Textual Data or Tabular Data. A widely used real-life application is Facial Recognition. It is comparatively less powerful than CNN and RNN. CNNsis mainly used for Image Data. It is used for Computer Vision. Some of the real-life applications are object detection in autonomous vehicles. It contains a combination of convolutional layers and neurons. It is more powerful than both ANN and RNN. It is also known asRNNs. It is used to process and interpret time series data. In this type of model, the output from a processing node is fed back into nodes in the same or previous layers. The most known types of RNN areLSTM(Long Short Term Memory) Networks Now that we know the basics about Neural Networks, We know that Neural Networksâ€™ learning capability is what makes it interesting. As the name suggestsSupervised Learning, it is a type of learning that is looked after by a supervisor. It is like learning with a teacher. You input all the data instances from the training dataset, and the final weights of the neural network, along with its architecture, define the trained neural network.Â This process involves training neural networks. These trained neural networks solve specific problems defined in the problem statement. In this, there is feedback from the environment to the model. Unlike supervised learning, there is nosupervisoror a teacher here. In this type of learning, there is no feedback from the environment, there is no desired output and the model learns on its own. During the training phase, you form the inputs into classes that define the similarity of the members. Each class contains similar input patterns. On inputting a new pattern, it can predict to which class that input belongs based on similarity with other patterns. If there is no such class, a new class is formed. It gets the best of both worlds, that is, the best of both Supervised learning and Unsupervised learning. It is likelearningwith a critique. Here there is no exact feedback from the environment, rather there is critique feedback. The critique tells how close our solution is. Hence the model learns on its own based on the critique information. It is similar to supervised learning in that it receives feedback from the environment, but it is different in that it does not receive the desired output information, rather it receives critique information. A Convolutional Neural Network(CNN)is a type of artificial intelligence especially good at processing images and videos. They draw inspiration from the structure of the human visual cortex. You can use CNNs in many applications, including image recognition, facial recognition, and medical imaging analysis. They are able to automatically extract features from images, which makes them very powerful tools. Here are some key points about CNNs, incorporating your keywords naturally: According to Arthur Samuel, one of the early American pioneers in the field of computer gaming and artificial intelligence, he definedmachine learningas: Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not delve into the details of such a procedure to see that it could become entirely automatic and that a machine programmed this way would â€œlearnâ€ from its experience. we can think of an artificial neuron as a simple or multiple linear regression model with an activation function at the end. A neuron from layer i will take the output of all the neurons from the later i-1 as inputs calculate the weighted sum and add bias to it. After this is sent to an activation function as we saw in the previous diagram. The first neuron in the first layer connects to all the inputs from the previous layer. Similarly, the second neuron in the first hidden layer also connects to all the inputs from the previous layer, and this pattern continues for all neurons in the first hidden layer. You consider the outputs of the previously hidden layer as inputs for the neurons in the second hidden layer, and each of these neurons connects to the previous neurons. This whole process is calledforward propagation. After this, there is an interesting thing that happens. Once we have predicted the output it is then compared to the actual output. We then calculate the loss and try to minimize it. But how can we minimize this loss? For this, there comes another concept which is known as Back Propagation. We will understand more about this in another article. I will tell you how it works. First, you calculate the loss, then you adjust the weights and biases to minimize the loss. You update the weights and biases using another algorithm called gradient descent. We will understand more about gradient descent in a later section. We basically move in the direction opposite to the gradient. This concept is derived from the Taylor series. Hereâ€™s a comparison of Machine Learning and Deep Learning in the context of neural networks: Neural networks anddeep learningare related but distinct concepts in the field of machine learning and artificial intelligence. Itâ€™s important to understand the differences between the two. A neural network is a computational model inspired by the structure and function of biologicalneural networksin the human brain. It consists of interconnected nodes, called artificial neurons, that transmit signals between each other. The connections have numeric weights that you can tune, allowing the neural network to learn and model complex patterns in data. Neural networks can be shallow, with only one hidden layer between the input and output layers, or they can have multiple hidden layers, making them â€œdeepâ€ neural networks. Even shallow neural networks are capable of modeling non-linear data and learning complex relationships. It is a subfield of machine learning that utilizes deep neural networks with multiple hidden layers. Deep neural networks can automatically learn hierarchies of features directly from data, without requiring manual feature engineering. The depth of the neural network, with many layers of increasing complexity, allows the model to learn rich representations of raw data. This depth helps deep learning models discover intricate structure in high-dimensional data, making them very effective for tasks like image recognition, natural language processing, and audio analysis. While all deep learning models are NNs, not all NN are deep learning models. The main distinction is the depth of the model: It provide a general framework for machine learning models inspired by the brain, while deep learning leverages the power of deep NN to tackle complex problems with raw, high-dimensional data. Deep learning has achieved remarkable success in many AI applications, but shallow NN still have their uses, especially for less complex tasks or when interpretability is important. Neural networks have enabled amazing achievements in a variety of industries and transformed modern computing. They perform complicated tasks like image recognition, natural language processing, and predictive analytics with unmatched accuracy thanks to their brain-inspired architecture and capacity to learn from data.Neural networksprovide an effective toolkit for realizing the enormous promise of artificial intelligence, whether it is through shallow networks modeling basic patterns or deep learning models automatically extracting hierarchical characteristics. Neural networks will continue to push the envelope as research develops, fostering innovation in industries ranging from finance to healthcare and influencing how we think about intelligent systems. Discover the intriguing realm of neural networks and break through to new machine learning frontiers. In this article you get a clear understanding of neural network and convoultional neural networks , these networks forecasting their input nodes their learning process.An Single layer of these neural nets provides logistic , walter pitts and at the end provide output node. While these feedback loops provides feedforward network to neural network architecture of biological neurons. Join our course on â€˜Neural Networksâ€˜ and revolutionize your understanding of AI. Master the techniques driving breakthroughs in image recognition, NLP, and predictive analytics. Enroll today and lead the future of innovation in fields like finance and healthcare! Did you find this article helpful? Please share your opinions/thoughts in the comments section below. The media shown in this article does not belong to Analytics Vidhya and the author uses it at their discretion. A. Neural networks are a subset of artificial intelligence (AI) that mimic the structure and function of the human brain to recognize patterns and make decisions.AI, on the other hand, is a broader field encompassing various techniques and technologies aimed at creating systems that can perform tasks requiring human-like intelligence. A.Â Yes, ChatGPT is a neural network-based model developed by OpenAI. It uses a variant of the Transformer architecture, specifically the GPT (Generative Pre-trained Transformer) architecture, for natural language processing tasks like text generation and understanding. A.Â A neural network serves as a computational model inspired by the structure and function of the human brain, consisting of interconnected nodes (neurons) organized in layers. Convolutional Neural Networks (CNNs) represent a type of neural network specifically designed to process structured grid-like data, such as images. They use convolutional layers to automatically and adaptively learn spatial hierarchies of features from the input data. A. You can implement neural networks in Python using various libraries and frameworks such as TensorFlow, Keras, PyTorch, and scikit-learn. These libraries provide high-level APIs and tools for building, training, and deployingNNs models efficiently. I am an undergraduate student at Netaji Subhash University of Technology ( Formerly Netaji Subhash Institute of Technology ) pursuing Bachelors of Technology in Information Technology. I am extremely passionate about coding and robotics. I am participating in various projects. Explore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics. Master Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple. This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data. Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications. Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course. Analyzing Types of Neural Networks in Deep Lear... Introduction to Convolutional Neural Networks (... Introduction to Artificial Neural Networks Neural Network 101: Definition, Types and Appli... Artificial Neural Networks â€“ Better Under... An Overview and Applications of Artificial Neur... Difference Between ANN, CNN and RNN Machine Learning vs Neural Networks: What is th... FeedForward Neural Networks: Layers, Functions,... An Introductory Guide to Deep Learning and Neur... ClearSubmit reply  Î” Very Very effective for beginners and great effort ClearSubmit reply  Î” Write, captivate, and earn accolades and rewards for your work We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in ourPrivacy Policy&Cookies Policy. Show details This site uses cookies to ensure that you get the best experience possible. To learn more about how we use cookies, please refer to ourPrivacy Policy&Cookies Policy. Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. It is needed for personalizing the website. Expiry: Session Type: HTTP This cookie is used to prevent Cross-site request forgery (often abbreviated as CSRF) attacks of the website Expiry: Session Type: HTTPS Preserves the login/logout state of users across the whole site. Expiry: Session Type: HTTPS Preserves users' states across page requests. Expiry: Session Type: HTTPS Google One-Tap login adds this g_state cookie to set the user status on how they interact with the One-Tap modal. Expiry: 365 days Type: HTTP Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously. Used by Microsoft Clarity, to store and track visits across websites. Expiry: 1 Year Type: HTTP Used by Microsoft Clarity, Persists the Clarity User ID and preferences, unique to that site, on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID. Expiry: 1 Year Type: HTTP Used by Microsoft Clarity, Connects multiple page views by a user into a single Clarity session recording. Expiry: 1 Day Type: HTTP Collects user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior. Expiry: 2 Years Type: HTTP Use to measure the use of the website for internal analytics Expiry: 1 Years Type: HTTP The cookie is set by embedded Microsoft Clarity scripts. The purpose of this cookie is for heatmap and session recording. Expiry: 1 Year Type: HTTP Collected user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior. Expiry: 2 Months Type: HTTP This cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the website is doing. The data collected includes the number of visitors, the source where they have come from, and the pages visited in an anonymous form. Expiry: 399 Days Type: HTTP Used by Google Analytics, to store and count pageviews. Expiry: 399 Days Type: HTTP Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. Expiry: 1 Day Type: HTTP Used to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels. Expiry: Session Type: PIXEL cookies ensure that requests within a browsing session are made by the user, and not by other sites. Expiry: 6 Months Type: HTTP use the cookie when customers want to make a referral from their gmail contacts; it helps auth the gmail account. Expiry: 2 Years Type: HTTP This cookie is set by DoubleClick (which is owned by Google) to determine if the website visitor's browser supports cookies. Expiry: 1 Year Type: HTTP this is used to send push notification using webengage. Expiry: 1 Year Type: HTTP used by webenage to track auth of webenagage. Expiry: Session Type: HTTP Linkedin sets this cookie to registers statistical data on users' behavior on the website for internal analytics. Expiry: 1 Day Type: HTTP Use to maintain an anonymous user session by the server. Expiry: 1 Year Type: HTTP Used as part of the LinkedIn Remember Me feature and is set when a user clicks Remember Me on the device to make it easier for him or her to sign in to that device. Expiry: 1 Year Type: HTTP Used to store information about the time a sync with the lms_analytics cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP Used to store information about the time a sync with the AnalyticsSyncHistory cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP Cookie used for Sign-in with Linkedin and/or to allow for the Linkedin follow feature. Expiry: 6 Months Type: HTTP allow for the Linkedin follow feature. Expiry: 1 Year Type: HTTP often used to identify you, including your name, interests, and previous activity. Expiry: 2 Months Type: HTTP Tracks the time that the previous page took to load Expiry: Session Type: HTTP Used to remember a user's language setting to ensure LinkedIn.com displays in the language selected by the user in their settings Expiry: Session Type: HTTP Tracks percent of page viewed Expiry: Session Type: HTTP Indicates the start of a session for Adobe Experience Cloud Expiry: Session Type: HTTP Provides page name value (URL) for use by Adobe Analytics Expiry: Session Type: HTTP Used to retain and fetch time since last visit in Adobe Analytics Expiry: 6 Months Type: HTTP Remembers a user's display preference/theme setting Expiry: 6 Months Type: HTTP Remembers which users have updated their display / theme preferences Expiry: 6 Months Type: HTTP Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in. Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers. Used by Google Adsense, to store and track conversions. Expiry: 3 Months Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP These cookies are used for the purpose of targeted advertising. Expiry: 6 Hours Type: HTTP These cookies are used for the purpose of targeted advertising. Expiry: 1 Month Type: HTTP These cookies are used to gather website statistics, and track conversion rates. Expiry: 1 Month Type: HTTP Aggregate analysis of website visitors Expiry: 6 Months Type: HTTP This cookie is set by Facebook to deliver advertisements when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website. Expiry: 4 Months Type: HTTP Contains a unique browser and user ID, used for targeted advertising. Expiry: 2 Months Type: HTTP Used by LinkedIn to track the use of embedded services. Expiry: 1 Year Type: HTTP Used by LinkedIn for tracking the use of embedded services. Expiry: 1 Day Type: HTTP Used by LinkedIn to track the use of embedded services. Expiry: 6 Months Type: HTTP Use these cookies to assign a unique ID when users visit a website. Expiry: 6 Months Type: HTTP These cookies are set by LinkedIn for advertising purposes, including: tracking visitors so that more relevant ads can be presented, allowing users to use the 'Apply with LinkedIn' or the 'Sign-in with LinkedIn' functions, collecting information about how visitors use the site, etc. Expiry: 6 Months Type: HTTP Used to make a probabilistic match of a user's identity outside the Designated Countries Expiry: 90 Days Type: HTTP Used to collect information for analytics purposes. Expiry: 1 year Type: HTTP Used to store session ID for a users session to ensure that clicks from adverts on the Bing search engine are verified for reporting purposes and for personalisation Expiry: 1 Day Type: HTTP UnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies. Cookie declaration last updated on 24/03/2023 by Analytics Vidhya. Cookies are small text files that can be used by websites to make a user's experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies, we need your permission. This site uses different types of cookies. Some cookies are placed by third-party services that appear on our pages. Learn more about who we are, how you can contact us, and how we process personal data in ourPrivacy Policy. Terms & conditionsRefund PolicyPrivacy PolicyCookies PolicyÂ© Analytics Vidhya 2024.All rights reserved.  Edit Resend OTP Resend OTP in45s"
https://www.tutorialspoint.com/artificial_intelligence/artificial_intelligence_neural_networks.htm,"Yet another research area in AI, neural networks, is inspired from the natural neural network of human nervous system. The inventor of the first neurocomputer, Dr. Robert Hecht-Nielsen, defines a neural network as âˆ’ The idea of ANNs is based on the belief that working of human brain by making the right connections, can be imitated using silicon and wires as livingneuronsanddendrites. The human brain is composed of 86 billion nerve cells calledneurons.They are connected to other thousand cells byAxons.Stimuli from external environment or inputs from sensory organs are accepted by dendrites. These inputs create electric impulses, which quickly travel through the neural network. A neuron can then send the message to other neuron to handle the issue or does not send it forward. ANNs are composed of multiplenodes, which imitate biologicalneuronsof human brain. The neurons are connected by links and they interact with each other. The nodes can take input data and perform simple operations on the data. The result of these operations is passed to other neurons. The output at each node is called itsactivationornode value. Each link is associated withweight.ANNs are capable of learning, which takes place by altering weight values. The following illustration shows a simple ANN âˆ’ There are two Artificial Neural Network topologies âˆ’FeedForwardandFeedback. In this ANN, the information flow is unidirectional. A unit sends information to other unit from which it does not receive any information. There are no feedback loops. They are used in
pattern generation/recognition/classification. They have fixed inputs and outputs. Here, feedback loops are allowed. They are used in content addressable memories. In the topology diagrams shown, each arrow represents a connection between two neurons and indicates the pathway for the flow of information. Each connection has a weight, an integer number that controls the signal between the two neurons. If the network generates a â€œgood or desiredâ€ output, there is no need to adjust the weights. However, if the network generates a â€œpoor or undesiredâ€ output or an error, then the system alters the weights in order to improve subsequent results. ANNs are capable of learning and they need to be trained. There are several learning strategies âˆ’ Supervised Learningâˆ’ It involves a teacher that is scholar than the ANN itself. For example, the teacher feeds some example data about which the teacher already knows the answers. For example, pattern recognizing. The ANN comes up with guesses while recognizing. Then the teacher provides the ANN with the answers. The network then compares it guesses with the teacherâ€™s â€œcorrectâ€ answers and makes adjustments according to errors. Unsupervised Learningâˆ’ It is required when there is no example data set with known answers. For example, searching for a hidden pattern. In this case, clustering i.e. dividing a set of elements into groups according to some unknown pattern is carried out based on the existing data sets present. Reinforcement Learningâˆ’ This strategy built on observation. The ANN makes a decision by observing its environment. If the observation is negative, the network adjusts its weights to be able to make a different required decision the next time. It is the training or learning algorithm. It learns by example. If you submit to the algorithm the example of what you want the network to do, it changes the networkâ€™s weights so that it can produce desired output for a particular input on finishing the training. Back Propagation networks are ideal for simple Pattern Recognition and Mapping Tasks. These are the graphical structures used to represent the probabilistic relationship among a set of random variables. Bayesian networks are also calledBelief NetworksorBayes Nets.BNs reason about uncertain domain. In these networks, each node represents a random variable with specific propositions. For example, in a medical diagnosis domain, the node Cancer represents the proposition that a patient has cancer. The edges connecting the nodes represent probabilistic dependencies among those random variables. If out of two nodes, one is affecting the other then they must be directly connected in the directions of the effect. The strength of the relationship between variables is quantified by the probability associated with each node. There is an only constraint on the arcs in a BN that you cannot return to a node simply by following directed arcs. Hence the BNs are called Directed Acyclic Graphs (DAGs). BNs are capable of handling multivalued variables simultaneously. The BN variables are composed of two dimensions âˆ’ Consider a finite set X = {X1, X2, â€¦,Xn} of discrete random variables, where each variableXimay take values from a finite set, denoted byVal(Xi).If there is a directed link from variableXito variable,Xj,then variableXiwill be a parent of variableXjshowing direct dependencies between the variables. The structure of BN is ideal for combining prior knowledge and observed data. BN can be used to learn the causal relationships and understand various problem domains and to predict future events, even in case of missing data. A knowledge engineer can build a Bayesian network. There are a number of steps the knowledge engineer needs to take while building it. Example problemâˆ’Lung cancer.A patient has been suffering from breathlessness. He visits the doctor, suspecting he has lung cancer. The doctor knows that barring lung cancer, there are various other possible diseases the patient might have such as tuberculosis and bronchitis. Gather Relevant Information of Problem Identify Interesting Variables The knowledge engineer tries to answer the questions âˆ’ For now let us consider nodes, with only discrete values. The variable must take on exactly one of these values at a time. Common types of discrete nodes areâˆ’ Boolean nodesâˆ’ They represent propositions, taking binary values TRUE (T) and FALSE (F). Ordered valuesâˆ’ A nodePollutionmight represent and take values from {low, medium, high} describing degree of a patientâ€™s exposure to pollution. Integral valuesâˆ’ A node calledAgemight represent patientâ€™s age with possible values from 1 to 120. Even at this early stage, modeling choices are being made. Possible nodes and values for the lung cancer example âˆ’ Create Arcs between Nodes Topology of the network should capture qualitative relationships between variables. For example, what causes a patient to have lung cancer? - Pollution and smoking. Then add arcs from nodePollutionand nodeSmokerto nodeLung-Cancer. Similarly if patient has lung cancer, then X-ray result will be positive. Then add arcs from nodeLung-Cancerto nodeX-Ray. Specify Topology Conventionally, BNs are laid out so that the arcs point from top to bottom. The set of parent nodes of a node X is given by Parents(X). TheLung-Cancernode has two parents (reasons or causes):PollutionandSmoker, while nodeSmokeris anancestorof nodeX-Ray. Similarly,X-Rayis a child (consequence or effects) of nodeLung-Cancerandsuccessorof nodesSmokerandPollution. Conditional Probabilities Now quantify the relationships between connected nodes: this is done by specifying a conditional probability distribution for each node. As only discrete variables are considered here, this takes the form of aConditional Probability Table (CPT). First, for each node we need to look at all the possible combinations of values of those parent nodes. Each such combination is called aninstantiationof the parent set. For each distinct instantiation of parent node values, we need to specify the probability that the child will take. For example, theLung-Cancernodeâ€™s parents arePollutionandSmoking.They take the possible values = { (H,T), ( H,F), (L,T), (L,F)}. The CPT specifies the probability of cancer for each of these cases as <0.05, 0.02, 0.03, 0.001> respectively. Each node will have conditional probability associated as follows âˆ’ They can perform tasks that are easy for a human but difficult for a machine âˆ’ Aerospaceâˆ’ Autopilot aircrafts, aircraft fault detection. Automotiveâˆ’ Automobile guidance systems. Militaryâˆ’ Weapon orientation and steering, target tracking, object discrimination, facial recognition, signal/image identification. Electronicsâˆ’ Code sequence prediction, IC chip layout, chip failure analysis, machine vision, voice synthesis. Financialâˆ’ Real estate appraisal, loan advisor, mortgage screening, corporate bond rating, portfolio trading program, corporate financial analysis, currency value prediction, document readers, credit application evaluators. Industrialâˆ’ Manufacturing process control, product design and analysis, quality inspection systems, welding quality analysis, paper quality prediction,  chemical product design analysis, dynamic modeling of chemical process systems, machine maintenance analysis, project bidding, planning, and management. Medicalâˆ’ Cancer cell analysis, EEG and ECG analysis, prosthetic design, transplant time optimizer. Speechâˆ’ Speech recognition, speech classification, text to speech conversion. Telecommunicationsâˆ’ Image and data compression, automated information services, real-time spoken language translation. Transportationâˆ’ Truck Brake system diagnosis, vehicle scheduling, routing systems. Softwareâˆ’ Pattern Recognition in facial recognition, optical character recognition, etc. Time Series Predictionâˆ’ ANNs are used to make predictions on stocks and natural calamities. Signal Processingâˆ’ Neural networks can be trained to process an audio signal and filter it appropriately in the hearing aids. Controlâˆ’ ANNs are often used to make steering decisions of physical vehicles. Anomaly Detectionâˆ’ As ANNs are expert at recognizing patterns, they can also be trained to generate an output when something unusual occurs that misfits the pattern. Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical and non-technical subjects. Tutorials Point is a leading Ed Tech company striving to provide the best learning material on technical and non-technical subjects. Â© Copyright 2024. All Rights Reserved."
https://brilliant.org/courses/intro-neural-networks/,"Delve into the inner machinery of neural networks to discover how these flexible learning tools actually work. Teaching machines to teach themselves Neural Networks Think image recognition is easy? Try seeing in pixels. The Computer Vision Problem Why do we need neural networks? Some things just can't be programmed. The Folly of Computer Programming Do you have to be living to be learning? Can Computers Learn? Complete all lessons above to reach this milestone. 0 of 4 lessons complete Meet your first artificial neuron and learn how to encode simple logical operations. The Decision Box You can count on simple artificial neurons â€” literally. Activation Arithmetic Hone your intuition with this graphical model of a binary neuron. Decision Boundaries Escape the limitations of single neurons by stacking them in layers. Building an XOR Gate Sorting things into groups? The neuron knows best. Classification Real data isn't black and white â€” this neuron sees in shades of gray. Sigmoid Neurons Take a shot at building your first learning algorithm. Training a Single Neuron Complete all lessons above to reach this milestone. 0 of 7 lessons complete Got some complex data to classify? Try adding a hidden layer to your ANN. Hidden Layers Classifying isn't an ANN's only schtick. They are used to model lots of different data. Curve Fitting Don't think an ANN can model it? Think again â€” they're universal. Universal Approximator Learn how an ANN learns to see â€” and how you can trick it. A Shape-Recognizing Network Complete all lessons above to reach this milestone. 0 of 4 lessons complete Artificial neural networks learn by detecting patterns in huge amounts of information. Much like your own brain, artificial neural nets are flexible, data-processing machines that make predictions and decisions. In fact, the best ones outperform humans at tasks like chess and cancer diagnoses.

In this course, you'll dissect the internal machinery of artificial neural nets through hands-on experimentation, not hairy mathematics. You'll develop intuition about the kinds of problems they are suited to solve, and by the end youâ€™ll be ready to dive into the algorithms, or build one for yourself. A basic proficiency with algebra will help you understand this course. Remembering how to get the slope from an equation of a line is enough. Some basic knowledge of logic, like what **AND** and **OR** mean would also be useful. You don't need to know how to code to learn a lot from this course."
https://www.w3schools.com/ai/ai_neural_networks.asp,"W3Schools offers a wide range of services and products for beginners and professionals,helping millions of people everyday to learn and master new skills. Enjoy our free tutorials like millions of other internet users since 1999 Explore our selection of references covering all popular coding languages Create your own website withW3Schools Spaces- no setup required Test your skills with different exercises Test yourself with multiple choice questions Document your knowledge Create afreeW3Schools Account to Improve Your Learning Experience Track your learning progress at W3Schools and collect rewards Become a PLUS user and unlock powerful features (ad-free, hosting, support,..) Not sure where you want to start? Follow our guided path With our online code editor, you can edit code and view the result in your browser Learn the basics of HTML in a fun and engaging video tutorial We have created a bunch of responsive website templates you can use - for free! Host your own website, and share it to the world withW3Schools Spaces Create your own server using Python, PHP, React.js, Node.js, Java, C#, etc. Large collection of code snippets for HTML, CSS and JavaScript Build fast and responsive sites using our freeW3.CSSframework Read long term trends of browser usage Test your typing speed Learn Amazon Web Services Use our color picker to find different RGB, HEX and HSL colors. W3Schools Coding Game! Help the lynx collect pine cones Get personalized learning journey based on your current skills and goals Join our newsletter and get access to exclusive content every month Contact us about W3Schools Academy for educational institutions Contact us about W3Schools Academy for your organization About sales:sales@w3schools.comAbout errors:help@w3schools.com The deep learning revolutionstarted around 2010. Since then, Deep Learning has solved many ""unsolvable"" problems. The deep learning revolution was not started by a single discovery.
It more or less happened when several needed factors were ready: Scientists agree that our brain has between 80 and 100 billion neurons. These neurons have hundreds of billions connections between them. Image credit: University of Basel, Biozentrum. Neurons (aka Nerve Cells) are the fundamental units of our brain and nervous system. The neurons are responsible for receiving input from the external world,
for sending output (commands to our muscles),
and for transforming the electrical signals in between. Artificial Neural Networksare normally called Neural Networks (NN). Neural networks are in fact multi-layerPerceptrons. The perceptron defines the first step into multi-layered neural networks. Neural Networksare the essence ofDeep Learning. Neural Networksare one of the most significant discoveries in history. Neural Networks can solve problems that can NOT be solved by algorithms: Input data (Yellow) are processed against a hidden layer (Blue)
and modified against another hidden layer (Green) to produce the final output (Red). Tom Michael Mitchell (born 1951) is an American computer scientist and University Professor at the Carnegie Mellon University (CMU). He is a former Chair of the Machine Learning Department at CMU. ""A computer program is said to learn from experience E with respect to some class of tasks T
and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."" Tom Mitchell (1999) E: Experience (the number of times).T: The Task (driving a car).P: The Performance (good or bad). In 2015,Matthew Lai, a student at Imperial College in London created a neural network calledGiraffe. Giraffe could be trained in 72 hours to play chess at the same level as an international master. Computers playing chess are not new, but the way this program was created was new. Smart chess playing programs take years to build, while Giraffe was built in 72 hours with a neural network. Classical programming uses programs (algorithms) to create results: Data + Computer Algorithm =Result Machine Learning uses results to create programs (algorithms): Data + Result =Computer Algorithm Machine Learning is often considered equivalent with Artificial Intelligence. This is not correct. Machine learning is a subset of Artificial Intelligence. Machine Learning is a discipline of AI that uses data to teach machines. ""Machine Learning is a field of study that gives computers the ability to learn without being programmed."" Arthur Samuel (1959) The fact that computers can do this millions of times, 
has proven that computers can make very intelligent decisions. If you want to use W3Schools services as an educational institution, team or enterprise, send us an e-mail:sales@w3schools.com If you want to report an error, or if you want to make a suggestion, send us an e-mail:help@w3schools.com"
https://towardsdatascience.com/understanding-neural-networks-19020b758230,"Sign up Sign in Sign up Sign in Member-only story Tony Yiu Follow Towards Data Science -- 15 Share Deep learning is a hot topic these days. But what is it that makes it special and sets it apart from other aspects of machine learning? That is a deep question (pardon the pun). To even begin to answer it, we will need to learn the basics of neural networks. Neural networks are the workhorses of deep learning. And while they may look like black boxes, deep down (sorry, I will stop the terrible puns) they are trying to accomplish the same thing as any other model â€” to make good predictions. In this post, we will explore the ins and outs of a simple neural network. And by the end, hopefully you (and I) will have gained a deeper and more intuitive understanding of how neural networks do what they do. Letâ€™s start with a really high level overview so we know what we are working with.Neural networks are multi-layer networks of neurons (the blue and magenta nodes in the chart below) that we use to classify things, make predictions, etc. Below is the diagram of a simple neural network with five inputs, 5 outputs, and two hidden layers of neurons. -- -- 15 Your home for data science and AI. The worldâ€™s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals. Data scientist. Founder Alpha Beta Blog. Doing my best to explain the complex in plain English. Support my writing:https://tonester524.medium.com/membership Help Status About Careers Press Blog Privacy Terms Text to speech Teams"
https://www.coursera.org/articles/neural-network-example,"Discover neural network examples like self-driving cars and automatic content moderation, as well as a description of technologies powered by neural networks, like computer vision and speech recognition. A neural network is a type of artificial intelligence that allows machines to think similarly to humans by making â€œorganicâ€ connections through preexisting knowledge and learning from experience. One popular example of neural networks in use is the self-driving car, which needs to make decisions about and react to a wide number of random variables at any given moment. Microsoft CEO Satya Nadella named 2023 the â€œYear of AIâ€ due to the breakthroughs and gains the field of artificial intelligence made over the year. While itâ€™s true that AI growth in 2023 was expansive, the first neural network was created in 1958 by research psychologist Frank Rosenblatt, nearly 70 years ago [1]. Called the perceptron, Rosenblattâ€™s rudimentary invention created a foundation for the field that ultimately led to neural networks as we understand them today. Use this article to discover neural network examples to help you conceptualize how the technology works and the many neural network applications that may be possible across industries. A neural network is a machine learning system that attempts to mimic the way human intelligence works to power AI. It's structured using nodes arranged in layers that filter data and transfer information through the system to make connections. The network user creates an input, and the neural network delivers an output. However, under the surface, the input filters through a system of hidden layers, where the nodes carry varying weights to add complexity and nuance to the machineâ€™s understanding of that input. The more layers within the neural network, the more points of consideration the neural network will use to create the output. Neural networks are useful tools for open-ended or general problems where the associations between the variables arenâ€™t obvious or easy to label. When you offer nonlinear or complicated data to the neural network, the technology can discover and model how the data relates. A neural network simulates the way humans think. Itâ€™s no surprise that neural networks are versatile since our brains are also so versatile. Below, you will find examples of different technologies that neural networks contribute to, applications in specific industries, and use cases for companies using neural networks to solve problems. A neural network acts as a framework, supporting how artificial intelligence will operate and what it will do with the data presented to it. As a framework, it powers specific technologies like computer vision, speech recognition, natural language processing, and recommendation engines, giving us specific use cases for neural network technology. Letâ€™s take a closer look at each of these AI fields. Computer vision allows artificial intelligence to â€œlookâ€ at an image or video and process the information to understand and make decisions. Neural networks make computer vision faster and more accurate than was previously possible because a neural network can learn from data in real time without needing as much prior training. Much like human vision, artificial intelligence can use computer vision to observe and learn, classifying visual data for a broad range of applications. Speech recognition allows AI to â€œhearâ€ and understand natural language requests and conversations. Scientists have been working on speech recognition for computers since at least 1962. But today, advancements in neural networks and deep learning make it possible for artificial intelligence to have an unscripted conversation with a human, responding in ways that feel natural to a human ear. You can also use neural networks to enhance human speech, for example, during recorded teleconferencing or for hearing aids. Natural language processing (NLP) is similar to speech recognition. In addition to understanding and interpreting spoken requests, NLP focuses on understanding text. This technology enables AI chatbots like ChatGPT to have a written conversation with you. Neural networks allow computer scientists to train NLP systems much faster because they do not have to hand code and train the algorithm. A recommendation engine is an AI tool that suggests other products or media you might like based on what youâ€™ve browsed, purchased, read, or watched. With neural networks, a recommendation engine can gain a deeper understanding of consumer behavior and offer further targeted results that are likely to interest consumers. Recommendation tools can help encourage customers to stay more engaged on a website and make it easier for them to find items they like. All the technologies mentioned above benefit from neural network artificial intelligence. In practice, these areas of artificial intelligence offer many uses. A few specific neural network examples include:  Medical imaging:Healthcare professionals can use neural networks to read medical images, such as X-rays or MRIs. Artificial intelligence can analyze a medical image incredibly fast compared to a human professional and can continuously analyze images night and day, unlike a person constrained by human needs like hunger and fatigue.  Self-driving cars:Neural networks power self-driving cars. While on the road, these cars must be aware of many different variables happening simultaneously and randomly. In this environment, artificial intelligence also needs to make decisions based on the information it receives. A neural network enables the complex thinking a self-driving vehicle requires.  Public safety and security:Neural networks also offer various solutions for public safety and security. For example, artificial intelligence can be used for fraud detection, traffic accident detection, or predicting suspicious or criminal behavior.  Agriculture:In agriculture, farmers can use artificial intelligence for tasks like irrigation, pest control, predicting weather patterns, and choosing seeds optimized for their growing area. For these tasks, the artificial intelligence will need sensors to help it gain more information about the growing conditionsâ€”for example, a sensor to detect moisture levels in soil.  Online content moderation:Neural networks can detect online content that goes against community standards, acting as a quick and effective content moderator that never stops working. In fact, Meta reported in 2021 that it uses artificial intelligence to flag 97 percent of the content it removes from Facebook for community standards violations [2].  Voice-activated virtual assistants:Using speech recognition technology, the neural network at the center of your voice-activated virtual assistant can understand what you say to it and respond accordingly. With the advanced ability of neural networks, voice-activated virtual assistants can also understand the tone and context of what you say.  AI subtitles:Speech recognition and natural language processing together make it possible for artificial intelligence to automatically subtitle a video by listening to and understanding speech, and then translating it into a text caption.  Weâ€™ve discussed technologies and applications for neural networks, but what are some examples of companies using neural networks for solutions specific to their industries? Letâ€™s take a look at some solutions from Google and IBM:  You can use Google Translate to automatically translate the text contained in an image. For example, you could take a picture of a street sign or handwritten note, and Google Translate will scan it and provide a translation.  In 2018, IBM Watson used neural networks to create customized highlight reels of the Masters golf tournament. Users could curate the highlights they saw based on their preferences, taking advantage of a spoiler-free mode that would avoid ruining the cliffhanger moments.  In a partnership between IBM Watson, Quest Diagnostics, and Memorial Sloan Kettering Cancer Center, artificial intelligence bolstered by neural networks began reviewing lab results from cancer patients to provide genetic testing. Comparing the results against a vast library of cancer-related research, the AI then suggests the best course of individualized treatment. An AI agent can complete this work in a fraction of the time it takes a human health care professional.  If youâ€™re ready to discover more about the concept of neural networks, consider the courseNeural Networks and Deep Learningoffered by DeepLearning.AI on Coursera. With this course, you can learn about artificial neural networks, deep learning, and neural network architecture, among other topics.       Cornell Aeronautical Laboratory, Inc. â€œThe Perceptron: A Perceiving and Recognizing Automaton, https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf.â€ Accessed January 19, 2024. Meta. â€œCommunity Standards Enforcement Report, First Quarter 2021, https://about.fb.com/news/2021/05/community-standards-enforcement-report-q1-2021/.â€ Accessed January 19, 2024.              Editorial Team Courseraâ€™s editorial team is comprised of highly experienced professional editors, writers, and fact... This content has been made available for informational purposes only. Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals.   Coursera Footer Get Started with AI Popular Career Certificates Popular Subjects Popular Resources Coursera Community More"
https://en.wikipedia.org/wiki/Convolutional_neural_network,"Aconvolutional neural network(CNN) is aregularizedtype offeed-forward neural networkthat learnsfeaturesby itself viafilter(or kernel) optimization. This type ofdeep learningnetwork has been applied to process and make predictions from many different types of data including text, images and audio.[1]Convolution-based networks are the de-facto standard indeep learning-based approaches tocomputer visionand image processing, and have only recently been replaced -- in some cases -- by newer deep learning architectures such as thetransformer.Vanishing gradientsand exploding gradients, seen duringbackpropagationin earlier neural networks, are prevented by using regularized weights over fewer connections.[2][3]For example, foreachneuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 Ã— 100 pixels. However, applying cascadedconvolution(or cross-correlation) kernels,[4][5]only 25 neurons are required to process 5x5-sized tiles.[6][7]Higher-layer features are extracted from wider context windows, compared to lower-layer features. Some applications of CNNs include: CNNs are also known asshift invariantorspace invariant artificial neural networks, based on the shared-weight architecture of theconvolutionkernels or filters that slide along input features and provide translation-equivariantresponses known as feature maps.[13][14]Counter-intuitively, most convolutional neural networks are notinvariant to translation, due to the downsampling operation they apply to the input.[15] Feed-forward neural networksare usually fully connected networks, that is, each neuron in onelayeris connected to all neurons in the nextlayer. The ""full connectivity"" of these networks makes them prone tooverfittingdata. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.[16] Convolutional networks wereinspiredbybiologicalprocesses[17][18][19][20]in that the connectivity pattern betweenneuronsresembles the organization of the animalvisual cortex. Individualcortical neuronsrespond to stimuli only in a restricted region of thevisual fieldknown as thereceptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to otherimage classification algorithms. This means that the network learns to optimize thefilters(or kernels) through automated learning, whereas in traditional algorithms these filters arehand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.[to whom?] A convolutional neural network consists of an input layer,hidden layersand an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs adot productof the convolution kernel with the layer's input matrix. This product is usually theFrobenius inner product, and its activation function is commonlyReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such aspooling layers, fully connected layers, and normalization layers.
Here it should be noted how close a convolutional neural network is to amatched filter.[21] In a CNN, the input is atensorwith shape: (number of inputs) Ã— (input height) Ã— (input width) Ã— (inputchannels) After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (number of inputs) Ã— (feature map height) Ã— (feature map width) Ã— (feature mapchannels). Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.[22]Each convolutional neuron processes data only for itsreceptive field. Althoughfully connected feedforward neural networkscan be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 Ã— 100 has 10,000 weights foreachneuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper.[6]For example, using a 5 Ã— 5 tiling region, each with the same shared weights, requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen duringbackpropagationin earlier neural networks.[2][3] To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers,[23]which are based on a depthwise convolution followed by a pointwise convolution. Thedepthwise convolutionis a spatial convolution applied independently over each channel of the input tensor, while thepointwise convolutionis a standard convolution restricted to the use of1Ã—1{\displaystyle 1\times 1}kernels. Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 Ã— 2 are commonly used. Global pooling acts on all the neurons of the feature map.[24][25]There are two common types of pooling in popular use: max and average.Max poolinguses the maximum value of each local cluster of neurons in the feature map,[26][27]whileaverage poolingtakes the average value. Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditionalmultilayer perceptronneural network (MLP). The flattened matrix goes through a fully connected layer to classify the images. In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron'sreceptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is theentire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers. To manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution[28][29]expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios,[30]thus having a variable receptive field size. Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights. The vectors of weights and biases are calledfiltersand represent particularfeaturesof the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces thememory footprintbecause a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.[31]  A deconvolutional neural network is essentially the reverse of a CNN. It consists of deconvolutional layers and unpooling layers.[32] A deconvolutional layer is the transpose of a convolutional layer. Specifically, a convolutional layer can be written as a multiplication with a matrix, and a deconvolutional layer is multiplication with the transpose of that matrix.[33] An unpooling layer expands the layer. The max-unpooling layer is the simplest, as it simply copies each entry multiple times. For example, a 2-by-2 max-unpooling layer is[x]â†¦[xxxx]{\displaystyle [x]\mapsto {\begin{bmatrix}x&x\\x&x\end{bmatrix}}}. Deconvolution layers are used in image generators. By default, it creates periodic checkerboard artifact, which can be fixed by upscale-then-convolve.[34] CNN are often compared to the way the brain achieves vision processing in livingorganisms.[35] Work byHubelandWieselin the 1950s and 1960s showed that catvisual corticescontain neurons that individually respond to small regions of thevisual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as itsreceptive field.[36]Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space.[citation needed]The cortex in each hemisphere represents the contralateralvisual field.[citation needed] Their 1968 paper identified two basic visual cell types in the brain:[18] Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.[37][36] Inspired by Hubel and Wiesel's work, in 1969,Kunihiko Fukushimapublished a deep CNN that usesReLUactivation function.[38]Unlike most modern networks, this network used hand-designed kernels. It was not used in his neocognitron, since all the weights were nonnegative; lateral inhibition was used instead. The rectifier has become the most popular activation function for CNNs anddeep neural networksin general.[39] The ""neocognitron"" was introduced byKunihiko Fukushimain 1979.[40][19][17]The kernels were trained byunsupervised learning. It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers: In a variant of the neocognitron called thecresceptron, instead of using Fukushima's spatial averaging with inhibition and saturation, J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.[41]Max-pooling is often used in modern CNNs.[42] Severalsupervisedandunsupervised learningalgorithms have been proposed over the decades to train the weights of a neocognitron.[17]Today, however, the CNN architecture is usually trained throughbackpropagation. The term ""convolution"" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the firstConference on Neural Information Processing Systemsin 1987. Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to thesignal-processing concept of a filter, and demonstrated it on a speech recognition task.[7]They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function (""For convenience, we denote * as correlation instead of convolution. Note that convolving a(t) with b(t) is equivalent to correlating a(-t) with b(t)."").[7]Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here. Thetime delay neural network(TDNN) was introduced in 1987 byAlex Waibelet al. for phoneme recognition and was one of the first convolutional networks, as it achieved shift-invariance.[43]A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, usingbackpropagation.[44]Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.[43] TDNNs are convolutional networks that share weights along the temporal dimension.[45]They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution.[46]Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron. TDNNs improved the performance of far-distance speech recognition.[47] Denker et al. (1989) designed a 2-D CNN system to recognize hand-writtenZIP Codenumbers.[48]However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.[49] Following the advances in the training of 1-D CNNs by Waibel et al. (1987),Yann LeCunet al. (1989)[49]used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. 
Wei Zhang et al. (1988)[13][14]used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991)[50]and breast cancer detection in mammograms (1994).[51] This approach became a foundation of moderncomputer vision. In 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system.[26]In their system they used several TDNNs per word, one for eachsyllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. LeNet-5, a pioneering 7-level convolutional network byLeCunet al. in 1995,[52]classifies hand-written numbers on checks (British English:cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources. It was superior than other commercial courtesy amount reading systems (as of 1995). The system was integrated inNCR's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day.[53] A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.[13][14]It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with back-propagation. The training algorithm was further improved in 1991[54]to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991)[50]and automatic detection of breast cancer inmammograms (1994).[51] A different convolution-based design was proposed in 1988[55]for application to decomposition of one-dimensionalelectromyographyconvolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.[56][57] Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations ongraphics processing units(GPUs). In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation onCPU.[58]In 2005, another paper also emphasised the value ofGPGPUformachine learning.[59] The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.[60]In the same period, GPUs were also used for unsupervised training ofdeep belief networks.[61][62][63][64] In 2010, Dan Ciresan et al. atIDSIAtrained deep feedforward networks on GPUs.[65]In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU.[24]In 2011, the network win an image recognition contest where they achieved superhuman performance for the first time.[66]Then they won more competitions and achieved state of the art on several benchmarks.[67][42][27] Subsequently,AlexNet, a similar GPU-based CNN by Alex Krizhevsky et al. won theImageNet Large Scale Visual Recognition Challenge2012.[68]It was an early catalytic event for theAI boom. Compared to the training of CNNs usingGPUs, not much attention was given to CPU. (Viebke et al 2019) parallelizes CNN by thread- andSIMD-level parallelism that is available on theIntel Xeon Phi.[69][70] In the past, traditionalmultilayer perceptron(MLP) models were used for image recognition.[example needed]However, the full connectivity between nodes caused thecurse of dimensionality, and was computationally intractable with higher-resolution images. A 1000Ã—1000-pixel image withRGB colorchannels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale. For example, inCIFAR-10, images are only of size 32Ã—32Ã—3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200Ã—200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights. Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignoreslocality of referencein data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated byspatially localinput patterns. Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of avisual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features: Together, these properties allow CNNs to achieve better generalization onvision problems. Weight sharing dramatically reduces the number offree parameterslearned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks. A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below. The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnablefilters(orkernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter isconvolvedacross the width and height of the input volume, computing thedot productbetween the filter entries and the input, producing a 2-dimensionalactivation mapof that filter. As a result, the network learns filters that activate when it detects some specific type offeatureat some spatial position in the input.[73][nb 1] Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter. Self-supervised learninghas been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.[citation needed] When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing asparse local connectivitypattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume. The extent of this connectivity is ahyperparametercalled thereceptive fieldof the neuron. The connections arelocal in space(along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned (British English:learnt) filters produce the strongest response to a spatially local input pattern. Threehyperparameterscontrol the size of the output volume of the convolutional layer: the depth,stride, and padding size: The spatial size of the output volume is a function of the input volume sizeW{\displaystyle W}, the kernel field sizeK{\displaystyle K}of the convolutional layer neurons, the strideS{\displaystyle S}, and the amount of zero paddingP{\displaystyle P}on the border. The number of neurons that ""fit"" in a given volume is then: If this number is not aninteger, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in asymmetricway. In general, setting zero padding to beP=(Kâˆ’1)/2{\textstyle P=(K-1)/2}when the stride isS=1{\displaystyle S=1}ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding. A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as adepth slice, the neurons in each depth slice are constrained to use the same weights and bias. Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as aconvolutionof the neuron's weights with the input volume.[nb 2]Therefore, it is common to refer to the sets of weights as a filter (or akernel), which is convolved with the input. The result of this convolution is anactivation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to thetranslation invarianceof the CNN architecture.[15] Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a ""locally connected layer"". Another important concept of CNNs is pooling, which is used as a form of non-lineardown-sampling. Pooling provides downsampling because it reduces the spatial dimensions (height and width) of the input feature maps while retaining the most important information. There are several non-linear functions to implement pooling, wheremax poolingandaverage poolingare the most common. Pooling aggregates information from small regions of the input creatingpartitionsof the input feature map, typically using a fixed-size window (like 2x2) and applying a stride (often 2) to move the window across the input.[75]Note that without using a stride greater than 1, pooling would not perform downsampling, as it would simply move the pooling window across the input one step at a time, without reducing the size of the feature map. In other words, the stride is what actually causes the downsampling by determining how much the pooling window moves over the input. Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters,memory footprintand amount of computation in the network, and hence to also controloverfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as aReLU layer) in a CNN architecture.[73]:â€Š460â€“461While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used.[15][72]The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2Ã—2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:fX,Y(S)=maxa,b=01S2X+a,2Y+b.{\displaystyle f_{X,Y}(S)=\max _{a,b=0}^{1}S_{2X+a,2Y+b}.}In this case, everymax operationis over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well). In addition to max pooling, pooling units can use other functions, such asaveragepooling orâ„“2-normpooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.[76] Due to the effects of fast spatial reduction of the size of the representation,[which?]there is a recent trend towards using smaller filters[77]or discarding pooling layers altogether.[78] A channel max pooling (CMP) operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F âˆˆ R(CÃ—MÃ—N) and C âˆˆ R(cÃ—MÃ—N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation.[79] See[80][81]for reviews for pooling methods. ReLU is the abbreviation ofrectified linear unit. It was proposed byAlston Householderin 1941,[82]and used in CNN byKunihiko Fukushimain 1969.[38]ReLU applies the non-saturatingactivation functionf(x)=max(0,x){\textstyle f(x)=\max(0,x)}.[68]It effectively removes negative values from an activation map by setting them to zero.[83]It introducesnonlinearityto thedecision functionand in the overall network without affecting the receptive fields of the convolution layers.
In 2011, Xavier Glorot, Antoine Bordes andYoshua Bengiofound that ReLU enables better training of deeper networks,[84]compared to widely used activation functions prior to 2011. Other functions can also be used to increase nonlinearity, for example the saturatinghyperbolic tangentf(x)=tanhâ¡(x){\displaystyle f(x)=\tanh(x)},f(x)=|tanhâ¡(x)|{\displaystyle f(x)=|\tanh(x)|}, and thesigmoid functionÏƒ(x)=(1+eâˆ’x)âˆ’1{\textstyle \sigma (x)=(1+e^{-x})^{-1}}. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty togeneralizationaccuracy.[85] After several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional)artificial neural networks. Their activations can thus be computed as anaffine transformation, withmatrix multiplicationfollowed by a bias offset (vector additionof a learned or fixed bias term). The ""loss layer"", or ""loss function"", specifies howtrainingpenalizes the deviation between the predicted output of the network, and thetruedata labels (during supervised learning). Variousloss functionscan be used, depending on the specific task. TheSoftmaxloss function is used for predicting a single class ofKmutually exclusive classes.[nb 3]Sigmoidcross-entropyloss is used for predictingKindependent probability values in[0,1]{\displaystyle [0,1]}.Euclideanloss is used forregressingtoreal-valuedlabels(âˆ’âˆž,âˆž){\displaystyle (-\infty ,\infty )}. Hyperparameters are various settings that are used to control the learning process. CNNs use morehyperparametersthan a standard multilayer perceptron (MLP). The kernel is the number of pixels processed together. It is typically expressed as the kernel's dimensions, e.g., 2x2, or 3x3. Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.[citation needed] The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor. Since feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature valuesvawith pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next. The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity. Common filter sizes found in the literature vary greatly, and are usually chosen based on the data set. Typical filter sizes range from 1x1 to 7x7. As two famous examples,AlexNetused 3x3, 5x5, and 11x11.Inceptionv3used 1x1, 3x3, and 5x5. The challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and withoutoverfitting. Max poolingis typically used, often with a 2x2 dimension. This implies that the input is drasticallydownsampled, reducing processing cost. Greater poolingreduces the dimensionof the signal, and may result in unacceptableinformation loss. Often, non-overlapping pooling windows perform best.[76] Dilation involves ignoring pixels within a kernel. This reduces processing memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels. Specifically, the processed pixels after the dilation are the cells (1,1), (1,3), (1,5), (3,1), (3,3), (3,5), (5,1), (5,3), (5,5), where (i,j) denotes the cell of the i-th row and j-th column in the expanded 5x5 kernel. Accordingly, dilation of 4 expands the kernel to 7x7.[citation needed] It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeedequivariantto translations of the input.[72]However, layers with a stride greater than one ignore theNyquist-Shannon sampling theoremand might lead toaliasingof the input signal[72]While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice[86]and yield models that are not equivariant to translations.
Furthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input.[87][15]One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer.[72]Additionally, several other partial solutions have been proposed, such asanti-aliasingbefore downsampling operations,[88]spatial transformer networks,[89]data augmentation, subsampling combined with pooling,[15]andcapsule neural networks.[90] The accuracy of the final model is based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such ask-fold cross-validationare applied. Other strategies include usingconformal prediction.[91][92] Regularizationis a process of introducing additional information to solve anill-posed problemor to preventoverfitting. CNNs use various types of regularization. Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting isdropout, introduced in 2014.[93]At each training stage, individual nodes are either ""dropped out"" of the net (ignored) with probability1âˆ’p{\displaystyle 1-p}or kept with probabilityp{\displaystyle p}, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights. In the training stages,p{\displaystyle p}is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored. At testing time after training has finished, we would ideally like to find a sample average of all possible2n{\displaystyle 2^{n}}dropped-out networks; unfortunately this is unfeasible for large values ofn{\displaystyle n}. However, we can find an approximation by using the full network with each node's output weighted by a factor ofp{\displaystyle p}, so theexpected valueof the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates2n{\displaystyle 2^{n}}neural nets, and as such allows for model combination, at test time only a single network needs to be tested. By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even fordeep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features[clarification needed]that better generalize to new data. DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability1âˆ’p{\displaystyle 1-p}. Each unit thus receives input from a random subset of units in the previous layer.[94] DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected. Even before Dropout, in 2013 a technique called stochastic pooling,[95]the conventionaldeterministicpooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to amultinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout anddata augmentation. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small localdeformations. This is similar to explicitelastic deformationsof the input images,[96]which delivers excellent performance on theMNIST data set.[96]Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. Because the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s.[52]For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.[97] One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a ""zero norm"". A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is calledelastic net regularization. Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and useprojected gradient descentto enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vectorwâ†’{\displaystyle {\vec {w}}}of every neuron to satisfyâ€–wâ†’â€–2<c{\displaystyle \|{\vec {w}}\|_{2}<c}. Typical values ofc{\displaystyle c}are order of 3â€“4. Some papers report improvements[98]when using this form of regularization. Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.[99] An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to theretina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.[100] Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (""pose vectors"") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the humanvisual systemimposes coordinate frames in order to represent shapes.[101] CNNs are often used inimage recognitionsystems. In 2012, anerror rateof 0.23% on theMNIST databasewas reported.[27]Another paper on using CNN for image classification reported that the learning process was ""surprisingly fast""; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.[24]Subsequently, a similar CNN calledAlexNet[102]won theImageNet Large Scale Visual Recognition Challenge2012. When applied tofacial recognition, CNNs achieved a large decrease in error rate.[103]Another paper reported a 97.6% recognition rate on ""5,600 still images of more than 10 subjects"".[20]CNNs were used to assessvideo qualityin an objective way after manual training; the resulting system had a very lowroot mean square error.[104] TheImageNet Large Scale Visual Recognition Challengeis a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,[105]a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winnerGoogLeNet[106](the foundation ofDeepDream) increased the mean averageprecisionof object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.[107]The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.[citation needed] In 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.[108] Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.[109][110]Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.[111][112][113]Long short-term memory(LSTM)recurrentunits are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.[114][115]Unsupervised learningschemes for training spatio-temporal features have been introduced, based on Convolutional Gated RestrictedBoltzmann Machines[116]and Independent Subspace Analysis.[117]Its application can be seen intext-to-video model.[citation needed] CNNs have also been explored fornatural language processing. CNN models are effective for various NLP problems and achieved excellent results insemantic parsing,[118]search query retrieval,[119]sentence modeling,[120]classification,[121]prediction[122]and other traditional NLP tasks.[123]Compared to traditional language processing methods such asrecurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required.[124][125][126][127] A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.[128] CNNs have been used indrug discovery. Predicting the interaction between molecules and biologicalproteinscan identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network forstructure-based drug design.[129]The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,[130]AtomNet discovers chemical features, such asaromaticity,sp3carbons, andhydrogen bonding. Subsequently, AtomNet was used to predict novel candidatebiomoleculesfor multiple disease targets, most notably treatments for theEbola virus[131]andmultiple sclerosis.[132] CNNs have been used in the game ofcheckers. From 1999 to 2001,Fogeland Chellapilla published papers showing how a convolutional neural network could learn to playcheckerusing co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%.[133][134]It also earned a win against the programChinookat its ""expert"" level of play.[135] CNNs have been used incomputer Go. In December 2014, Clark andStorkeypublished a paper showing that a CNN trained by supervised learning from a database of human professional games could outperformGNU Goand win some games againstMonte Carlo tree searchFuego 1.1 in a fraction of the time it took Fuego to play.[136]Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a6 danhuman player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of theMonte Carlo tree searchprogram Fuego simulating ten thousand playouts (about a million positions) per move.[137] A couple of CNNs for choosing moves to try (""policy network"") and evaluating positions (""value network"") driving MCTS were used byAlphaGo, the first to beat the best human player at the time.[138] Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better.[139][12]Dilated convolutions[140]might enable one-dimensional convolutional neural networks to effectively learn time series dependences.[141]Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients.[142]Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.[143]CNNs can also be applied to further tasks in time series analysis (e.g., time series classification[144]or quantile forecasting[145]). As archaeological findings such asclay tabletswithcuneiform writingare increasingly acquired using3D scanners, benchmark datasets are becoming available, includingHeiCuBeDa[146]providing almost 2000 normalized 2-D and 3-D datasets prepared with theGigaMesh Software Framework.[147]Socurvature-based measures are used in conjunction with geometric neural networks (GNNs), e.g. for period classification of those clay tablets being among the oldest documents of human history.[148][149] For many applications, training data is not very available. Convolutional neural networks usually require a large amount of training data in order to avoidoverfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known astransfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.[150] End-to-end training and prediction are common practice incomputer vision. However, human interpretable explanations are required forcritical systemssuch as aself-driving cars.[151]With recent advances invisual salience,spatial attention, andtemporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.[152][153] A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network withQ-learning, a form ofreinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.[154] Preliminary results were presented in 2014, with an accompanying paper in February 2015.[155]The research described an application toAtari 2600gaming. Other deep reinforcement learning models preceded it.[156] Convolutional deep belief networks(CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training likedeep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR[157]have been obtained using CDBNs.[158] The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid[159]by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks."
https://www.datacamp.com/tutorial/introduction-to-deep-neural-networks,Error: 403 Client Error: Forbidden for url: https://www.datacamp.com/tutorial/introduction-to-deep-neural-networks
https://blog.csdn.net/RayChiu757374816/article/details/121351693,
https://www.w3schools.com/ai/ai_neural_networks.asp,"W3Schools offers a wide range of services and products for beginners and professionals,helping millions of people everyday to learn and master new skills. Enjoy our free tutorials like millions of other internet users since 1999 Explore our selection of references covering all popular coding languages Create your own website withW3Schools Spaces- no setup required Test your skills with different exercises Test yourself with multiple choice questions Document your knowledge Create afreeW3Schools Account to Improve Your Learning Experience Track your learning progress at W3Schools and collect rewards Become a PLUS user and unlock powerful features (ad-free, hosting, support,..) Not sure where you want to start? Follow our guided path With our online code editor, you can edit code and view the result in your browser Learn the basics of HTML in a fun and engaging video tutorial We have created a bunch of responsive website templates you can use - for free! Host your own website, and share it to the world withW3Schools Spaces Create your own server using Python, PHP, React.js, Node.js, Java, C#, etc. Large collection of code snippets for HTML, CSS and JavaScript Build fast and responsive sites using our freeW3.CSSframework Read long term trends of browser usage Test your typing speed Learn Amazon Web Services Use our color picker to find different RGB, HEX and HSL colors. W3Schools Coding Game! Help the lynx collect pine cones Get personalized learning journey based on your current skills and goals Join our newsletter and get access to exclusive content every month Contact us about W3Schools Academy for educational institutions Contact us about W3Schools Academy for your organization About sales:sales@w3schools.comAbout errors:help@w3schools.com The deep learning revolutionstarted around 2010. Since then, Deep Learning has solved many ""unsolvable"" problems. The deep learning revolution was not started by a single discovery.
It more or less happened when several needed factors were ready: Scientists agree that our brain has between 80 and 100 billion neurons. These neurons have hundreds of billions connections between them. Image credit: University of Basel, Biozentrum. Neurons (aka Nerve Cells) are the fundamental units of our brain and nervous system. The neurons are responsible for receiving input from the external world,
for sending output (commands to our muscles),
and for transforming the electrical signals in between. Artificial Neural Networksare normally called Neural Networks (NN). Neural networks are in fact multi-layerPerceptrons. The perceptron defines the first step into multi-layered neural networks. Neural Networksare the essence ofDeep Learning. Neural Networksare one of the most significant discoveries in history. Neural Networks can solve problems that can NOT be solved by algorithms: Input data (Yellow) are processed against a hidden layer (Blue)
and modified against another hidden layer (Green) to produce the final output (Red). Tom Michael Mitchell (born 1951) is an American computer scientist and University Professor at the Carnegie Mellon University (CMU). He is a former Chair of the Machine Learning Department at CMU. ""A computer program is said to learn from experience E with respect to some class of tasks T
and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."" Tom Mitchell (1999) E: Experience (the number of times).T: The Task (driving a car).P: The Performance (good or bad). In 2015,Matthew Lai, a student at Imperial College in London created a neural network calledGiraffe. Giraffe could be trained in 72 hours to play chess at the same level as an international master. Computers playing chess are not new, but the way this program was created was new. Smart chess playing programs take years to build, while Giraffe was built in 72 hours with a neural network. Classical programming uses programs (algorithms) to create results: Data + Computer Algorithm =Result Machine Learning uses results to create programs (algorithms): Data + Result =Computer Algorithm Machine Learning is often considered equivalent with Artificial Intelligence. This is not correct. Machine learning is a subset of Artificial Intelligence. Machine Learning is a discipline of AI that uses data to teach machines. ""Machine Learning is a field of study that gives computers the ability to learn without being programmed."" Arthur Samuel (1959) The fact that computers can do this millions of times, 
has proven that computers can make very intelligent decisions. If you want to use W3Schools services as an educational institution, team or enterprise, send us an e-mail:sales@w3schools.com If you want to report an error, or if you want to make a suggestion, send us an e-mail:help@w3schools.com"
https://medium.com/thedeephub/a-complete-guide-to-nlp-techniques-for-data-science-beginners-dae7f2e3cae7,"Sign up Sign in Sign up Sign in Member-only story Harsh Chourasia Follow The Deep Hub -- Share Stepping into a new world, when I first started to learn NLP, I felt that I was entering an altogether different realm. As a developer with the background from the MERN stack, numbers, logic, and structure were what I worked with. Teaching machines how to understand language made me go gaga because it was fascinating but scary at the same time. I understand completely if youâ€™re just getting started with NLP. There is just so much to unpack, but letâ€™s get as simple as possible so you can get started without feeling overwhelmed. Here are some of the top essential NLP techniques that every beginner in data science should know. At its core, Natural Language Processing (NLP) is teaching computers to understand, interpret, and respond to human language, whether it be written text, spoken words, or indeed any combination of the two. Itâ€™s fascinating because the language isnâ€™t like the numbers; itâ€™s full of ambiguity, idioms, and emotions. Itâ€™s tough to get a machine to find some meaning in that, but when you do, itâ€™s justâ€¦ -- -- Your data science hub. A Medium publication dedicated to exchanging ideas and empowering your knowledge. Data Scientist | I write about Data Science, AI, programming, tech, self-Improvement and personal growth for a well-rounded, successful and fulfilling life!! Help Status About Careers Press Blog Privacy Terms Text to speech Teams"
https://www.geeksforgeeks.org/natural-language-processing-nlp-tutorial/,"Natural Language Processing (NLP) is the branch of Artificial Intelligence (AI) that gives the ability to machine understand and process human languages. Human languages can be in the form of text or audio format. The applications of Natural Language Processing are as follows: This NLP tutorial is designed for both beginners and professionals. Whether you are a  beginner or a data scientist, this guide will provide you with the knowledge and skills you need to take your understanding of NLP to the next level.   There are two components of Natural Language Processing: Some of natural language processing libraries include: To explore in detail, you can refer to this article:NLP Libraries in Python Text Normalizationtransforms text into a consistent format improves the quality and makes it easier to process in NLP tasks. Key steps in text normalization includes: 1. Regular Expressions (RE)are sequences of characters that define search patterns. 2.Tokenizationis a process of splitting text into smaller units called tokens. 3.Lemmatizationreduces words to their base or root form. 4.Stemmingreduces works to their root by removing suffixes. Types of stemmers include: 5.Stopword removalis a process to remove common words from the document. 6.Parts of Speech (POS) Taggingassigns a part of speech to each word in sentence based on definition and context. Text representationconverts textual data into numerical vectors that are processed by the following methods: Text Embedding Techniquesrefer to the methods and models used to create these vector representations, including traditional methods (like TFIDF and BOW) and more advanced approaches: 1. Word Embedding 2. Pre-Trained Embedding 3. Document Embedding â€“Doc2Vec Deep learninghas revolutionized Natural Language Processing (NLP) by enabling models to automatically learn complex patterns and representations from raw text. Below are some of the key deep learning techniques used in NLP: Pre-trained modelsunderstand language patterns, context and semantics. The provided models are trained on massive corpora and can be fine tuned for specific tasks. To learn how to fine tune a model, refer to this article:Transfer Learning with Fine-tuning 1. Text Classification 2. Information Extraction 3. Sentiment Analysis 4. Machine Translation 5. Text Summarization 6. Text Generation Natural Language Processing (NLP)emerged in 1950 whenAlan Turingpublished his groundbreaking paper titledComputing Machinery and Intelligence. Turingâ€™s work laid the foundation forNLP, which is a subset ofArtificial Intelligence (AI)focused on enabling machines to automatically interpret and generate human language. Over time, NLP technology has evolved, giving rise to different approaches for solving complex language-related tasks. TheHeuristic-based approach to NLPwas one of the earliest methods used in natural language processing. It relies on predefined rules and domain-specific knowledge. These rules are typically derived from expert insights. A classic example of this approach isRegular Expressions (Regex), which are used for pattern matching and text manipulation tasks. As NLP advanced,Statistical NLPemerged, incorporatingmachine learning algorithmsto model language patterns. This approach applies statistical rules and learns from data to tackle various language processing tasks. Popularmachine learning algorithmsin this category include: The most recent advancement in NLP is the adoption ofDeep Learningtechniques. Neural networks, particularlyRecurrent Neural Networks (RNNs),Long Short-Term Memory Networks (LSTMs), andTransformers, have revolutionized NLP tasks by providing superior accuracy. These models require large amounts of data and considerable computational power for training Ambiguity is the main challenge of natural language processing because in natural language, words are unique, but they have different meanings depending upon the context which causes ambiguity on lexical, syntactic, and semantic levels. The four main pillars of NLP are 1.) Outcomes, 2.) Â Sensory acuity, 3.) behavioural flexibility, and 4.) report. Python is considered the best programming language for NLP because of their numerous libraries, simple syntax, and ability to easily integrate with other programming languages. There are four stages included in the life cycle of NLP â€“ development, validation, deployment, and monitoring of the models. "
https://www.kdnuggets.com/2020/01/intro-guide-nlp-data-scientists.html,Error: 403 Client Error: Forbidden for url: https://www.kdnuggets.com/2020/01/intro-guide-nlp-data-scientists.html
https://www.datacamp.com/blog/what-is-natural-language-processing,Error: 403 Client Error: Forbidden for url: https://www.datacamp.com/blog/what-is-natural-language-processing
https://machinelearningmastery.com/the-beginners-guide-to-natural-language-processing-with-python/,Error: 403 Client Error: Forbidden for url: https://machinelearningmastery.com/the-beginners-guide-to-natural-language-processing-with-python/
https://www.datacamp.com/blog/how-to-learn-nlp,Error: 403 Client Error: Forbidden for url: https://www.datacamp.com/blog/how-to-learn-nlp
https://towardsdatascience.com/natural-language-processing-nlp-for-beginners-6d19f3eedfea,"Sign up Sign in Sign up Sign in Member-only story Behic Guven Follow Towards Data Science -- Share Inthis post, I will introduce you to one of the most known artificial intelligence field called Natural Language Processing. After the introduction, I will walk you through a hands-on exercise where we will extract some valuable information from a specific website. For the hands-on project, we will use a specific NLP module called NLTK (Natural Language Toolkit), which will be covered after the introduction section. After reading this article, you will have a better understanding of natural language processing applications and how they work. Without losing any time, letâ€™s get started! Natural language refers to the language we use in our daily life. This field has been around for a long time, but artificial intelligenceâ€¦ -- -- Your home for data science and AI. The worldâ€™s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals. Python | Deep Learning | Itinerant of this beautiful life trip â€” For Business reach me atwww.sonsuzdesign.blog Help Status About Careers Press Blog Privacy Terms Text to speech Teams"
https://thedatascientist.com/natural-language-processing-data-science-tutorial-python/,"The Data Scientist Natural Language Processing (NLP) is the branch of data science primarily concerned with dealing with textual data. It is the intersection of linguistics, artificial intelligence, and computer science.  NLP deals with human-computer interaction and helps computers understand natural language better. The main goal of Natural Language Processing is to help computers understand language as well as we do. It has numerous applications including but not limited to text summarization,sentiment analysis, language translation, named entity recognition, relation extraction, etc. Natural Language Processing is considered more challenging than otherdata sciencedomains. This is due to a number of reasons. It can convey the same meaning using multiple different combinations of words.  Natural Language is also ambiguous, the same combination of words can also have different meanings, and sometimes interpreting the context can become difficult. Thus, it can become challenging when working with natural language. To work with more complex systems, you need to make sure you solve simple NLP tasks. No matter the difficulty, address a reliable service forData Science assignment helpand build a strong understanding of fundamentals necessary for ML, AI, NLP, and LLM-related challenges. Save time with experts to learn more and perform better. Two primary ways to understand natural language are syntactic analysis and semantic analysis. The syntactic analysis deals with the syntax of the sentences whereas, the semantic analysis deals with the meaning being conveyed by those sentences. An important thing to note here is that even if a sentence is syntactically correct that doesnâ€™t necessarily mean it is semantically correct. In syntactic analysis, we use rules of formal grammar to validate a group of words. With syntactic analysis, we validate the structure of our sentences. Semantic analysis deals with the part where we try to understand the meaning conveyed by sentences. This will allow computers to understand natural language better. It still remains largely unsolved and more work is being done on it. Some of the most popularly used packages for different NLP methods are the following:  NaturalLanguage Toolkit or NLTK is one of the widely used NLP packages to deal with human language data. It comes with numerous unstructured data and human-readable text. UsingNLTKwe can easilyprocess texts and understand textual databetter. Spacyis another popular NLP package and is used for advanced Natural Language Processing tasks. It contains a lot of state-of-the-art models for several different problems. It is an open-source package that was created with the purpose that itâ€™ll be used tobuild real products. Hugging Faceis the most popular NLP package out there right now. It is an open-source package with numerous state-of-the-art models that can be applied tosolve various different problems. They also have numerous datasets and courses to help NLP enthusiasts get started. Before we move on to the next part, letâ€™s import all the necessary libraries that we will be using in this tutorial.import spacy import nltk import gensim from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer from nltk.stem import PorterStemmer from nltk.tokenize import word_tokenize from nltk import ngrams from sklearn import preprocessing import gensim.downloader as API Thedata cleaning processlargely depends on the problem that we are working on. But normally, what we do is remove any special characters such as $, %, #, @, <, >, etc. These symbols donâ€™t hold any information for ourmodel to learn. They act as noise in our data so, we discard them. There are a number of NLP methods to preprocess data. In thisdata sciencetutorial, we will be going over some of them. If any character in our text is in uppercase we convert it to lowercase. Otherwise, our model will perceive the uppercase and lowercase characters as different from each other. This will cause problems when parsing the text later on. For example, the sentence â€œThe dog belongs to Jimâ€ would be converted to â€œthe dog belongs to himâ€. Letâ€™s take a look at how to convert our textual data to lowercase.text_data = """""" Let's convert this demo text to Lowercase for this NLP Tutorial using NLTK. NLTK stands for Natural Language Toolkit """""" lower_text = text_data.lower() print (lower_text)Output:letâ€™s convert this demo text to lowercase for this nlp tutorial using nltk. nltk stands for natural language toolkit In tokenization, we take our text from the documents and break them down into individual words.For example â€œThe dog belongs to Jimâ€ would be converted to a list of tokens [â€œTheâ€, â€œdogâ€, â€œbelongsâ€, â€œtoâ€, â€œJimâ€]. Now we will take our textual data and make word tokens of our data. Then we will be printing them.word_tokens = word_tokenize(text_data) print(word_tokens)Output:[â€˜Letâ€™, â€œâ€˜sâ€, â€˜convertâ€™, â€˜thisâ€™, â€˜demoâ€™, â€˜textâ€™, â€˜toâ€™, â€˜Lowercaseâ€™, â€˜forâ€™, â€˜thisâ€™, â€˜NLPâ€™, â€˜Tutorialâ€™, â€˜usingâ€™, â€˜NLTKâ€™, â€˜.â€™, â€˜NLTKâ€™, â€˜standsâ€™, â€˜forâ€™, â€˜Naturalâ€™, â€˜Languageâ€™, â€˜Toolkitâ€™] We remove words from our text data that donâ€™t add much information to the document. So, they add noise to the data. For different domains, it is possible that the stop words may differ from each other. Consider an example, if â€œtheâ€ and â€œtoâ€ our some tokens in our stopwords list, when we remove stopwords from our sentence â€œThe dog belongs to Jimâ€ we will be left with â€œdog belongs Jimâ€. Stop words donâ€™t hold a great deal of information so it is better to remove them since they act as noise in our data. We getstopword = stopwords.words('english') removing_stopwords = [word for word in word_tokens if word not in stopword] print (removing_stopwords)Output:[â€˜Letâ€™, â€œâ€˜sâ€, â€˜convertâ€™, â€˜demoâ€™, â€˜textâ€™, â€˜Lowercaseâ€™, â€˜NLPâ€™, â€˜Tutorialâ€™, â€˜usingâ€™, â€˜NLTKâ€™, â€˜.â€™, â€˜NLTKâ€™, â€˜standsâ€™, â€˜Naturalâ€™, â€˜Languageâ€™, â€˜Toolkitâ€™] In stemming we reduce a word to its root word. It transforms the word back to its original form i.e reduces inflection. Thus, we are only left with the stem words. For example, when we apply stemming on â€œCaringâ€, we will be left with â€œCarâ€. Using stemming, we can reduce a word to its original form but it is important to note here that it doesnâ€™t always have some meaning. With the following code, we can reduce a word to its root word. Weâ€™ll be using thePorter Stemmersince it is one of the most commonly used stemmers.ps = PorterStemmer() stemmed_words = [ps.stem(word) for word in word_tokens] print(stemmed_words)Output:[â€˜letâ€™, â€œâ€˜sâ€, â€˜convertâ€™, â€˜thiâ€™, â€˜demoâ€™, â€˜textâ€™, â€˜toâ€™, â€˜lowercasâ€™, â€˜forâ€™, â€˜thiâ€™, â€˜nlpâ€™, â€˜tutoriâ€™, â€˜useâ€™, â€˜nltkâ€™, â€˜.â€™, â€˜nltkâ€™, â€˜standâ€™, â€˜forâ€™, â€˜naturâ€™, â€˜languagâ€™, â€˜toolkitâ€™] Lemmatization does the same thing as stemming but in lemmatization, we get a root word that has some meaning. Whereas stemming from the root word may or may not have any meaning. For example, when we apply lemmatization on â€œCaringâ€ we will be left with â€œCareâ€.Lemmatization is similar to Stemming but in the case of lemmatization, the reduced word will have some meaning. Now weâ€™ll take a look at how to perform lemmatization using Python. Weâ€™ll be using a different list of tokens to be able to see better how lemmatization works.wnl = WordNetLemmatizer() word_tokens2 = [""corpora"",""better"",""rocks"",""care"",""classes""] lemmatized_word = [wnl.lemmatize(word) for word in word_tokens2] print (lemmatized_word)Output:[â€˜corpusâ€™, â€˜betterâ€™, â€˜rockâ€™, â€˜careâ€™, â€˜classâ€™] N Grams are used to preserve the sequence of information which is present in the document. When N = 1, they are called Unigrams. When N = 2, they are called bigrams. When N = 3, they are called trigrams. And so on. In unigrams, since each word is taken individually, no sequence information is preserved. For example, â€œToday is Monday.â€ Unigrams = Today, is, Monday Bigrams = Today is, is Monday Trigrams = Today is MondayLetâ€™s see how we can convert our text data to N-grams. Over here the value of N will be 3, so weâ€™ll be making trigrams.n_grams = ngrams(text_data.split(), 3) for grams in n_grams: print(grams)Output:(â€œLetâ€™sâ€, â€˜convertâ€™, â€˜thisâ€™)(â€˜convertâ€™, â€˜thisâ€™, â€˜demoâ€™)(â€˜thisâ€™, â€˜demoâ€™, â€˜textâ€™)(â€˜demoâ€™, â€˜textâ€™, â€˜toâ€™)(â€˜textâ€™, â€˜toâ€™, â€˜Lowercaseâ€™)(â€˜toâ€™, â€˜Lowercaseâ€™, â€˜forâ€™)(â€˜Lowercaseâ€™, â€˜forâ€™, â€˜thisâ€™)(â€˜forâ€™, â€˜thisâ€™, â€˜NLPâ€™)(â€˜thisâ€™, â€˜NLPâ€™, â€˜Tutorialâ€™)(â€˜NLPâ€™, â€˜Tutorialâ€™, â€˜usingâ€™)(â€˜Tutorialâ€™, â€˜usingâ€™, â€˜NLTK.â€™)(â€˜usingâ€™, â€˜NLTK.â€™, â€˜NLTKâ€™)(â€˜NLTK.â€™, â€˜NLTKâ€™, â€˜standsâ€™)(â€˜NLTKâ€™, â€˜standsâ€™, â€˜forâ€™)(â€˜standsâ€™, â€˜forâ€™, â€˜Naturalâ€™)(â€˜forâ€™, â€˜Naturalâ€™, â€˜Languageâ€™)(â€˜Naturalâ€™, â€˜Languageâ€™, â€˜Toolkitâ€™) In order to helpmachines understand textual data, we have to convert them to a format that will make it easier for them to understand the text. This is why we convert text to numbers. There are many NLP methods to convert text to numbers, but weâ€™ll be covering some of them in this article. Word vectors or word embeddings are textual data mapped to real numbers. After numbers have been converted to word vectors, we can perform a number of operations on them. Such as, finding similar words, classifying text, clustering documents, etc. Now letâ€™s discuss some methods for converting words to word vectors. In one-hot vector encoding, we made embeddings of the entire corpus. In these types of word vectors, all the words are independent of each other. We couldnâ€™t find the dependence of one word on other words. So, they werenâ€™t of much use to us.While making a one-hot encoded vector, it simply placed a 1 where the word was and 0 everywhere else in the vector. Letâ€™s see how we can convert our text to one-hot encoded vectors. Weâ€™ll be using a different list of tokens to better understand how one hot encoding works.word_tokens3 = ['corpora', 'better', 'rocks', 'care', 'classes','better','apple'] lab_encoder = preprocessing.LabelEncoder() int_label_encoder = lab_encoder.fit_transform(word_tokens3) lab_encoded = int_label_encoder.reshape(len(int_label_encoder),1) one_hot_encoder = preprocessing.OneHotEncoder(sparse=False) one_hot_encoded = one_hot_encoder.fit_transform(lab_encoded) print(one_hot_encoded) print(word_tokens3)Output:[[0. 0. 0. 0. 1. 0.][0. 1. 0. 0. 0. 0.][0. 0. 0. 0. 0. 1.][0. 0. 1. 0. 0. 0.][0. 0. 0. 1. 0. 0.][0. 1. 0. 0. 0. 0.][1. 0. 0. 0. 0. 0.]][â€˜corporaâ€™, â€˜betterâ€™, â€˜rocksâ€™, â€˜careâ€™, â€˜classesâ€™, â€˜betterâ€™, â€˜appleâ€™] With word2vec, we were able to form a dependence of words with other words. These were a considerable improvement over One Hot Vector. One hot vector didnâ€™t consider context whereas, word2vec does consider the context. Thus, we can use them to find word similarities. Now letâ€™s see how we can make Word2Vec vectors of ourdata. We wonâ€™t be training a Word2Vec modelfrom scratch, weâ€™ll just load a pre-trained Word2Vec model using Gensim which is another important package for different NLP methods.model = api.load(""word2vec-google-news-300"") model.most_similar(""obama"")Output:[(â€˜romneyâ€™, 0.9566564559936523),(â€˜presidentâ€™, 0.9400959610939026),(â€˜barackâ€™, 0.9376799464225769),(â€˜clintonâ€™, 0.9285898804664612),(â€˜saysâ€™, 0.9087842702865601),(â€˜billâ€™, 0.9080009460449219),(â€˜claimsâ€™, 0.9074634909629822),(â€˜hillaryâ€™, 0.8889248371124268),(â€˜talksâ€™, 0.8864543437957764),(â€˜governmentâ€™, 0.8833804130554199)]Letâ€™s go through some different methods to create Word2Vec vectors. The following are two methods we can use to obtain word2vec vectors: In the CBOW (continuous bag of words) model, we predict the target (center) word using the context (neighboring) words. The CBOW model is faster than the skip-gram model because it requires fewer computations and it is great at representing less frequent words. With Skip Gram, we predict the context words using the target word. Even though the skip-gram model is a bit slower than the CBOW model, it is still great at representing rare words. Now weâ€™ll be going through one of the important NLP methods for recognizing entities. Itâ€™s called named entity recognition. Named Entity Recognition is an important information retrieval technique. To understand the working of named entity recognition, look at the diagram below. From the above diagram, we can see that a named entity recognition model takes text as input and returns the entities along with their labels present in the text. It has numerous applications. It can be used for content classification, using it we can detect entities in text and classify the content based on those entities. In academia and research, it can be used for retrieving information faster.Now letâ€™s take a look at how we can do NER in python. First weâ€™ll load a pre-trained spacy pipeline which is trained on numerous different forms of textual data. Using that, we can use different NLP methods. For now letâ€™s take a look at NER. nlp = spacy.load(""en_core_web_sm"") # Process whole documents text = (""When Sebastian Thrun started working on self-driving cars at "" ""Google in 2007, few people outside of the company took him "" ""seriously. â€œI can tell you very senior CEOs of major American "" ""car companies would shake my hand and turn away because I wasnâ€™t "" ""worth talking to,â€ said Thrun, in an interview with Recode earlier "" ""this week."") doc = nlp(text) # Find named entities, phrases and concepts for entity in doc.ents: print(entity.text, entity.label_) Output:Sebastian Thrun PERSON2007 DATEAmerican NORPThrun PERSONRecode ORGearlier this week DATE In thisdata sciencetutorial, we looked at different methods for natural language processing, also abbreviated as NLP. We went through different preprocessing techniques to prepare our text to apply models and get insights from them. We discussed word vectors and why we use them in NLP. Then we used NER to identify entities and their labels in our text. If you want tolearn more about data scienceor become a data scientist, make sure to visitBeyond Machine. If you want to learn more about topics such asexecutive data scienceand data strategy, make sure to visitTesseract Academy.  Join my exclusivedata science programand get mentored personally by me. Dr Stylianos (Stelios) Kampakis is the CEO of The Data Scientist. He is a data scientist and tokenomics expert with more than 10 years of experience. He has worked with companies of all sizes: from startups to organisations like theUS Navy,VodafoneandBritish Land. His work expands multiple sectors including fintech, sports analytics, health-tech, general AI, medical statistics, predictive maintenance and others. He has worked with many different types of technologies, from statistical models, to deep learning, to large language models. He has 2 patents pending to his name, and has published 3 books on data science, AI and data strategy. He has helped many people follow a career in data science and technology. His seminal work in token economics has led to many successful token economic designs using tools such as agent based modelling and game theory. He is a member of the Royal Statistical Society, honorary research fellow at theUCL Centre for Blockchain Technologies, theCyprus Blockchain Centre, a data science advisor forLondon Business Schooland CEO ofThe Tesseract Academy. We are based inLondon, UKbut work with clients from all over the world. +44 7761712921 / Mon â€“ Fri, 8:00-22:00 We are based inLondon, UKbut work with clients from all over the world. +44 7761712921 / Mon â€“ Fri, 8:00-22:00 The Data Scientist, 85 Great Portland St, London W1W 7LT, United Kingdom"
https://medium.com/@vaniukov.s/nlp-vs-llm-a-comprehensive-guide-to-understanding-key-differences-0358f6571910,"Sign up Sign in Sign up Sign in Slava Vaniukov Follow -- 5 Listen Share The NLP and LLM technologies are central to the analysis and generation of human language on a large scale. With their growing prevalence, distinguishing between LLM vs NLP becomes increasingly important. NLP encompasses a suite of algorithms to understand, manipulate, and generate human language. Since its inception in the 1950s, NLP has evolved to analyze textual relationships. It uses part-of-speech tagging, named entity recognition, and sentiment analysis methods. As exemplified by OpenAIâ€™s ChatGPT, LLMs leverage deep learning to train on extensive text sets. Although they can mimic human-like text, their comprehension of languageâ€™s nuances is limited. Unlike NLP, which focuses on language analysis, LLMs primarily generate text. I am pleased to present this guide, offering a concise yet comprehensivecomparison of NLP and LLMs. We will explore the intricacies of these technologies, delve into their diverse applications, and examine their challenges. NLP facilitates machinesâ€™ understanding and engagement with human language in meaningful ways. It can be used for applications from spell-checking and auto-correction to chatbots and voice assistants. NLP is about creating algorithms that enable the generation of human language. It bridges the gap between digital systems and human communication. This technology paves the way for enhanced data analysis and insight across industries. Natural Language Processing relies on various processes to enable computers to produce human language: NLPâ€™s applications are extensive, influencing various sectors by: Despite progress, NLP encounters several hurdles that, if addressed, could refine its accuracy and integration into technology: Large Language Models offer a comprehensive approach to language tasks. They exhibit fluency and adaptability far beyond traditional Natural Language Processing systems. LLMs utilize a sophisticatedtech stack for generative AI, enabling them to: LLMs are characterized by several key attributes that set them apart: The effectiveness of Large Language Models is rooted in their foundational technologies: LLMs find application in a myriad of sectors, including: Despite their advanced capabilities, LLMs face limitations and ethical dilemmas that need careful consideration: NLP and LLM play pivotal roles in enhancing human-computer interaction through language. Although they share common objectives, there are several differences in their methodologies, capabilities, and application areas. Letâ€™s focus on NLP vs LLM performance, scalability, accuracy, and their utility across various sectors. NLP:Demonstrates high accuracy in specialized tasks such as syntax parsing and entity recognition. LLM:Excels at generating human-like text and managing a wide spectrum of language tasks. NLP:More efficient at executing specific tasks with lower computational demands. LLM:Highly scalable and adept at undertaking diverse tasks, albeit requiring greater computational resources. NLP:Exhibits high accuracy and reliability within specialized domains. May face challenges in tasks that require a rich understanding of context. LLM:Achieves reliability in producing coherent language output. It may also generate inaccurate or biased content influenced by its training data. NLP:Utilized for processing medical records, extracting pertinent patient information, and enabling predictive diagnostics. LLM:Facilitates patient interaction, disseminates information, and provides general medical advice. NLP: Applied in sentiment analysis, risk assessment, and enhancing customer service. It is particularly adept at processing financial language throughgenerative AI in banking. LLM:Useful for creating financial reports, conducting market analyses, and automating customer service interactions. NLP:Improves customer experience through chatbots, personalized recommendations, and analysis of customer feedback. LLM:Aids in generating content, managing large-scale customer interactions, and automating aspects of digital marketing. Fusing NLP and LLMs is a significant leap forward in developing advanced language processing systems. This collaboration combines NLPâ€™s precise capabilities with LLMâ€™s expansive contextual knowledge. It can also significantly improve AI applicationsâ€™ efficiency and effectiveness across industries. Integrating NLP with LLM technologies offers several key advantages: The collaborative potential of NLP and LLM has been demonstrated through various successful applications. Letâ€™s take a look at how this synergy can revolutionize AI applications: The continued integration of NLP and Large Language Models is expected to unlock new capabilities and applications. Undoubtedly, it will influence how we interact with AI technologies: While NLP vs LLMs each have unique approaches to processing human language â€” with NLP focusing on specific algorithmic modeling and LLMs on broad capabilities through massive pre-training â€” they complement each other well. Their integration promises richer AI interactions, deeper industry integration, and continuous AI ethics and technology advancements. Responsible development and application of these technologies remain paramount. As we look toward the future, the intersection of LLM and NLP is poised to usher in a new era of AI-driven solutions. For organizations interested in exploring the potential of NLP and LLM in their projects, Softermii offers expertise and support to harness these technologies effectively.Contact our team, and letâ€™s pave the way for innovative and ethical AI applications. -- -- 5 Co-Founder and CEO at Softermii, with over 9-years of experience in the web and mobile development industry and passion for traveling. Help Status About Careers Press Blog Privacy Terms Text to speech Teams"
https://medium.com/aimonks/roadmap-to-learn-natural-language-processing-in-2023-6e3a9372b8cc,"Sign up Sign in Sign up Sign in Gourav Didwania Follow ð€ðˆ ð¦ð¨ð§ð¤ð¬.ð¢ð¨ -- Listen Share Learning Natural Language Processing (NLP) can be a rewarding journey, but it can also be complex due to its multidisciplinary nature. Hereâ€™s a roadmap to help you get started and progress in your NLP learning journey Natural Language Processing (NLP) is a valuable area of study from a learning perspective because it bridges the gap between human communication and machines henceunlocking the power of words. It enables us to teach computers to understand, generate, and interact with human language. Learning NLP equips individuals with skills to analyze vast amounts of textual data, build intelligent chatbots, automate language-related tasks, and contribute to groundbreaking advancements in fields like artificial intelligence and linguistics. These techniques represent manual practices aimed at optimizing our text data for improved model performance. Letâ€™s delve into them with a more detailed understanding: Itâ€™s worth noting that these are just a few of the techniques discussed here, and staying updated with various methods is essential for continual learning and improvement. Textual data that isnâ€™t directly compatible with Machine Learning algorithms. Therefore, our initial task involves preprocessing this data before feeding it into our Machine Learning models. This step aims to familiarize ourselves with the fundamental processing techniques essential for tackling nearly every NLP challenge. Techniques such asTokenization, Lemmatization, Stemming, Parts of Speech (POS), Stopwords removal, and Punctuation removal are used. In this phase, we explore fundamental techniques for transforming our textual data into numerical vectors, making it suitable for Machine Learning algorithms. These techniques include: These methods are essential for converting text data into a format that Machine Learning algorithms can effectively process and analyze. At this stage, we delve into advanced techniques for converting words into vectors, enhancing our ability to represent and analyze textual data: These advanced methods empower us to represent text data in a more meaningful and context-aware manner, enabling improved performance in various Natural Language Processing tasks. Having completed the preceding steps, itâ€™s time to put our knowledge into practice by tackling a typical or straightforward NLP use case. This hands-on experience involves implementing machine learning algorithms such as the Naive Bayes or Support Vector Machine Classifier. By doing so, we gain a practical understanding of the concepts covered thus far, providing a solid foundation for comprehending the subsequent stages of our NLP journey. Weâ€™ll be covering a project with the tools and techniques we have learned this far. In this step, we now start exploring deep learning models for Natural Language Processing (NLP), gaining insights into their core architectures: P.S. You need to know an advanced level understanding ofArtificial Neural Network Understanding these deep learning models is crucial for more advanced NLP applications and lays the foundation for grasping subsequent concepts in the NLP learning journey. At this stage, weâ€™ll start using advanced text preprocessing techniques such as Word Embedding and Word2Vec that will empower us to tackle moderate-level projects in the field of Natural Language Processing (NLP) and establish ourselves as proficient practitioners: By mastering these advanced preprocessing techniques, we gain a competitive edge and the ability to undertake more complex NLP projects, solidifying our expertise in this domain. In this step, we delve into advanced NLP architectural components that expand our understanding of deep learning and its applications in NLP: By grasping these advanced architectural elements, weâ€™ll be well-equipped to tackle sophisticated NLP challenges and leverage cutting-edge techniques to enhance our NLP projects. In this step, we focus on mastering theTransformerarchitecture, a pivotal advancement in Natural Language Processing (NLP). Transformers are a groundbreaking architecture designed to address sequence-to-sequence tasks while efficiently handling long-range relationships within text data. They achieve this by leveraging self-attention models. Understanding Transformers is essential for staying at the forefront of NLP developments and effectively harnessing their capabilities for tasks like language translation, text generation, and question-answering systems. Mastery of Transformers marks a significant milestone in our NLP journey and weâ€™ll be able to cover most of the used cases effectively. In this step, we delve into advanced Transformer models, including: Comprehending these advanced Transformer models enhances our NLP expertise, enabling us to excel in a variety of NLP applications and stay up-to-date with the latest advancements in the field. While this NLP roadmap may seem like a lot at first glance, remember that weâ€™ll be covering each topic one by one, gradually mastering the intricacies of NLP. The journey may be challenging, but step by step, weâ€™ll build a solid foundation and become experts in the field of Natural Language Processing. Stay curious and keep learning! -- -- AImonks (https://medium.com/aimonks) is an AI-Educational Publication. Data Scientist @ Ola ðŸ“ˆ | MLOps enthusiast ðŸ¤– | Medium BloggerðŸ–‹ï¸ | Let's dive into the world of AI together!ðŸ’¡ Collaborate athttps://linktr.ee/gouravdidwania Help Status About Careers Press Blog Privacy Terms Text to speech Teams"
https://www.ibm.com/topics/natural-language-processing,"Editorial Lead, AI Models Writer Natural language processing (NLP) is a subfield of computer science andartificial intelligence (AI)that usesmachine learningto enable computers to understand and communicate with human language. NLP enables computers and digital devices to recognize, understand and generate text and speech by combining computational linguisticsâ€”the rule-based modeling of human languageâ€”together with statistical modeling, machine learning anddeep learning. NLP research has helped enable the era ofgenerative AI, from the communication skills oflarge language models(LLMs) to the ability of image generation models to understand requests. NLP is already part of everyday life for many, powering search engines, promptingchatbotsfor customer service with spoken commands, voice-operated GPS systems and question-answering digital assistants on smartphones such as Amazonâ€™s Alexa, Appleâ€™s Siri and Microsoftâ€™s Cortana. NLP also plays a growing role in enterprise solutions that help streamline and automate business operations, increase employee productivity and simplify business processes. NLP makes it easier for humans to communicate and collaborate with machines, by allowing them to do so in the natural human language they use every day. This offers benefits across many industries and applications. NLP is especially useful in fully or partiallyautomating taskslike customer support, data entry and document handling. For example, NLP-powered chatbots can handle routine customer queries, freeing up human agents for more complex issues. Indocument processing, NLP tools can automatically classify, extract key information and summarize content, reducing the time and errors associated with manual data handling. NLP facilitates language translation, converting text from one language to another while preserving meaning, context and nuances. NLP enhances data analysis by enabling the extraction of insights from unstructured text data, such as customer reviews, social media posts and news articles. By usingtext miningtechniques, NLP can identify patterns, trends and sentiments that are not immediately obvious in large datasets. Sentiment analysis enables theextraction of subjective qualitiesâ€”attitudes, emotions, sarcasm, confusion or suspicionâ€”from text. This is often used for routing communications to the system or the person most likely to make the next response. This allows businesses to better understand customer preferences, market conditions and public opinion. NLP tools can also perform categorization and summarization of vast amounts of text, making it easier for analysts to identify key information and make data-driven decisions more efficiently. NLP benefits search by enabling systems to understand the intent behind user queries, providing more accurate and contextually relevant results. Instead of relying solely on keyword matching, NLP-powered search engines analyze the meaning of words and phrases, making it easier to find information even when queries are vague or complex. This improves user experience, whether in web searches, document retrieval or enterprise data systems. NLP powers advanced language models tocreate human-like textfor various purposes. Pre-trained models, such as GPT-4, can generate articles, reports, marketing copy, product descriptions and even creative writing based on prompts provided by users. NLP-powered tools can also assist in automating tasks like drafting emails, writing social media posts or legal documentation. By understanding context, tone and style, NLP sees to it that the generated content is coherent, relevant and aligned with the intended message, saving time and effort in content creation while maintaining quality. Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. NLP combines the power of computational linguistics together withmachine learning algorithmsand deep learning. Computational linguistics uses data science to analyze language and speech. It includes two main types of analysis: syntactical analysis and semantical analysis. Syntactical analysis determines the meaning of a word, phrase or sentence by parsing the syntax of the words and applying preprogrammed rules of grammar. Semantical analysis uses the syntactic output to draw meaning from the words and interpret their meaning within the sentence structure. The parsing of words can take one of two forms. Dependency parsing looks at the relationships between words, such as identifying nouns and verbs, while constituency parsing then builds a parse tree (or syntax tree): a rooted and ordered representation of the syntactic structure of the sentence or string of words. The resulting parse trees underly the functions of language translators and speech recognition. Ideally, this analysis makes the outputâ€”either text or speechâ€”understandable to both NLP models and people. Self-supervised learning (SSL)in particular is useful for supporting NLP because NLP requires large amounts of labeled data to train AI models. Because these labeled datasets require time-consuming annotationâ€”a process involving manual labeling by humansâ€”gathering sufficient data can be prohibitively difficult. Self-supervised approaches can be more time-effective and cost-effective, as they replace some or all manually labeled training data.Three different approaches to NLP include: The earliest NLP applications were simple if-then decision trees, requiring preprogrammed rules. They are only able to provide answers in response to specific prompts, such as the original version of Moviefone, which had rudimentary natural language generation (NLG) capabilities. Because there is no machine learning or AI capability in rules-based NLP, this function is highly limited and not scalable. Developed later, statistical NLP automatically extracts, classifies and labels elements of text and voice data and then assigns a statistical likelihood to each possible meaning of those elements. This relies on machine learning, enabling a sophisticated breakdown of linguistics such as part-of-speech tagging.Statistical NLP introduced the essential technique of mapping language elementsâ€”such as words and grammatical rulesâ€”to a vector representation so that language can be modeled by using mathematical (statistical) methods, including regression or Markov models. This informed early NLP developments such as spellcheckers and T9 texting (Text on 9 keys, to be used on Touch-Tone telephones). Recently, deep learning models have become the dominant mode of NLP, by using huge volumes of raw,unstructureddataâ€”both text and voiceâ€”to become ever more accurate. Deep learning can be viewed as a further evolution of statistical NLP, with the difference that it usesneural networkmodels. There are several subcategories of models: Sequence-to-Sequence(seq2seq) models: Basedon recurrent neural networks (RNN), they have mostly been used for machine translation by converting a phrase from one domain (such as the German language) into the phrase of another domain (such as English). Transformer models: They usetokenizationof language (the position of each tokenâ€”words or subwords) and self-attention (capturing dependencies and relationships) to calculate the relation of different language parts to one another.Transformer modelscan be efficiently trained by usingself-supervised learningon massive text databases. A landmark intransformer modelswas Googleâ€™s bidirectional encoder representations from transformers (BERT), which became and remains the basis of how Googleâ€™s search engine works. Autoregressive models: This type of transformer model is trained specifically to predict the next word in a sequence, which represents a huge leap forward in the ability to generate text. Examples of autoregressive LLMs include GPT,Llama, Claude and the open-source Mistral. Foundation models: Prebuilt and curated foundation models can speed the launching of an NLP effort and boost trust in its operation. For example, theIBMÂ® Graniteâ„¢foundation models are widely applicable across industries. They support NLP tasks including content generation and insight extraction. Additionally, they facilitate retrieval-augmented generation, a framework for improving the quality of response by linking the model to external sources of knowledge. The models also perform named entity recognition which involves identifying and extracting key information in a text. Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights. Several NLP tasks typically help process human text and voice data in ways that help the computer make sense of what itâ€™s ingesting. Some of these tasks include: Coreference resolution Named entity recognition Part-of-speech tagging Word sense disambiguation This is the task of identifying if and when two words refer to the same entity. The most common example is determining the person or object to which a certain pronoun refers (such as â€œsheâ€ = â€œMaryâ€). But it can also identify a metaphor or an idiom in the text (such as an instance in which â€œbearâ€ isnâ€™t an animal, but a large and hairy person). NERidentifies words or phrases as useful entities. NER identifies â€œLondonâ€ as a location or â€œMariaâ€ as a person's name. Also called grammatical tagging, this is the process of determining which part of speech a word or piece of text is, based on its use and context. For example, part-of-speech identifies â€œmakeâ€ as a verb in â€œI can make a paper plane,â€ and as a noun in â€œWhat make of car do you own?â€ This is the selection of a word meaning for a word with multiple possible meanings. This uses a process of semanticanalysisto examine the word in context. For example, word sense disambiguation helps distinguish the meaning of the verb â€œmakeâ€ in â€œmake the gradeâ€ (to achieve) versus â€œmake a betâ€ (to place). Sorting out â€œI will be merry when I marry Maryâ€ requires a sophisticated NLP system. NLP works by combining various computational techniques to analyze, understand and generate human language in a way that machines can process. Here is an overview of a typical NLP pipeline and its steps: NLP text preprocessing prepares raw text for analysis by transforming it into a format that machines can more easily understand. It begins with tokenization, which involves splitting the text into smaller units like words, sentences or phrases. This helps break down complex text into manageable parts. Next, lowercasing is applied to standardize the text by converting all characters to lowercase, ensuring that words like ""Apple"" and ""apple"" are treated the same. Stop word removal is another common step, where frequently used words like ""is"" or ""the"" are filtered out because they don't add significant meaning to the text.Stemmingorlemmatizationreduces words to their root form (e.g., ""running"" becomes ""run""), making it easier to analyze language by grouping different forms of the same word. Additionally, text cleaning removes unwanted elements such as punctuation, special characters and numbers that may clutter the analysis. After preprocessing, the text is clean, standardized and ready for machine learning models to interpret effectively. Feature extraction is the process of converting raw text into numerical representations that machines can analyze and interpret. This involves transforming text into structured data by using NLP techniques likeBag of Wordsand TF-IDF, which quantify the presence and importance of words in a document. More advanced methods includeword embeddingslike Word2Vec or GloVe, which represent words as dense vectors in a continuous space, capturing semantic relationships between words. Contextual embeddings further enhance this by considering the context in which words appear, allowing for richer, more nuanced representations. Text analysis involves interpreting and extracting meaningful information from text data through various computational techniques. This process includes tasks such as part-of-speech (POS) tagging, which identifies grammatical roles of words and named entity recognition (NER), which detects specific entities like names, locations and dates. Dependency parsing analyzes grammatical relationships between words to understand sentence structure, while sentiment analysis determines the emotional tone of the text, assessing whether it is positive, negative or neutral. Topic modeling identifies underlying themes or topics within a text or across a corpus of documents. Natural language understanding (NLU) is a subset of NLP that focuses on analyzing the meaning behind sentences. NLU enables software to find similar meanings in different sentences or to process words that have different meanings. Through these techniques, NLP text analysis transforms unstructured text into insights. Processed data is then used to train machine learning models, which learn patterns and relationships within the data. During training, the model adjusts its parameters to minimize errors and improve its performance. Once trained, the model can be used to make predictions or generate outputs on new, unseen data. The effectiveness of NLP modeling is continually refined through evaluation, validation and fine-tuning to enhance accuracy and relevance in real-world applications. Different software environments are useful throughout the said processes. For example, the Natural Language Toolkit (NLTK) is a suite of libraries and programs for English that is written in the Python programming language. It supports text classification, tokenization, stemming, tagging, parsing and semantic reasoning functionalities. TensorFlow is a free and open-source software library for machine learning and AI that can be used to train models for NLP applications. Tutorials and certifications abound for those interested in familiarizing themselves with such tools. Even state-of-the-art NLP models are not perfect, just as human speech is prone to error. As with any AI technology, NLP comes with potential pitfalls. Human language is filled with ambiguities that make it difficult for programmers to write software that accurately determines the intended meaning of text or voice data. Human language might take years for humans to learnâ€”and many never stop learning. But then programmers must teach natural language-powered applications to recognize and understand irregularities so their applications can be accurate and useful.Associated risks might include: As with any AI function,biased dataused in training will skew the answers. The more diverse the users of an NLP function, the more significant this risk becomes, such as in government services, healthcare and HR interactions. Training datasets scraped from the web, for example, are prone to bias. As in programming, there is a risk of garbage in, garbage out (GIGO).Speech recognition, also known as speech-to-text, is the task of reliably converting voice data into text data. But NLP solutions can become confused if spoken input is in an obscure dialect, mumbled, too full of slang, homonyms, incorrect grammar, idioms, fragments, mispronunciations, contractions or recorded with too much background noise. New words are continually being invented or imported. The conventions of grammar can evolve or be intentionally broken. In these cases, NLP can either make a best guess or admit itâ€™s unsureâ€”and either way, this creates a complication. When people speak, their verbal delivery or even body language can give an entirely different meaning than the words alone. Exaggeration for effect, stressing words for importance or sarcasm can be confused by NLP, making the semantic analysis more difficult and less reliable. NLP applications can now be found across virtually every industry. In financial dealings, nanoseconds might make the difference between success and failure when accessing data, or making trades or deals. NLP can speed the mining of information from financial statements, annual and regulatory reports, news releases or even social media. New medical insights and breakthroughs can arrive faster than many healthcare professionals can keep up. NLP and AI-based tools can help speed the analysis of health records and medical research papers, making better-informed medical decisions possible, or assisting in the detection or even prevention of medical conditions. NLP can analyze claims to look for patterns that can identify areas of concern and find inefficiencies in claims processingâ€”leading to greater optimization of processing and employee efforts. Almost any legal case might require reviewing mounds of paperwork, background information and legal precedent. NLP can help automate legal discovery, assisting in the organization of information, speeding review and making sure that all relevant details are captured for consideration. Learn about the five key orchestration capabilities that can help organizations address the challenges of implementing generative AI effectively. Easily design scalable AI assistants and agents, automate repetitive tasks and simplify complex processes withÂ IBMÂ® watsonxâ„¢ OrchestrateÂ®. Accelerate the business value of artificial intelligence with a powerful and flexible portfolio of libraries, services and applications. Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value. IBMÂ® Graniteâ„¢ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options. Discover how natural language processing can help you to converse more naturally with computers. We surveyed 2,000 organizations about their AI initiatives to discover what's working, what's not and how you can get ahead. Explore IBM Developer's website to access blogs, articles, newsletters and learn more about IBM embeddable AI. Learn fundamental concepts and build your skills with hands-on labs, courses, guided projects, trials and more. Easily design scalable AI assistants and agents, automate repetitive tasks and simplify complex processes withÂ IBMÂ® watsonxâ„¢ OrchestrateÂ®."
https://www.geeksforgeeks.org/natural-language-processing-overview/,"The meaning of NLP is Natural Language Processing (NLP) which is a fascinating and rapidly evolving field that intersects computer science, artificial intelligence, and linguistics. NLP focuses on the interaction between computers and human language, enabling machines to understand, interpret, and generate human language in a way that is both meaningful and useful. With the increasing volume of text data generated every day, from social media posts to research articles, NLP has become an essential tool for extracting valuable insights and automating various tasks.  Natural Language Processing  In this article, we will explore the fundamental concepts and techniques of Natural Language Processing, shedding light on how it transforms raw text into actionable information. From tokenization and parsing to sentiment analysis and machine translation, NLP encompasses a wide range of applications that are reshaping industries and enhancing human-computer interactions. Whether you are a seasoned professional or new to the field, this overview will provide you with a comprehensive understanding of NLP and its significance in todayâ€™s digital age. Table of Content Natural language processing (NLP) is a field of computer science and a subfield of artificial intelligence that aims to make computers understand human language. NLP uses computational linguistics, which is the study of how language works, and various models based on statistics, machine learning, and deep learning. These technologies allow computers to analyze and process text or voice data, and to grasp their full meaning, including the speakerâ€™s or writerâ€™s intentions and emotions. NLP powers many applications that use language, such as text translation, voice recognition, text summarization, and chatbots. You may have used some of these applications yourself, such as voice-operated GPS systems, digital assistants, speech-to-text software, and customer service bots. NLP also helps businesses improve their efficiency, productivity, and performance by simplifying complex tasks that involve language. NLP encompasses a wide array of techniques that aimed at enabling computers to process and understand human language. These tasks can be categorized into several broad areas, each addressing different aspects of language processing. Here are some of the key NLP techniques: Working of Natural Language Processing  Working in natural language processing (NLP) typically involves using computational techniques to analyze and understand human language. This can include tasks such as language understanding, language generation, and language interaction. Preprocessing is crucial to clean and prepare the raw text data for analysis. Common preprocessing steps include: Extracting meaningful features from the text data that can be used for various NLP tasks. Selecting and training a machine learning or deep learning model to perform specific NLP tasks. Deploying the trained model and using it to make predictions or extract insights from new text data. Evaluating the performance of the NLP algorithm using metrics such as accuracy, precision, recall, F1-score, and others. Continuously improving the algorithm by incorporating new data, refining preprocessing techniques, experimenting with different models, and optimizing features. There are a variety of technologies related to natural language processing (NLP) that are used to analyze and understand human language. Some of the most common include: In conclusion, the field of Natural Language Processing (NLP) has significantly transformed the way humans interact with machines, enabling more intuitive and efficient communication. NLP encompasses a wide range of techniques and methodologies to understand, interpret, and generate human language. From basic tasks like tokenization and part-of-speech tagging to advanced applications like sentiment analysis and machine translation, the impact of NLP is evident across various domains. As the technology continues to evolve, driven by advancements in machine learning and artificial intelligence, the potential for NLP to enhance human-computer interaction and solve complex language-related challenges remains immense. Understanding the core concepts and applications of Natural Language Processing is crucial for anyone looking to leverage its capabilities in the modern digital landscape. NLP models are computational systems that can process natural language data, such as text or speech, and perform various tasks, such as translation, summarization, sentiment analysis, etc. NLP models are usually based on machine learning or deep learning techniques that learn from large amounts of language data. NLP models can be classified into two main types: rule-based and statistical. Rule-based models use predefined rules and dictionaries to analyze and generate natural language data. Statistical models use probabilistic methods and data-driven approaches to learn from language data and make predictions. NLP models face many challenges due to the complexity and diversity of natural language. Some of these challenges include ambiguity, variability, context-dependence, figurative language, domain-specificity, noise, and lack of labeled data. NLP models have many applications in various domains and industries, such as search engines, chatbots, voice assistants, social media analysis, text mining, information extraction, natural language generation, machine translation, speech recognition, text summarization, question answering, sentiment analysis, and more.  M "
https://www.deeplearning.ai/resources/natural-language-processing/,"âœ¨ New course! Enroll inReasoning with o1 Natural Language Processing (NLP) is one of the hottest areas of artificial intelligence (AI) thanks to applications like text generators that compose coherent essays, chatbots that fool people into thinking theyâ€™re sentient, and text-to-image programs that produce photorealistic images of anything you can describe. Recent years have brought arevolutionin the ability of computers to understand human languages, programming languages, and even biological and chemical sequences, such as DNA and protein structures, that resemble language. The latest AI models are unlocking these areas to analyze the meanings of input text and generate meaningful, expressive output. Natural language processing (NLP)is the discipline of building machines that can manipulate human language â€” or data that resembles human language â€” in the way that it is written, spoken, and organized. It evolved from computational linguistics, which uses computer science to understand the principles of language, but rather than developing theoretical frameworks, NLP is an engineering discipline that seeks to build technology to accomplish useful tasks. NLP can be divided into two overlapping subfields: natural language understanding (NLU), which focuses on semantic analysis or determining the intended meaning of text, and natural language generation (NLG), which focuses on text generation by a machine. NLP is separate from â€” but often used in conjunction with â€” speech recognition, which seeks to parse spoken language into words, turning sound into text and vice versa. NLP is an integral part of everyday life and becoming more so as language technology is applied to diverse fields like retailing (for instance, in customer service chatbots) and medicine (interpreting or summarizing electronic health records). Conversational agents such as Amazonâ€™sAlexaand Appleâ€™sSiriutilize NLP to listen to user queries and find answers. The most sophisticated such agents â€” such as GPT-3, which was recently opened forcommercial applicationsâ€” can generate sophisticated prose on a wide variety of topics as well as power chatbots that are capable of holding coherent conversations. Google uses NLP toimprove its search engine results, and social networks like Facebook use it to detect and filterhate speech. NLP is growing increasingly sophisticated, yet much work remains to be done. Current systems are prone to bias and incoherence, and occasionally behave erratically. Despite the challenges, machine learning engineers have many opportunities to apply NLP in ways that are ever more central to a functioning society. NLP is used for a wide variety of language-related tasks, including answering questions, classifying text in a variety of ways, and conversing with users. Here are 11 tasks that can be solved by NLP: NLP models work by finding relationships between the constituent parts of language â€”Â for example, the letters, words, and sentences found in a text dataset. NLP architectures use various methods for data preprocessing, feature extraction, and modeling. Some of these processes are: TF(word in a document)= Number of occurrences of that word in document / Number of words in document IDF(word in a corpus)=log(number of documents in the corpus / number of documents that include the word) A word is important if it occurs many times in a document. But that creates a problem. Words like â€œaâ€ and â€œtheâ€ appear often. And as such, their TF score will always be high. We resolve this issue by using Inverse Document Frequency, which is high if the word is rare and low if the word is common across the corpus. TheTF-IDFscore of a term is the product of TF and IDF. P(Wn)=P(Wn|Wnâˆ’1) Deep learning is also used to create such language models. Deep-learning models take as input a word embedding and, at each time state, return the probability distribution of the next word as the probability for every word in the dictionary. Pre-trained language models learn the structure of a particular language by processing a large corpus, such as Wikipedia. They can then be fine-tuned for a particular task. For instance, BERT has been fine-tuned for tasks ranging fromfact-checkingtowriting headlines. Most of the NLP tasks discussed above can be modeled by a dozen or so general techniques. Itâ€™s helpful to think of these techniques in two categories: Traditional machine learning methods and deep learning methods. Traditional Machine learning NLP techniques: P(label | text) = P(label) x P(text|label) / P(text) and predicts based on which joint distribution has the highest probability. The naive assumption in the Naive Bayes model is that the individual words are independent. Thus: P(text|label) = P(word_1|label)*P(word_2|label)*â€¦P(word_n|label) In NLP, such statistical methods can be applied to solve problems such as spam detection orfinding bugs in software code. Deep learning NLP Techniques: Over the years, many NLP models have made waves within the AI community, and some have even made headlines in the mainstream news. The most famous of these have been chatbots and language models. Here are some of them: Many languages and libraries support NLP. Here are a few of the most useful. NLP has been at the center of a number of controversies. Some are centered directly on the models and their outputs, others on second-order concerns, such as who has access to these systems, and how training them impacts the natural world. â€œNonsense on stiltsâ€: Writer Gary Marcus has criticized deep learning-based NLP for generating sophisticated language that misleads users to believe that natural language algorithms understand what they are saying and mistakenly assume they are capable of more sophisticated reasoning than is currently possible. If you are just starting out, many excellent courses can help. If you want to learn more about NLP, try reading research papers. Work through the papers that introduced the models and techniques described in this article. Most are easy to find onarxiv.org. You might also take a look at these resources: We highly recommend learning to implementbasic algorithms(linear and logistic regression, Naive Bayes, decision trees, and vanilla neural networks) in Python. The next step is to take an open-source implementation and adapt it to a new dataset or task. NLP is one of the fast-growing research domains in AI, with applications that involve tasks including translation, summarization, text generation, and sentiment analysis. Businesses use NLP to power a growing number of applications, both internal â€”Â likedetecting insurance fraud,determining customer sentiment, andoptimizing aircraft maintenance â€”Â and customer-facing, likeGoogle Translate. Aspiring NLP practitioners can begin by familiarizing themselves with foundational AI skills: performing basic mathematics, coding in Python, and using algorithms like decision trees, Naive Bayes, and logistic regression. Online courses can help you build your foundation. They can also help as you proceed into specialized topics. Specializing in NLP requires a working knowledge of things like neural networks, frameworks like PyTorch and TensorFlow, and various data preprocessing techniques. The transformer architecture, which has revolutionized the field since it was introduced in 2017, is an especially important architecture. NLP is an exciting and rewarding discipline, and has potential to profoundly impact the world in many positive ways. Unfortunately, NLP is also the focus of several controversies, and understanding them is also part of being a responsible practitioner. For instance, researchers have found that models will parrot biased language found in their training data, whether theyâ€™re counterfactual, racist, or hateful. Moreover, sophisticated language models can be used to generate disinformation. A broader concern is that training large models produces substantial greenhouse gas emissions. This page is only a brief overview of what NLP is all about. If you have an appetite for more, DeepLearning.AI offers courses for everyone in their NLP journey, fromAI beginnersand those who areready to specialize. No matter your current level of expertise or aspirations, remember to keep learning!"
https://www.javatpoint.com/nlp,"NLP tutorial provides basic and advanced concepts of the NLP tutorial. Our NLP tutorial is designed for beginners and professionals. NLP stands forNatural Language Processing, which is a part ofComputer Science, Human language,andArtificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages. It helps developers to organize knowledge for performing tasks such astranslation, automatic summarization, Named Entity Recognition (NER), speech recognition, relationship extraction,andtopic segmentation. (1940-1960) - Focused on Machine Translation (MT) The Natural Languages Processing started in the year 1940s. 1948- In the Year 1948, the first recognisable NLP application was introduced in Birkbeck College, London. 1950s- In the Year 1950s, there was a conflicting view between linguistics and computer science. Now, Chomsky developed his first book syntactic structures and claimed that language is generative in nature. In 1957, Chomsky also introduced the idea of Generative Grammar, which is rule based descriptions of syntactic structures. (1960-1980) - Flavored with Artificial Intelligence (AI) In the year 1960 to 1980, the key developments were: Augmented Transition Networks (ATN) Augmented Transition Networks is a finite state machine that is capable of recognizing regular languages. Case Grammar Case Grammar was developed byLinguist Charles J. Fillmorein the year 1968. Case Grammar uses languages such as English to express the relationship between nouns and verbs by using the preposition. In Case Grammar, case roles can be defined to link certain kinds of verbs and objects. For example:""Neha broke the mirror with the hammer"". In this example case grammar identify Neha as an agent, mirror as a theme, and hammer as an instrument. In the year 1960 to 1980, key systems were: SHRDLU SHRDLU is a program written byTerry Winogradin 1968-70. It helps users to communicate with the computer and moving objects. It can handle instructions such as ""pick up the green boll"" and also answer the questions like ""What is inside the black box."" The main importance of SHRDLU is that it shows those syntax, semantics, and reasoning about the world that can be combined to produce a system that understands a natural language. LUNAR LUNAR is the classic example of a Natural Language database interface system that is used ATNs and Woods' Procedural Semantics. It was capable of translating elaborate natural language expressions into database queries and handle 78% of requests without errors. 1980 - Current Till the year 1980, natural language processing systems were based on complex sets of hand-written rules. After 1980, NLP introduced machine learning algorithms for language processing. In the beginning of the year 1990s, NLP started growing faster and achieved good process accuracy, especially in English Grammar. In 1990 also, an electronic text introduced, which provided a good resource for training and examining natural language programs. Other factors may include the availability of computers with fast CPUs and more memory. The major factor behind the advancement of natural language processing was the Internet. Now, modern NLP consists of various applications, likespeech recognition, machine translation,andmachine text reading. When we combine all these applications then it allows the artificial intelligence to gain knowledge of the world. Let's consider the example of AMAZON ALEXA, using this robot you can ask the question to Alexa, and it will reply to you. A list of disadvantages of NLP is given below: There are the following two components of NLP - 1. Natural Language Understanding (NLU) Natural Language Understanding (NLU) helps the machine to understand and analyse human language by extracting the metadata from content such as concepts, entities, keywords, emotion, relations, and semantic roles. NLU mainly used in Business applications to understand the customer's problem in both spoken and written language. NLU involves the following tasks - 2. Natural Language Generation (NLG) Natural Language Generation (NLG) acts as a translator that converts the computerized data into natural language representation. It mainly involves Text planning, Sentence planning, and Text Realization. Difference between NLU and NLG There are the following applications of NLP - 1. Question Answering Question Answering focuses on building systems that automatically answer the questions asked by humans in a natural language. 2. Spam Detection Spam detection is used to detect unwanted e-mails getting to a user's inbox. 3. Sentiment Analysis Sentiment Analysis is also known asopinion mining. It is used on the web to analyse the attitude, behaviour, and emotional state of the sender. This application is implemented through a combination of NLP (Natural Language Processing) and statistics by assigning the values to the text (positive, negative, or natural), identify the mood of the context (happy, sad, angry, etc.) 4. Machine Translation Machine translation is used to translate text or speech from one natural language to another natural language. Example:Google Translator 5. Spelling correction Microsoft Corporation provides word processor software like MS-word, PowerPoint for the spelling correction. 6. Speech Recognition Speech recognition is used for converting spoken words into text. It is used in applications, such as mobile, home automation, video recovery, dictating to Microsoft Word, voice biometrics, voice user interface, and so on. 7. Chatbot Implementing the Chatbot is one of the important applications of NLP. It is used by many companies to provide the customer's chat services. 8. Information extraction Information extraction is one of the most important applications of NLP. It is used for extracting structured information from unstructured or semi-structured machine-readable documents. 9. Natural Language Understanding (NLU) It converts a large set of text into more formal representations such as first-order logic structures that are easier for the computer programs to manipulate notations of the natural language processing. There are the following steps to build an NLP pipeline - Step1: Sentence Segmentation Sentence Segment is the first step for building the NLP pipeline. It breaks the paragraph into separate sentences. Example:Consider the following paragraph - Independence Day is one of the important festivals for every Indian citizen. It is celebrated on the 15th of August each year ever since India got independence from the British rule. The day celebrates independence in the true sense. Sentence Segment produces the following result: Step2: Word Tokenization Word Tokenizer is used to break the sentence into separate words or tokens. Example: JavaTpoint offers Corporate Training, Summer Training, Online Training, and Winter Training. Word Tokenizer generates the following result: ""JavaTpoint"", ""offers"", ""Corporate"", ""Training"", ""Summer"", ""Training"", ""Online"", ""Training"", ""and"", ""Winter"", ""Training"", ""."" Step3: Stemming Stemming is used to normalize words into its base form or root form. For example, celebrates, celebrated and celebrating, all these words are originated with a single root word ""celebrate."" The big problem with stemming is that sometimes it produces the root word which may not have any meaning. For Example,intelligence, intelligent, and intelligently, all these words are originated with a single root word ""intelligen."" In English, the word ""intelligen"" do not have any meaning. Step 4: Lemmatization Lemmatization is quite similar to the Stamming. It is used to group different inflected forms of the word, called Lemma. The main difference between Stemming and lemmatization is that it produces the root word, which has a meaning. For example:In lemmatization, the words intelligence, intelligent, and intelligently has a root word intelligent, which has a meaning. Step 5: Identifying Stop Words In English, there are a lot of words that appear very frequently like ""is"", ""and"", ""the"", and ""a"". NLP pipelines will flag these words as stop words.Stop wordsmight be filtered out before doing any statistical analysis. Example:Heis agood boy. Step 6: Dependency Parsing Dependency Parsing is used to find that how all the words in the sentence are related to each other. Step 7: POS tags POS stands for parts of speech, which includes Noun, verb, adverb, and Adjective. It indicates that how a word functions with its meaning as well as grammatically within the sentences. A word has one or more parts of speech based on the context in which it is used. Example: ""Google""something on the Internet. In the above example, Google is used as a verb, although it is a proper noun. Step 8: Named Entity Recognition (NER) Named Entity Recognition (NER) is the process of detecting the named entity such as person name, movie name, organization name, or location. Example: Steve Jobsintroduced iPhone at the Macworld Conference in San Francisco, California. Step 9: Chunking Chunking is used to collect the individual piece of information and grouping them into bigger pieces of sentences. There are the following five phases of NLP: 1. Lexical Analysis and Morphological The first phase of NLP is the Lexical Analysis. This phase scans the source code as a stream of characters and converts it into meaningful lexemes. It divides the whole text into paragraphs, sentences, and words. 2. Syntactic Analysis (Parsing) Syntactic Analysis is used to check grammar, word arrangements, and shows the relationship among the words. Example:Agra goes to the Poonam  3. Semantic Analysis Semantic analysis is concerned with the meaning representation. It mainly focuses on the literal meaning of words, phrases, and sentences. 4. Discourse Integration Discourse Integration depends upon the sentences that proceeds it and also invokes the meaning of the sentences that follow it. 5. Pragmatic Analysis Pragmatic is the fifth and last phase of NLP. It helps you to discover the intended effect by applying a set of rules that characterize cooperative dialogues. For Example:""Open the door"" is interpreted as a request instead of an order. NLP is difficult because Ambiguity and Uncertainty exist in the language. Ambiguity There are the following three ambiguity - Lexical Ambiguity exists in the presence of two or more possible meanings of the sentence within a single word. Example: Manya is looking for amatch. In the above example, the word match refers to that either Manya is looking for a partner or Manya is looking for a match. (Cricket or other match) Syntactic Ambiguity exists in the presence of two or more possible meanings within the sentence. Example: I saw the girl with the binocular. In the above example, did I have the binoculars? Or did the girl have the binoculars? Referential Ambiguity exists when you are referring to something using the pronoun. Example:Kiran went to Sunita. She said, ""I am hungry."" In the above sentence, you do not know that who is hungry, either Kiran or Sunita. Natural Language Processing APIs allow developers to integrate human-to-machine communications and complete several useful tasks such as speech recognition, chatbots, spelling correction, sentiment analysis, etc. A list of NLP APIs is given below: Scikit-learn:It provides a wide range of algorithms for building machine learning models in Python. Natural language Toolkit (NLTK):NLTK is a complete toolkit for all NLP techniques. Pattern:It is a web mining module for NLP and machine learning. TextBlob:It provides an easy interface to learn basic NLP tasks like sentiment analysis, noun phrase extraction, or pos-tagging. Quepy:Quepy is used to transform natural language questions into queries in a database query language. SpaCy:SpaCy is an open-source NLP library which is used for Data Extraction, Data Analysis, Sentiment Analysis, and Text Summarization. Gensim:Gensim works with large datasets and processes data streams. Before learning NLP, you must have the basic knowledge of Python. Our NLP tutorial is designed to help beginners. We assure that you will not find any problem in this NLP tutorial. But if there is any mistake or error, please post the error in the contact form. We provides tutorials and interview questions of all technology like java tutorial, android, java frameworks G-13, 2nd Floor, Sec-3, Noida, UP, 201301, India [emailÂ protected]. Latest Post PRIVACY POLICY"
https://www.kellton.com/kellton-tech-blog/natural-language-processing-in-ai,"Reach out, we'd love to hear from you! NLP stands for Natural Language Processing, and this phenomenal innovation is, in many ways, driving the future of AI. To begin with, NLP technology is actively transforming how we interact with machines, automate tasks, and drive innovation. As machines and computers become more comfortable interacting with humans, it will set the stage for smoother human-machine interactions, seamless multilingual communication, and increased automation and innovation. Expect more advanced chatbots, improved healthcare diagnostics and treatments, and an increased thrust on ethical AI. Nearly all industries, from healthcare to insurance and retail to manufacturing, stand to benefit from the evolution of natural language processing in AI. In many ways, NLP and the rest of the AI stack are building a new world - a world where machines are trained to comprehend humans and respond appropriately. Forward-looking organizations, from tech startups to established enterprises, are increasingly investing in NLP-powered apps and systems to streamline operations and drive productivity and business results. In fact,Grand View Researchstates, â€œThe global NLP market size was close to USD 27.73 billion in 2022 and is likely to grow at an impressive CAGR of 40.4% from 2023 to 2030.â€ The rapid growth in the NLP space is a testament to the fact that businesses across the globe are willing to invest in this technology. The growth in NLP will also help push the existing boundaries of Artificial Intelligence and make AI a far more precious asset in the future. Thatâ€™s what weâ€™ll focus on in this blog. Weâ€™ll learn about the fundamentals of NLP. More importantly, weâ€™ll look into ways this technology will helpbuild a new era of AI. Letâ€™s start with what natural language processing (NLP) means. Natural language processing (NLP) is a pivotal innovation in modern AI. The simplest way to understand NLP is to imagine a bridge connecting humans with machines at a far deeper level than ever before. We use NLP in numerous real-life situations. So, when you interact with a chatbox installed on a website, with voice assistants such as Siri and Alexa, or with tools translating languages in real-time, you are using NLP-powered apps and systems. NLP uses an ever-increasing number of techniques to understand, process, and generate human language. The most common natural language processing techniques are tokenization, stemming and lemmatization, and named entity recognition (NER).  Revenues from the Natural Language Processing (NLP) market worldwide from 2017 to 2025 (in million U.S. dollars) The entire ecosystem of Natural language processing (NLP) thrives on a multitude of techniques, such as tokenization and transformer models. These natural language processing techniques supercharge an ever-growing number of use cases, fromhighly interactive chatbots to sentiment analysis. Letâ€™s take a quick look at some of the most common natural language processing techniques: In addition to tokenization, named entity recognition, stemming, and lemmatization, AI apps, and systems development companies use several other NLP techniques, such as text classification, sentiment analysis, and text summarization. Weâ€™ve now familiarized ourselves with some popular natural language processing techniques. Now, letâ€™s explore another key aspect of NLP: how it differs fromcore AI technology. Yes, NLP is a type of AI, but itâ€™s also evolved into a world of its own. NLP and AI are related in more than one way. One strengthens the other. However, it does not mean that NLP and AI are the same. Itâ€™s essential to understand what differentiates them from each other. Let us share a quick comparison table that explores the key differences between NLP and AI.  Whether you know it or not, NLP has entered our lives, and we use it like every day of our lives. Here are some of the examples of NLP in action: We have shared just the tip of the iceberg regarding how NLP is becoming an essential part of our lives. However, you must have a gist of how NLP impacts us all. Now, letâ€™s get down to the value that the proper applications of NLP solutions can generate. Natural language processing, or NLP, has numerous use cases across nearly all industries. However, the most common uses of NLP in the business world include:  Natural language processing is a powerful technology, which is increasingly driving innovation across the AI landscape. Nearly every industry stands to benefit from advancements in natural language processing in AI, which will eventually make machines more humane and beneficial for our world. To harness NLP's full value and drive business forward, you must strategically build, buy, and integrate NLP-powered solutions within your IT infrastructure. Thatâ€™s where an AI-first technology consulting partner, such as Kellton, can help you navigate the complex landscape of NLP with greater clarity and confidence. North America:+1.844.469.8900 Asia:+91.124.469.8900 Europe:+44.203.807.6911 Email:ask@kellton.com Â© 2024Â Kellton"
https://www.researchgate.net/publication/373398043_NATURAL_LANGUAGE_PROCESSING_TRANSFORMING_HOW_MACHINES_UNDERSTAND_HUMAN_LANGUAGE,Error: 403 Client Error: Forbidden for url: https://www.researchgate.net/publication/373398043_NATURAL_LANGUAGE_PROCESSING_TRANSFORMING_HOW_MACHINES_UNDERSTAND_HUMAN_LANGUAGE
https://online.york.ac.uk/the-role-of-natural-language-processing-in-ai/,Error: 403 Client Error: Forbidden for url: https://online.york.ac.uk/the-role-of-natural-language-processing-in-ai/
https://skillfloor.com/blog/the-role-of-natural-language-processing-nlp-in-ai-applications,"Alagar RNov 18, 2024043 Bruhitha Reddy GOct 7, 2024049 Alagar ROct 7, 2024049 Alagar ROct 4, 2024038 Bruhitha Reddy GOct 7, 2024049 Alagar RJul 25, 2024082 Alagar RJul 23, 2024072 Alagar ROct 7, 2024049 Alagar RSep 28, 20240185 Alagar RJul 27, 2024086 Alagar RNov 18, 2024043 Alagar ROct 4, 2024038 Alagar RAug 4, 20240333 Alagar RSep 30, 20240627 Alagar RJul 29, 2024068 Alagar RJul 22, 20240129 Alagar RJul 20, 20240111 Alagar RJul 26, 2024074 Alagar RJul 24, 20240129 Alagar RJul 22, 2024097 Alagar RJul 21, 2024082 Kalpana KadirvelDec 14, 202408 Alagar RDec 3, 2024021 Ajithkumar K GNov 19, 2024046 Alagar RNov 14, 2024042 Bruhitha Reddy GNov 12, 2024074 FathimaDec 10, 2024049 FathimaDec 8, 2024028 FathimaDec 7, 2024038 FathimaDec 5, 2024019 FathimaNov 26, 2024026 Alagar RDec 15, 2024013 Bruhitha Reddy GNov 25, 2024034 Ajithkumar K GNov 10, 2024049 Alagar ROct 4, 2024045 Alagar RNov 21, 20230180 Alagar RDec 19, 202401 Nikhil HegdeDec 13, 2024010 Kalpana KadirvelDec 1, 2024024 Kalpana KadirvelNov 30, 2024023 Nikhil HegdeNov 29, 2024015 Alagar RDec 17, 202407 Bruhitha Reddy GDec 16, 2024010 Bruhitha Reddy GDec 12, 202409 Bruhitha Reddy GDec 11, 202408 Alagar RDec 9, 202409 Join our subscribers list to get the latest news, updates and special offers directly in your inbox In the realm of AI, Natural Language Processing (NLP) is the pivotal technology that enables machines to understand, interpret, and generate human language. Its significance lies in its ability to bridge the communication gap between humans and machines. NLP empowers AI applications to process, analyze, and respond to natural language, making it a fundamental component in various domains, from chatbots and virtual assistants to sentiment analysis and machine translation. This introduction sets the stage for exploring how NLP plays a vital role in shaping AI applications.Foundations of NLPLinguistic principles are at the core ofNatural Language Processing (NLP). Understanding syntax, semantics, and pragmatics enables NLP systems to decipher language nuances. This knowledge helps in tasks like parsing sentences, identifying entities, and extracting meaning from text.Data preprocessing and tokenization are crucial steps in NLP. Raw text needs to be cleaned, standardized, and organized for effective analysis. Tokenization involves breaking down sentences into smaller units like words or phrases, allowing algorithms to process language effectively.Key NLP libraries and frameworks provide the tools necessary for building NLP applications. Libraries like NLTK, SpaCy, and frameworks like TensorFlow and PyTorch offer pre-built functions and models, expediting the development process and ensuring robust NLP solutions.NLP Techniques and ComponentsNatural Language Processing (NLP) encompasses a range of techniques and components that enable machines to process and understand human language. This includes text classification and sentiment analysis, which involve categorizing text into predefined classes and discerning emotions or opinions expressed. Named Entity Recognition (NER) identifies and extracts specific entities like names, locations, and organizations from text. Part-of-Speech (POS) tagging assigns grammatical labels to words, aiding in syntactic analysis. Additionally, dependency parsing establishes relationships between words in a sentence, crucial for understanding sentence structure. Machine translation and language generation are pivotal in tasks like automatic translation and content creation, respectively, demonstrating the diverse capabilities within NLP. These NLP techniques and components form the core building blocks for a wide range of AI applications, facilitating natural language understanding and generation.NLP Models and FrameworksNatural Language Processing (NLP) has seen a significant boost in recent years thanks to the emergence of powerful pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models, with their massive neural architectures, have demonstrated remarkable capabilities in understanding and generating human-like text.Training and fine-tuning NLP models is a critical aspect of their utility. During training, these models are exposed to vast amounts of text data to learn language patterns and semantics. Fine-tuning involves adapting these pre-trained models to specific tasks, making them versatile for various applications, from sentiment analysis to language translation.Transfer learning is a core principle in NLP. It allows the knowledge acquired by a model from one task or domain to be applied to another. This approach significantly reduces the need for extensive labeled data for each new task, making NLP more accessible and efficient. Transfer learning has opened doors to rapid advancements and innovations in the field, making NLP a pivotal technology in AI applications.NLP in Real-World ApplicationsNLP plays a pivotal role in numerous real-world applications, demonstrating its versatility and impact in various domains:Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency.Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies.Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach.Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes.Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks.In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Linguistic principles are at the core ofNatural Language Processing (NLP). Understanding syntax, semantics, and pragmatics enables NLP systems to decipher language nuances. This knowledge helps in tasks like parsing sentences, identifying entities, and extracting meaning from text.Data preprocessing and tokenization are crucial steps in NLP. Raw text needs to be cleaned, standardized, and organized for effective analysis. Tokenization involves breaking down sentences into smaller units like words or phrases, allowing algorithms to process language effectively.Key NLP libraries and frameworks provide the tools necessary for building NLP applications. Libraries like NLTK, SpaCy, and frameworks like TensorFlow and PyTorch offer pre-built functions and models, expediting the development process and ensuring robust NLP solutions.NLP Techniques and ComponentsNatural Language Processing (NLP) encompasses a range of techniques and components that enable machines to process and understand human language. This includes text classification and sentiment analysis, which involve categorizing text into predefined classes and discerning emotions or opinions expressed. Named Entity Recognition (NER) identifies and extracts specific entities like names, locations, and organizations from text. Part-of-Speech (POS) tagging assigns grammatical labels to words, aiding in syntactic analysis. Additionally, dependency parsing establishes relationships between words in a sentence, crucial for understanding sentence structure. Machine translation and language generation are pivotal in tasks like automatic translation and content creation, respectively, demonstrating the diverse capabilities within NLP. These NLP techniques and components form the core building blocks for a wide range of AI applications, facilitating natural language understanding and generation.NLP Models and FrameworksNatural Language Processing (NLP) has seen a significant boost in recent years thanks to the emergence of powerful pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models, with their massive neural architectures, have demonstrated remarkable capabilities in understanding and generating human-like text.Training and fine-tuning NLP models is a critical aspect of their utility. During training, these models are exposed to vast amounts of text data to learn language patterns and semantics. Fine-tuning involves adapting these pre-trained models to specific tasks, making them versatile for various applications, from sentiment analysis to language translation.Transfer learning is a core principle in NLP. It allows the knowledge acquired by a model from one task or domain to be applied to another. This approach significantly reduces the need for extensive labeled data for each new task, making NLP more accessible and efficient. Transfer learning has opened doors to rapid advancements and innovations in the field, making NLP a pivotal technology in AI applications.NLP in Real-World ApplicationsNLP plays a pivotal role in numerous real-world applications, demonstrating its versatility and impact in various domains:Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency.Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies.Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach.Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes.Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks.In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Data preprocessing and tokenization are crucial steps in NLP. Raw text needs to be cleaned, standardized, and organized for effective analysis. Tokenization involves breaking down sentences into smaller units like words or phrases, allowing algorithms to process language effectively.Key NLP libraries and frameworks provide the tools necessary for building NLP applications. Libraries like NLTK, SpaCy, and frameworks like TensorFlow and PyTorch offer pre-built functions and models, expediting the development process and ensuring robust NLP solutions.NLP Techniques and ComponentsNatural Language Processing (NLP) encompasses a range of techniques and components that enable machines to process and understand human language. This includes text classification and sentiment analysis, which involve categorizing text into predefined classes and discerning emotions or opinions expressed. Named Entity Recognition (NER) identifies and extracts specific entities like names, locations, and organizations from text. Part-of-Speech (POS) tagging assigns grammatical labels to words, aiding in syntactic analysis. Additionally, dependency parsing establishes relationships between words in a sentence, crucial for understanding sentence structure. Machine translation and language generation are pivotal in tasks like automatic translation and content creation, respectively, demonstrating the diverse capabilities within NLP. These NLP techniques and components form the core building blocks for a wide range of AI applications, facilitating natural language understanding and generation.NLP Models and FrameworksNatural Language Processing (NLP) has seen a significant boost in recent years thanks to the emergence of powerful pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models, with their massive neural architectures, have demonstrated remarkable capabilities in understanding and generating human-like text.Training and fine-tuning NLP models is a critical aspect of their utility. During training, these models are exposed to vast amounts of text data to learn language patterns and semantics. Fine-tuning involves adapting these pre-trained models to specific tasks, making them versatile for various applications, from sentiment analysis to language translation.Transfer learning is a core principle in NLP. It allows the knowledge acquired by a model from one task or domain to be applied to another. This approach significantly reduces the need for extensive labeled data for each new task, making NLP more accessible and efficient. Transfer learning has opened doors to rapid advancements and innovations in the field, making NLP a pivotal technology in AI applications.NLP in Real-World ApplicationsNLP plays a pivotal role in numerous real-world applications, demonstrating its versatility and impact in various domains:Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency.Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies.Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach.Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes.Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks.In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Key NLP libraries and frameworks provide the tools necessary for building NLP applications. Libraries like NLTK, SpaCy, and frameworks like TensorFlow and PyTorch offer pre-built functions and models, expediting the development process and ensuring robust NLP solutions.NLP Techniques and ComponentsNatural Language Processing (NLP) encompasses a range of techniques and components that enable machines to process and understand human language. This includes text classification and sentiment analysis, which involve categorizing text into predefined classes and discerning emotions or opinions expressed. Named Entity Recognition (NER) identifies and extracts specific entities like names, locations, and organizations from text. Part-of-Speech (POS) tagging assigns grammatical labels to words, aiding in syntactic analysis. Additionally, dependency parsing establishes relationships between words in a sentence, crucial for understanding sentence structure. Machine translation and language generation are pivotal in tasks like automatic translation and content creation, respectively, demonstrating the diverse capabilities within NLP. These NLP techniques and components form the core building blocks for a wide range of AI applications, facilitating natural language understanding and generation.NLP Models and FrameworksNatural Language Processing (NLP) has seen a significant boost in recent years thanks to the emergence of powerful pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models, with their massive neural architectures, have demonstrated remarkable capabilities in understanding and generating human-like text.Training and fine-tuning NLP models is a critical aspect of their utility. During training, these models are exposed to vast amounts of text data to learn language patterns and semantics. Fine-tuning involves adapting these pre-trained models to specific tasks, making them versatile for various applications, from sentiment analysis to language translation.Transfer learning is a core principle in NLP. It allows the knowledge acquired by a model from one task or domain to be applied to another. This approach significantly reduces the need for extensive labeled data for each new task, making NLP more accessible and efficient. Transfer learning has opened doors to rapid advancements and innovations in the field, making NLP a pivotal technology in AI applications.NLP in Real-World ApplicationsNLP plays a pivotal role in numerous real-world applications, demonstrating its versatility and impact in various domains:Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency.Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies.Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach.Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes.Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks.In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Natural Language Processing (NLP) encompasses a range of techniques and components that enable machines to process and understand human language. This includes text classification and sentiment analysis, which involve categorizing text into predefined classes and discerning emotions or opinions expressed. Named Entity Recognition (NER) identifies and extracts specific entities like names, locations, and organizations from text. Part-of-Speech (POS) tagging assigns grammatical labels to words, aiding in syntactic analysis. Additionally, dependency parsing establishes relationships between words in a sentence, crucial for understanding sentence structure. Machine translation and language generation are pivotal in tasks like automatic translation and content creation, respectively, demonstrating the diverse capabilities within NLP. These NLP techniques and components form the core building blocks for a wide range of AI applications, facilitating natural language understanding and generation.NLP Models and FrameworksNatural Language Processing (NLP) has seen a significant boost in recent years thanks to the emergence of powerful pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models, with their massive neural architectures, have demonstrated remarkable capabilities in understanding and generating human-like text.Training and fine-tuning NLP models is a critical aspect of their utility. During training, these models are exposed to vast amounts of text data to learn language patterns and semantics. Fine-tuning involves adapting these pre-trained models to specific tasks, making them versatile for various applications, from sentiment analysis to language translation.Transfer learning is a core principle in NLP. It allows the knowledge acquired by a model from one task or domain to be applied to another. This approach significantly reduces the need for extensive labeled data for each new task, making NLP more accessible and efficient. Transfer learning has opened doors to rapid advancements and innovations in the field, making NLP a pivotal technology in AI applications.NLP in Real-World ApplicationsNLP plays a pivotal role in numerous real-world applications, demonstrating its versatility and impact in various domains:Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency.Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies.Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach.Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes.Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks.In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Natural Language Processing (NLP) has seen a significant boost in recent years thanks to the emergence of powerful pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models, with their massive neural architectures, have demonstrated remarkable capabilities in understanding and generating human-like text.Training and fine-tuning NLP models is a critical aspect of their utility. During training, these models are exposed to vast amounts of text data to learn language patterns and semantics. Fine-tuning involves adapting these pre-trained models to specific tasks, making them versatile for various applications, from sentiment analysis to language translation.Transfer learning is a core principle in NLP. It allows the knowledge acquired by a model from one task or domain to be applied to another. This approach significantly reduces the need for extensive labeled data for each new task, making NLP more accessible and efficient. Transfer learning has opened doors to rapid advancements and innovations in the field, making NLP a pivotal technology in AI applications.NLP in Real-World ApplicationsNLP plays a pivotal role in numerous real-world applications, demonstrating its versatility and impact in various domains:Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency.Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies.Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach.Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes.Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks.In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Training and fine-tuning NLP models is a critical aspect of their utility. During training, these models are exposed to vast amounts of text data to learn language patterns and semantics. Fine-tuning involves adapting these pre-trained models to specific tasks, making them versatile for various applications, from sentiment analysis to language translation.Transfer learning is a core principle in NLP. It allows the knowledge acquired by a model from one task or domain to be applied to another. This approach significantly reduces the need for extensive labeled data for each new task, making NLP more accessible and efficient. Transfer learning has opened doors to rapid advancements and innovations in the field, making NLP a pivotal technology in AI applications.NLP in Real-World ApplicationsNLP plays a pivotal role in numerous real-world applications, demonstrating its versatility and impact in various domains:Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency.Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies.Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach.Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes.Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks.In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Transfer learning is a core principle in NLP. It allows the knowledge acquired by a model from one task or domain to be applied to another. This approach significantly reduces the need for extensive labeled data for each new task, making NLP more accessible and efficient. Transfer learning has opened doors to rapid advancements and innovations in the field, making NLP a pivotal technology in AI applications.NLP in Real-World ApplicationsNLP plays a pivotal role in numerous real-world applications, demonstrating its versatility and impact in various domains:Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency.Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies.Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach.Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes.Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks.In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. NLP plays a pivotal role in numerous real-world applications, demonstrating its versatility and impact in various domains:Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency.Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies.Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach.Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes.Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks.In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Chatbots and Virtual Assistants:NLP enables the development of intelligent chatbots and virtual assistants that can understand and respond to human language. These AI-powered agents are used in customer support, e-commerce, and even as personal assistants, enhancing user interactions and efficiency. Sentiment Analysis for Customer Feedback:NLP algorithms analyze textual data from customer reviews and social media to gauge sentiment. This helps businesses understand customer opinions, identify trends, and make data-driven decisions for product improvements and marketing strategies. Machine Translation in Global Business:NLP-driven machine translation tools bridge language barriers, facilitating global business operations. Companies use these tools for translating documents, websites, and communication, enabling cross-border collaborations and expanding market reach. Healthcare Applications (Clinical NLP):In the healthcare sector, clinical NLP is employed to extract insights from medical records, patient histories, and research documents. This aids in diagnosis, treatment recommendations, and medical research, improving patient care and outcomes. Legal and Regulatory Compliance:NLP assists legal professionals in sifting through vast volumes of legal documents, contracts, and regulations. It helps identify relevant information, analyze risks, and ensure compliance with complex legal requirements, saving time and reducing legal risks. In these applications, NLP leverages its ability to process and understand human language to enhance efficiency, decision-making, and user experiences across various industries.Ethical Considerations in NLPBias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups.Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data.Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment.Challenges and LimitationsAmbiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge.Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness.Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience.Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern.Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Bias and Fairness in NLP:Ethical concerns arise due to biases present in training data, which can lead to discriminatory outcomes in NLP applications. Addressing bias requires data preprocessing, fairness audits, and transparent algorithms to ensure equitable results for all user groups. Privacy Concerns:NLP often involves processing personal or sensitive information, raising privacy issues. Ethical NLP applications must prioritize data protection, implement encryption, and adhere to regulatory frameworks to safeguard user data. Responsible AI Practices:Ethical NLP practitioners must follow responsible AI guidelines, promoting transparency, accountability, and user consent. Continuous monitoring and feedback loops help mitigate ethical risks and ensure responsible NLP development and deployment. Ambiguity and Context Understanding:One of the primary challenges in NLP is dealing with the inherent ambiguity of human language. Words and phrases can have multiple meanings depending on context, making it difficult for AI systems to accurately interpret and respond to user input. Developing models that can understand context and disambiguate language is an ongoing challenge. Bias and Ethical Concerns:NLP models can inadvertently perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing bias in NLP algorithms and ensuring ethical considerations in data collection and usage is critical to prevent unintended harm and promote fairness. Multilingual and Low-Resource Languages:While NLP has made significant progress in widely spoken languages, it still faces hurdles in less common or low-resource languages. Building robust NLP models for diverse languages and dialects remains a challenge, limiting the accessibility of AI technologies to a global audience. Scalability and Processing Speed:NLP models, especially large-scale ones, demand substantial computational resources, making them less accessible for resource-constrained environments. Improving the scalability and processing speed of NLP models to accommodate real-time or edge computing applications is an ongoing concern. Addressing these challenges is crucial for the continued advancement and ethical use of NLP in AI applications.Recent Advances and Future TrendsTransformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Transformers and Pre-trained Models:Recent years have witnessed a transformative shift in NLP with the introduction of transformers and pre-trained models like BERT, GPT-3, and T5. These models have significantly improved natural language understanding and generation tasks, paving the way for more accurate and context-aware AI applications.Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Multimodal NLP (Combining Text and Other Data):The future of NLP extends beyond text to incorporate multiple data modalities, such as images, audio, and video. Multimodal NLP research aims to develop models that can understand and generate content across diverse data types, enabling richer and more immersive AI experiences.Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Explainable AI in NLP:Ensuring transparency and interpretability in NLP models is critical. Researchers are actively working on techniques to make NLP models more explainable, allowing users to understand how decisions are made and building trust in AI applications.NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. NLP in Edge Computing and IoT:As computing resources become more distributed, NLP is moving towards edge devices and the Internet of Things (IoT). Implementing NLP at the edge enables faster responses, reduced latency, and privacy-preserving AI applications, making it a promising frontier in the field.These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. These recent advances and future trends in NLP signify its continuous evolution, offering the potential for AI applications to become more versatile, accessible, and accountable in various domains.Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Natural Language Processing (NLP) plays a pivotal role in AI applications by enabling machines to understand and interact with human language. Its significance is bound to grow further, with continuous innovation and advancements. As NLP technologies evolve, we anticipate even greater integration into various industries, revolutionizing communication between humans and machines. This dynamic landscape promises a future where NLP-driven AI systems will become even more integral in shaping how we interact with technology and information. Alagar RSep 30, 2023069 Bruhitha Reddy GNov 23, 2024023 sfadmuserMar 10, 20212194 Ajithkumar K GNov 24, 2024025 Alagar RNov 18, 2024043 Ajithkumar K GNov 17, 2024039 Bruhitha Reddy GNov 16, 2024034 Alagar RNov 7, 2024092 Alagar ROct 18, 202301260 Alagar RDec 17, 202301049 Alagar RJun 18, 20240780 Alagar RSep 30, 20240627 Alagar RJul 19, 20240600 Alagar ROct 18, 202301260 Alagar ROct 7, 202301169 Alagar RDec 17, 202301049 Join our subscribers list to get the latest news, updates and special offers directly in your inbox"
https://udemy.benesse.co.jp/data-science/ai/nlp.html,"ã€ŒNLPã€ã¨ã¯ã€AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ãŒè‡ªç„¶è¨€èªžã‚’åˆ†æžã™ã‚‹æŠ€è¡“ã®ã“ã¨ã§ã€è‡ªç„¶è¨€èªžå‡¦ç†ã¨ã‚‚å‘¼ã°ã‚Œã¾ã™ã€‚ä¸–ç•Œã§å¸‚å ´è¦æ¨¡ãŒå¹´ã€…æ‹¡å¤§ã—ã€éžå¸¸ã«æ³¨ç›®ã‚’é›†ã‚ã¦ã„ã‚‹AIæŠ€è¡“ã§ã™ã€‚ ã“ã®è¨˜äº‹ã§ã¯ã€NLPã«ã¤ã„ã¦ã®åŸºç¤Žã‚„ã€ã§ãã‚‹ã“ã¨ã€ä½¿ã‚ã‚Œã¦ã„ã‚‹ä¾‹ãªã©ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ å…¬é–‹æ—¥ï¼š2020å¹´9æœˆ29æ—¥ å°‚é–€é ˜åŸŸï¼šäººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ / ç”ŸæˆAI / ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚° / æ©Ÿæ¢°å­¦ç¿’ æˆ‘å¦» å¹¸é•· Yukinaga Azuma ã€Œãƒ’ãƒˆã¨AIã®å…±ç”Ÿã€ãŒãƒŸãƒƒã‚·ãƒ§ãƒ³ã®ä¼šç¤¾ã€SAI-Labæ ªå¼ä¼šç¤¾ã®ä»£è¡¨å–ç· å½¹ã€‚AIã®æ•™è‚²/ç ”ç©¶/ã‚¢ãƒ¼ãƒˆã€‚æ±åŒ—å¤§å­¦å¤§å­¦é™¢ç†å­¦ç ”ç©¶ç§‘ã€ç‰©ç†å­¦å°‚æ”»ä¿®äº†ã€‚åšå£«ï¼ˆç†å­¦ï¼‰ã€‚æ³•æ”¿å¤§å­¦ãƒ‡ã‚¶ã‚¤ãƒ³å·¥å­¦éƒ¨å…¼ä»»è¬›å¸«ã€‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ•™è‚²ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ Udemyã§ã€åæ•°ä¸‡äººã«AIã‚’æ•™ãˆã‚‹äººæ°—è¬›å¸«ã€‚è¤‡æ•°ã®æœ‰åä¼æ¥­ã§AIæŠ€è¡“ã‚’æŒ‡å°Žã€‚ã€ŒAGIç¦å²¡ã€ã€Œè‡ªç”±ç ”ç©¶å®¤ AIRS-Labã€ã‚’ä¸»å®°ã€‚è‘—æ›¸ã«ã€ã€Œã¯ã˜ã‚ã¦ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã€Œã¯ã˜ã‚ã¦ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°2ã€ï¼ˆSBã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ï¼‰ã€ã€ŒPythonã§å‹•ã‹ã—ã¦å­¦ã¶ï¼ã‚ãŸã‚‰ã—ã„æ•°å­¦ã®æ•™ç§‘æ›¸ã€ã€Œã‚ãŸã‚‰ã—ã„è„³ç§‘å­¦ã¨äººå·¥çŸ¥èƒ½ã®æ•™ç§‘æ›¸ã€ã€ŒGoogle Colaboratoryã§å­¦ã¶! ã‚ãŸã‚‰ã—ã„äººå·¥çŸ¥èƒ½æŠ€è¡“ã®æ•™ç§‘æ›¸ã€ã€ŒPyTorchã§ä½œã‚‹ï¼æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãƒ»AI ã‚¢ãƒ—ãƒªé–‹ç™ºå…¥é–€ã€ã€ŒBERTå®Ÿè·µå…¥é–€ã€ã€Œç”ŸæˆAIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å…¥é–€ã€ï¼ˆç¿”æ³³ç¤¾ï¼‰ã€‚å…±è‘—ã«ã€ŒNo.1ã‚¹ã‚¯ãƒ¼ãƒ«è¬›å¸«é™£ã«ã‚ˆã‚‹ ä¸–ç•Œä¸€å—ã‘ãŸã„iPhoneã‚¢ãƒ—ãƒªé–‹ç™ºã®æŽˆæ¥­ã€ï¼ˆæŠ€è¡“è©•è«–ç¤¾ï¼‰ã€‚ INDEX ã€ŒNLPã€ã¨ã¯ã€å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’AIãŒåˆ†æžã™ã‚‹æŠ€è¡“ã®ã“ã¨ã§ã™ã€‚NLPã¯ã€Natural Language Processingã®ç•¥ã§ã€è‡ªç„¶è¨€èªžå‡¦ç†ã¨ã‚‚å‘¼ã°ã‚Œã¾ã™ã€‚ è‡ªç„¶è¨€èªžã¨ã¯ã€äººé–“ãŒæ—¥å¸¸ã§ã‚„ã‚Šå–ã‚Šã™ã‚‹æ—¥æœ¬èªžã‚„è‹±èªžãªã©ã®ã€ã„ã‚ã‚†ã‚‹ã€Œè¨€è‘‰ã€ã®ã“ã¨ã§ã€NLPã¯ãã®ã‚ˆã†ãªè‡ªç„¶è¨€èªžã‚’å‡¦ç†ãƒ»åˆ†æžã™ã‚‹æŠ€è¡“ã§ã™ã€‚ ãªãŠã€è‡ªç„¶è¨€èªžå‡¦ç†ã«ã¤ã„ã¦ã¯ã€ã€Œè‡ªç„¶è¨€èªžå‡¦ç†ã¨ã¯ï¼Ÿã‚¹ãƒžãƒ¼ãƒˆã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã«ã‚‚ä½¿ã‚ã‚Œã¦ã„ã‚‹æŠ€è¡“ã‚’ã‚ã‹ã‚Šã‚„ã™ãè§£èª¬ï¼ã€ã‚’ã”è¦§ãã ã•ã„ã€‚ å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æžã™ã‚‹ãŸã‚ã«ã€AIã¯è¨€èªžã‚’å­¦ã¶å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚AIã‚„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«ã¨ã£ã¦ã€è‡ªç„¶è¨€èªžã®æ–‡ç« ã®æ§‹é€ ã‚’èª­ã¿è§£ãã‚ˆã†ãªæ§‹æ–‡è§£æžã‚„å½¢æ…‹ç´ è§£æžã¯å®¹æ˜“ã§ã™ã€‚ ã—ã‹ã—ã€äººé–“ãŒç™ºã™ã‚‹è¨€è‘‰ã«ã¯ã€åŒã˜è¨€è‘‰ã‚’ä½¿ã£ã¦ã„ã¦ã‚‚ã€æ„å‘³ãŒç•°ãªã‚‹ã¨ã„ã†å ´åˆãŒå¤šãã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€
â€œé æ…®â€ã‚„â€œç›¸æ‰‹ã«é…æ…®ã™ã‚‹æ°—æŒã¡â€ã‹ã‚‰ã€ã‚ãˆã¦æ›–æ˜§ãªè¨€è‘‰ã‚’ä½¿ç”¨ã™ã‚‹ã‚±ãƒ¼ã‚¹ã‚‚ã‚ã‚‹ã§ã—ã‚‡ã†ã€‚ ãã®ãŸã‚ã€AIãŒãƒ†ã‚­ã‚¹ãƒˆã®æ„å‘³ã‚„æ–‡è„ˆã‚’è§£æžã™ã‚‹ã“ã¨ã¯å®¹æ˜“ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ã—ã‹ã—è¿‘å¹´ã«ãŠã„ã¦ã¯ã€ãƒ“ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚„AIã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®é£›èºçš„æˆé•·ã«ã‚ˆã£ã¦ã€AIã«ã‚ˆã‚‹è‡ªç„¶è¨€èªžã®è§£æžãƒ»å‡¦ç†ãŒé€²ã‚“ã§ã„ã¾ã™ã€‚ãã—ã¦ã€BERTã®ã‚ˆã†ãªç”»æœŸçš„ãªè‡ªç„¶è¨€èªžå‡¦ç†ãƒ¢ãƒ‡ãƒ«ã‚‚ç™»å ´ã—ã¾ã—ãŸã€‚ BERTã«ã¤ã„ã¦ã¯ã€ã€ŒBERTã¨ã¯ä½•ã‹ï¼ŸGoogleãŒèª‡ã‚‹æœ€å…ˆç«¯æŠ€è¡“ã®ä»•çµ„ã¿ã‚’è§£èª¬ï¼ã€ã‚’ã”è¦§ãã ã•ã„ã€‚ AIåˆ†é‡Žã«ãŠã‘ã‚‹ã€ŒNLPã€ã¨æ··åˆã—ã‚„ã™ã„è¨€è‘‰ã«ã€ã€Œå¿ƒç†å­¦åˆ†é‡Žã«ãŠã‘ã‚‹NLPã€ãŒã‚ã‚Šã¾ã™ã€‚ å¿ƒç†å­¦åˆ†é‡Žã«ãŠã‘ã‚‹NLP(Neuro Linguistic Programing)ã¯ã€ç¥žçµŒè¨€èªžãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€è¿‘ä»£å¿ƒç†å­¦ã‚»ãƒ©ãƒ”ã‚¹ãƒˆã®ãƒªãƒãƒ£ãƒ¼ãƒ‰ãƒ»ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¨è¨€èªžå­¦è€…ã‚¸ãƒ§ãƒ³ãƒ»ã‚°ãƒªãƒ³ãƒ€ãƒ¼ã«ã‚ˆã£ã¦æå”±ã•ã‚Œã¾ã—ãŸã€‚èƒ½åŠ›ã®é«˜ã„ã‚»ãƒ©ãƒ”ã‚¹ãƒˆãŒä½¿ã£ã¦ã„ã‚‹è¨€è‘‰ã‚„è©±ã—æ–¹ã€ãƒŽãƒ³ãƒãƒ¼ãƒãƒ«(éžè¨€èªž)ã‚’åˆ†æžã—ã€æ‰‹æœ¬ã¨ãªã‚‹ã‚¹ã‚­ãƒ«ã¨ã—ã¦ä½“ç³»åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚ ç¥žçµŒè¨€èªžãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ä¿¡é ¼é–¢ä¿‚ã®æ§‹ç¯‰ã‚„æ€è€ƒæ•´ç†ã«å½¹ç«‹ã¤ã¨ã•ã‚Œã¦ãŠã‚Šã€å¿ƒç†å­¦ã‚„å¿ƒç†ç™‚æ³•ã®åˆ†é‡Žã§ç ”ç©¶ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€ã“ã®ã€Œç¥žçµŒè¨€èªžãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ã¯ã€Œè‡ªç„¶è¨€èªžå‡¦ç†ã€ã¨ã¯ç•°ãªã‚‹ã‚‚ã®ã§ã‚ã‚‹ãŸã‚ã€æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚  ï¼¼æ–‡å­—ã‚ˆã‚Šå‹•ç”»ã§å­¦ã³ãŸã„ã‚ãªãŸã¸ï¼ NLPã¯ã€ä¸‹è¨˜ã®é †ç•ªã§å‡¦ç†ã•ã‚Œã¾ã™ã€‚ ãã‚Œãžã‚Œã®ã‚¹ãƒ†ãƒƒãƒ—ã§ä½•ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹ã®ã‹è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚ å½¢æ…‹ç´ è§£æžã¯ã€ç°¡å˜ã«è¨€ã†ã¨ã€æ–‡ã‚’å‹•è©žã‚„åè©žã€å½¢å®¹è©žãªã©ã®å“è©žã«åˆ†ã‘ã‚‹ã“ã¨ã§ã™ã€‚ä¸‹è¨˜ã®ä¾‹æ–‡ã‚’ã”è¦§ãã ã•ã„ã€‚ ä¾‹æ–‡ï¼šå¤ªéƒŽ(åè©ž) / ã¯(åŠ©è©ž) / ãƒ”ã‚¯ãƒ‹ãƒƒã‚¯(åè©ž) / ãŒ(åŠ©è©ž) / å¥½ãã (å½¢å®¹å‹•è©ž) ã“ã®ã‚ˆã†ã«æ–‡ã‚’æœ€å°é™ã®å˜ä½ã«ã™ã‚‹ã“ã¨ã§ã€ä¸€ã¤ã²ã¨ã¤ã®è¨€è‘‰ã®æ„å‘³ã‚’ç†è§£ã—ã¾ã™ã€‚ å½¢æ…‹ç´ è§£æžã®è©³ã—ã„èª¬æ˜Žã«é–¢ã—ã¦ã¯ã€ã€Œå½¢æ…‹ç´ è§£æžã¨ã¯ï¼ŸãŠã™ã™ã‚ã®5å¤§è§£æžãƒ„ãƒ¼ãƒ«ã‚„å®Ÿéš›ã®å¿œç”¨ä¾‹ã‚’ç´¹ä»‹ã€ã‚’ã”è¦§ãã ã•ã„ã€‚ æ§‹æ–‡è§£æžã§ã¯ã€å½¢æ…‹ç´ è§£æžã‚’ã—ãŸè¨€èªžè¦ç´ ã‚’ã‚‚ã¨ã«æ–‡ã®æ§‹é€ ã‚’æ˜Žç¢ºã«ã—ã¦ã„ãã¾ã™ã€‚ç°¡å˜ã«è¨€ã†ã¨ã€æ–‡ã‚’ä¸»èªžã‚„è¿°èªžã€ç›®çš„èªžã«åˆ†é¡žã—ã¦ã„ãä½œæ¥­ã§ã™ã€‚ ä¾‹ãˆã°ã€ä¸Šè¨˜ã®ä¾‹æ–‡ã®å ´åˆã€ã€Œå¤ªéƒŽã¯ã€ã¯ä¸»èªžã¨ãªã‚‹åè©žå¥ã€ã€Œãƒ”ã‚¯ãƒ‹ãƒƒã‚¯ãŒå¥½ãã ã€ã¯å‹•è©žå¥ã«ãªã‚Šã€ã•ã‚‰ã«ã€Œãƒ”ã‚¯ãƒ‹ãƒƒã‚¯ãŒã€ã¯ç›®çš„èªžã¨ãªã‚‹åè©žå¥ã€ã€Œå¥½ãã ã€ã¯è¿°èªžã¨ãªã‚‹å‹•è©žå¥ã«åˆ†é¡žã•ã‚Œã¾ã™ã€‚ æ§‹æ–‡è§£æžã‚’ã™ã‚‹ã“ã¨ã§ã€ãã‚Œãžã‚Œã®è¨€èªžè¦ç´ ã®é–¢ä¿‚æ€§ãŒæ˜Žç¢ºã«ãªã‚Šã€æ–‡æ§‹é€ ã®å›³å¼åŒ–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ æ„å‘³è§£æžã§ã¯ã€æ§‹æ–‡è§£æžã‚’ã‚‚ã¨ã«ãã‚Œãžã‚Œã®å˜èªžã®é–¢ä¿‚æ€§ã‚’åˆ¤æ–­ã—ã¾ã™ã€‚ä¸‹è¨˜ã®ä¾‹æ–‡ã‚’ã”è¦§ãã ã•ã„ã€‚ ä¾‹æ–‡ï¼šç·‘ã«
/ å…‰ã‚‹ / ã‚ªãƒ¼ãƒ­ãƒ© / ã¨ / æ˜Ÿ/ ã¯ / ç¾Žã—ã„ ä¸Šè¨˜ã®æ–‡ã®å ´åˆã€ã‚ªãƒ¼ãƒ­ãƒ©ãŒç·‘ã«å…‰ã‚‹ã“ã¨ã¯ã™ãã«ç†è§£ã§ãã¾ã™ãŒã€ã‚ªãƒ¼ãƒ­ãƒ©ã ã‘ã§ãªãæ˜Ÿã‚‚ç·‘ã«å…‰ã‚‹ã¨ã„ã†æ„å‘³ã«ã‚‚èª­ã¿å–ã‚Œã¾ã™ã€‚ã—ã‹ã—ã€æ„å‘³è§£æžã§è¾žæ›¸ã‚’å¼•ããªãŒã‚‰å„å˜èªžã®é–¢ä¿‚æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã„ãã¨ã€ã‚ªãƒ¼ãƒ­ãƒ©ã¯ç·‘ã«å…‰ã‚‹ã‚‚ã®ã®ã€æ˜ŸãŒç·‘ã«å…‰ã‚‹ã¨è¡¨ç¾ã•ã‚Œã‚‹ã“ã¨ã¯å°‘ãªã„ã¨ã„ã†ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€AIã¯ã€Œã“ã®æ–‡ãŒç¤ºã™ã€Žç·‘ã«å…‰ã‚‹ã€ã‚‚ã®ã¯ã‚ªãƒ¼ãƒ­ãƒ©ã ã‘ã§ã‚ã‚‹ã€ã¨çŸ¥ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§ã™ã€‚ ã“ã®ã‚ˆã†ã«æ„å‘³è§£æžã‚’ã™ã‚‹ã“ã¨ã§ã€å„å˜èªžã®é–¢ä¿‚æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚ãŸã ã—ã€æ™®æ®µã‹ã‚‰äººã€…ãŒä½¿ç”¨ã™ã‚‹æ—¥æœ¬èªžã¯ã€ä¾‹æ–‡ã‚ˆã‚Šã‚‚ã£ã¨è¤‡é›‘ã‹ã¤ã€æ›–æ˜§ãªã‚‚ã®ã§ã™ã€‚ãã®ãŸã‚ã€AIãŒæ›–æ˜§æ€§ã«æƒ‘ã‚ã•ã‚Œãªã„ãŸã‚ã®æ‰‹æ³•ã‚„æŠ€è¡“ã®ç ”ç©¶ãŒç¾åœ¨ã§ã‚‚é€²ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ æœ€å¾Œã®å·¥ç¨‹ãŒæ–‡è„ˆè§£æžã§ã™ã€‚æ–‡è„ˆè§£æžã§ã¯ã€ãã®æ–‡ã ã‘ã§ãªãã€æ–‡ã®å‘¨ã‚Šã«ã‚ã‚‹æ–‡ç« ã«é–¢ã—ã¦ã‚‚å½¢æ…‹ç´ è§£æžã‚„æ„å‘³è§£æžã‚’è¡Œã„ã¾ã™ã€‚ä¾‹ãˆã°ã€ä¸‹è¨˜ã®ã‚ˆã†ãªæ–‡ãŒã‚ã‚‹ã¨ã—ã¾ã—ã‚‡ã†ã€‚ ä¾‹æ–‡ï¼šãã‚ŒãŒå¤ªéƒŽã®å¥½ç‰©ã ã€‚ ã“ã®å ´åˆã€â€œãã‚Œâ€ãŒä½•ã§ã‚ã‚‹ã®ã‹ã‚’ã€ä¸Šè¨˜ã®1æ–‡ã‹ã‚‰ç†è§£ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚â€œãã‚Œâ€ãŒä½•ã‚’æŒ‡ã™ã®ã‹ã‚’çŸ¥ã‚‹ãŸã‚ã«ã¯ã€å‰ã®æ–‡ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€1æ–‡ã ã‘ã§ãªãã€ã™ã¹ã¦ã®æ–‡ç« ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã“ã¨ãŒæ„å‘³ã‚’ç†è§£ã™ã‚‹ä¸Šã§å¿…è¦ä¸å¯æ¬ ã§ã™ã€‚  å…ˆè¿°ã®é€šã‚Šã€NLPã§ã§ãã‚‹ã“ã¨ã¯ã€éžæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨è†¨å¤§ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è§£æžã§ã™ã€‚ éžæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã¯ã€é›»å­ãƒ¡ãƒ¼ãƒ«ã‚„æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã€ç”»åƒã‚„å‹•ç”»ãªã©ã®ã“ã¨ã§ã™ã€‚å¯¾ã—ã¦ã€å•†å“ç•ªå·ã¨ä¾¡æ ¼ã®ãƒªã‚¹ãƒˆãªã©ã¨ã„ã£ãŸã€ã€Œåˆ—ã€ã¨ã€Œè¡Œã€ã®ã‚ã‚‹è¡¨ã«ã¾ã¨ã‚ã‚‰ã‚Œã‚‹æ•°å€¤ãªã©ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨å‘¼ã³ã¾ã™ã€‚ å…ˆã»ã©ã‹ã‚‰è¦‹ã¦ããŸã¨ãŠã‚Šã€æ–‡ç« ãªã©ã®éžæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¯ã€å®¹æ˜“ã«è§£æžã—ã¥ã‚‰ã„ã“ã¨ãŒç‰¹å¾´ã§ã™ã€‚ã—ã‹ã—ã€ç¾åœ¨ã§ã¯æ©Ÿæ¢°å­¦ç¿’ã®é£›èºçš„æˆé•·ã«ã‚ˆã‚Šã€å‡¦ç†ç²¾åº¦ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚ãã®ãŸã‚ã€éŸ³å£°èªè­˜æŠ€è¡“ãªã©ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ©Ÿæ¢°çš„ã«è§£æžã™ã‚‹NLPã¯è†¨å¤§ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è§£æžã‚’å‡¦ç†ã§ãã¾ã™ã€‚è¿‘å¹´ã€Twitterã‚„Facebookãªã©ã®SNSãŒé£›èºçš„ã«æˆé•·ã—ã¾ã—ãŸã€‚SNSã®ãƒ†ã‚­ã‚¹ãƒˆè§£æžã‚’è¡Œã†ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‹ãƒ¼ã‚ºã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€NLPã¯ãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†é‡Žã§æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€åŒ»ç™‚è¨˜éŒ²ã‹ã‚‰ç—‡çŠ¶ã‚’ç‰¹å®šã—ã€äºˆé˜²ã‚„æ²»ç™‚ã«ä½¿ã‚ã‚Œã‚‹ã‚±ãƒ¼ã‚¹ã‚‚ã‚ã¾ã™ã€‚  NLPã¯ã€ã©ã®ã‚ˆã†ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹ã®ã§ã—ã‚‡ã†ã‹ã€‚NLPã®ä¾‹ã¨ã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚‚ã®ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚ ãã‚Œãžã‚Œã€ã©ã®ã‚ˆã†ã«æ´»ç”¨ã•ã‚Œã¦ã„ã‚‹ã®ã‹ã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚ æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã¯ã€Googleæ¤œç´¢ã‚„Yahoo!æ¤œç´¢ã¨ã„ã£ãŸã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã«ã‚ã‚‹Webã‚µã‚¤ãƒˆã‚’æ¤œç´¢ã§ãã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã“ã¨ã§ã™ã€‚ æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã¯ã€æ¤œç´¢ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æœ€é©ãªæƒ…å ±ã‚’æä¾›ã§ãã‚‹ã‚ˆã†ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã«ã‚ã‚‹Webã‚µã‚¤ãƒˆã‚’çµžã‚Šè¾¼ã¿ã¾ã™ã€‚ ã—ã‹ã—ã€Webã‚µã‚¤ãƒˆã«æŽ²è¼‰ã•ã‚Œã¦ã„ã‚‹æ–‡ç« ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãŒç†è§£ã§ããªã‘ã‚Œã°ã€æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆã£ãŸWebã‚µã‚¤ãƒˆã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚ã“ã®ã‚ˆã†ã«ã€Webã‚µã‚¤ãƒˆã‚’æŽ¢ã™ä½œæ¥­ã«å¿…è¦ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãŸã‚ã€NLPãŒæ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ ãƒ‘ã‚½ã‚³ãƒ³ã‚„ã‚¹ãƒžãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã§å…¥åŠ›ã—ãŸã²ã‚‰ãŒãªã®æ–‡å­—ã‚’ã€æ¼¢å­—ã‚„é¡”æ–‡å­—ã€çµµæ–‡å­—ã«å¤‰æ›ã™ã‚‹æ©Ÿèƒ½ã§ã™ã€‚è¨€è‘‰ã®å¥åˆ‡ã‚Œã‚„åŒéŸ³ç•°ç¾©èªžã®é•ã„ã‚’è§£æžã™ã‚‹ã“ã¨ã§ã€æ„å›³ã—ãŸæ„å‘³ã®å¤‰æ›ãŒå¯èƒ½ã¨ãªã‚‹ä»•çµ„ã¿ã«ãªã£ã¦ã„ã¾ã™ã€‚ éŸ³å£°å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã¨ã¯ã€è©±ã—ã‹ã‘ãŸæƒ…å ±ã‚’AIãŒèžãå–ã‚Šã€é©åˆ‡ã«å¿œãˆã¦ãã‚Œã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®ã“ã¨ã§ã™ã€‚æœ‰åãªéŸ³å£°å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã«ã¯ã€ã‚¢ãƒ¬ã‚¯ã‚µã‚„LINE Clovaãªã©ã‚’æŒ™ã’ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã¡ã‚‰ã‚‚ã€éŸ³å£°ã§å…¥åŠ›ã•ã‚ŒãŸæ–‡ã®å¥åˆ‡ã‚Œã‚„æ„å‘³ã‚’åˆ†æžã—ã¦ã„ã¾ã™ã€‚  NLPã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆãƒžã‚¤ãƒ‹ãƒ³ã‚°ã‚’åŠ¹çŽ‡åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ ãƒ†ã‚­ã‚¹ãƒˆãƒžã‚¤ãƒ‹ãƒ³ã‚°ã¨ã¯ã€å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’çµ„ã¿ä¸Šã’ã‚‹æŠ€è¡“ã®ã“ã¨ã§ã™ã€‚ä¾‹ãˆã°ã€ãã®æ–‡ç« ã®ä¸­ã«åŒã˜ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã„ãã¤ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã‚„ã€ãã‚Œãžã‚Œã®ãƒ¯ãƒ¼ãƒ‰ã®é–¢é€£æ€§ãªã©ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ ãƒ†ã‚­ã‚¹ãƒˆãƒžã‚¤ãƒ‹ãƒ³ã‚°ã«ã¤ã„ã¦ã€è©³ã—ãã¯ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒžã‚¤ãƒ‹ãƒ³ã‚°ã‚’æ´»ç”¨äº‹ä¾‹ã‹ã‚‰æ‰‹æ³•ã¾ã§ä¸å¯§ã«è§£èª¬ã€ã‚’ã”è¦§ãã ã•ã„ã€‚ ãƒ†ã‚­ã‚¹ãƒˆãƒžã‚¤ãƒ‹ãƒ³ã‚°ã®æœ‰åãªæ‰‹æ³•ã«ã¯ã€ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æžï¼ˆæ„Ÿæƒ…åˆ†æžï¼‰ã€å¯¾å¿œåˆ†æžï¼ˆã‚³ãƒ¬ã‚¹ãƒãƒ³ãƒ‡ãƒ³ã‚¹åˆ†æžï¼‰ã€ä¸»æˆåˆ†åˆ†æžï¼ˆãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ï¼‰ã€å…±èµ·èªžè§£æžãªã©ãŒã‚ã‚Šã¾ã™ã€‚ãã‚Œãžã‚Œã®æ‰‹æ³•ã«ã¤ã„ã¦ç°¡æ½”ã«ã”ç´¹ä»‹ã—ã¾ã™ã€‚ ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ(sentiment)åˆ†æžã¨ã¯ã€æ„Ÿæƒ…ã‚’åˆ†æžã™ã‚‹æ‰‹æ³•ã§ã™ã€‚SNSãªã©ã«æ›¸ãè¾¼ã¾ã‚ŒãŸå†…å®¹ã«å¯¾ã—ã¦è¡Œã‚ã‚Œã‚‹ã“ã¨ãŒå¤šãã€ãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãªã©ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ãªã©ã‹ã‚‰æ•£å¸ƒå›³ã‚’ä½œæˆã—ã€è¡Œå‹•ã®åã‚Šãªã©ã‚’åˆ†æžã™ã‚‹æ‰‹æ³•ã®ã“ã¨ã§ã™ã€‚ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¤ãƒ¡ãƒ¼ã‚¸ãªã©å¸‚å ´èª¿æŸ»ã‚’ã™ã‚‹éš›ã«ç”¨ã„ã‚‰ã‚Œã¾ã™ã€‚ è¤‡æ•°ã‚ã‚‹å¤‰æ•°ã®ä¸­ã‹ã‚‰é€£å‹•ã—ã¦ã„ã‚‹ã‚‚ã®ã‚’è¦‹ã¤ã‘ã¦ã€é‡è¦ãªæˆåˆ†ã ã‘ã‚’åˆ†æžã™ã‚‹æ‰‹æ³•ã®ã“ã¨ã§ã™ã€‚ ä¸»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã«è¡Œã‚ã‚Œã‚‹åˆ†æžæ‰‹æ³•ã®ã“ã¨ã§ã™ã€‚å…±èµ·èªžè§£æžã‚’ã™ã‚‹ã“ã¨ã§ã€é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ ã“ã®è¨˜äº‹ã§ã¯ã€AIåˆ†é‡Žã®ã€ŒNLPã€ã¨ã¯ä½•ã‹ã€NLPã§ã§ãã‚‹ã“ã¨ãªã©ã‚’è§£èª¬ã—ã¾ã—ãŸã€‚æ–‡ç« è§£æžã«ã¯æ¬ ã‹ã›ãªã„æŠ€è¡“ã§ã‚ã‚‹NLPã¯ã€ä»Šå¾Œã‚‚é£›èºã‚’é‚ã’ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ è‡ªç„¶è¨€èªžå‡¦ç†ã¨ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ: AIã«ã‚ˆã‚‹æ–‡ç« ç”Ÿæˆã¨ä¼šè©±ã‚¨ãƒ³ã‚¸ãƒ³é–‹ç™º â˜…4.1ï¼ˆ1,486 ä»¶ã®è©•ä¾¡ï¼‰ 10,613 äººã®å—é¨“ç”Ÿ ä½œæˆè€…: æˆ‘å¦» å¹¸é•· Yukinaga Azuma(äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ / ç”ŸæˆAI / ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚° / æ©Ÿæ¢°å­¦ç¿’) ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ·±å±¤å­¦ç¿’ï¼‰ã‚’åˆ©ç”¨ã—ã¦ã€æ—¥æœ¬èªžã‚’è§£æžã—æ–‡ç« ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚ NLPã‚’åˆ©ç”¨ã—ãŸäººå·¥çŸ¥èƒ½ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®æ§‹ç¯‰ã‚‚è¡Œã„ã¾ã™ã€‚ ï¼¼ç„¡æ–™ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ï¼ï¼ è©•ä¾¡ï¼šâ˜…â˜…â˜…â˜…â˜…ã“ã®ã‚³ãƒ¼ã‚¹ã¯ã€è‡ªç„¶è¨€èªžå‡¦ç†ã¨ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé–‹ç™ºã®åŸºæœ¬ã‹ã‚‰å¿œç”¨ã¾ã§ã‚’ä¸å¯§ã«ã‚«ãƒãƒ¼ã—ã¦ãŠã‚Šã€éžå¸¸ã«å½¹ç«‹ã¡ã¾ã—ãŸã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®èª¬æ˜ŽãŒéžå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ãã€å®Ÿå‹™ã«ç›´çµã™ã‚‹å†…å®¹ã§ã€è‡ªåˆ†ã§ã™ãã«å®Ÿè£…å¯èƒ½ã§ã—ãŸã€‚ã¾ãŸã€èª²é¡Œã‚’é€šã—ã¦å­¦ç¿’å†…å®¹ã‚’å®šç€ã•ã›ã€å®Ÿå‹™ã«å¿œç”¨ã§ãã‚‹è‡ªä¿¡ãŒã¤ãã¾ã—ãŸã€‚åˆå¿ƒè€…ã‹ã‚‰ä¸­ç´šè€…ã¾ã§å¹…åºƒããŠã™ã™ã‚ã§ãã‚‹ã‚³ãƒ¼ã‚¹ã§ã™ã€‚ è©•ä¾¡ï¼šâ˜…â˜…â˜…â˜…â˜…ã¯ã˜ã‚ã¦è‡ªç„¶è¨€èªžå‡¦ç†ã®è¬›åº§ã‚’å—ã‘ã¾ã—ãŸãŒã€åˆ†ã‹ã‚Šã‚„ã™ãã€èˆˆå‘³æ·±ã„ã‚‚ã®ã§ã—ãŸã€‚ä»–ã®æ©Ÿæ¢°å­¦ç¿’ã¨ã¯é•ã£ãŸå°è±¡ã‚’å—ã‘ã¾ã—ãŸã€‚è‡ªåˆ†ã§ã‚‚ä½•ã‹ä½œã‚Œãã†ï¼ã¨æ€ãˆã‚‹å†…å®¹ã§ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚  ã‚¸ã‚§ãƒãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–AIï¼ˆç”ŸæˆAIï¼‰å…¥é–€ã€ChatGPT/Midjourneyã€‘ -ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãŒé–‹ãæœªæ¥- ã€ChatGPTã€‘åˆå¿ƒè€…å‘ã‘è¬›åº§ ãƒ“ã‚¸ãƒã‚¹ã§æ´»ç”¨ã§ãã‚‹ç¨‹ã«è¿”ç­”ã®ç²¾åº¦ã‚„å“è³ªã‚’ä¸Šã’ã‚‹ã‚³ãƒ„ã‚’å¾¹åº•è§£èª¬ã€éžã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã€‘ ã€ŒChatGPTã€ã®ã‚³ãƒ¼ã‚¹ä¸€è¦§ã‚’è¦‹ã‚‹ ã¿ã‚“ãªã®AIè¬›åº§ ã‚¼ãƒ­ã‹ã‚‰Pythonã§å­¦ã¶äººå·¥çŸ¥èƒ½ã¨æ©Ÿæ¢°å­¦ç¿’ ã€2023å¹´æœ€æ–°ç‰ˆã€‘ ã€ä¸–ç•Œã§91ä¸‡äººãŒå—è¬›ã€‘åŸºç¤Žã‹ã‚‰ç†è§£ã—ã€Pythonã§å®Ÿè£…ï¼æ©Ÿæ¢°å­¦ç¿’26ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç†è«–ã¨å®Ÿè·µã‚’é€šã˜ã¦ãƒžã‚¹ã‚¿ãƒ¼ã—ã‚ˆã† ã€Œæ©Ÿæ¢°å­¦ç¿’ã€ã®ã‚³ãƒ¼ã‚¹ä¸€è¦§ã‚’è¦‹ã‚‹ LangChainã«ã‚ˆã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºå…¥é–€â€•GPTã‚’ä½¿ã£ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å®Ÿè£…ã¾ã§ AIãƒ‘ãƒ¼ãƒ•ã‚§ã‚¯ãƒˆãƒžã‚¹ã‚¿ãƒ¼è¬›åº§ -Google Colaboratoryã§éš…ã€…ã¾ã§å­¦ã¶å®Ÿç”¨çš„ãªäººå·¥çŸ¥èƒ½/æ©Ÿæ¢°å­¦ç¿’- ã€Œäººå·¥çŸ¥èƒ½ã€ã®ã‚³ãƒ¼ã‚¹ä¸€è¦§ã‚’è¦‹ã‚‹ Pythonã«ã‚ˆã‚‹æ™‚ç³»åˆ—åˆ†æž~æ©Ÿæ¢°å­¦ç¿’ãƒ»ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ç·¨ ã€PythonÃ—æ ªä¾¡åˆ†æžã€‘æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»åŠ å·¥ãƒ»å¯è¦–åŒ–ã—ã¦æ™‚ç³»åˆ—åˆ†æžï¼æœ€çµ‚çš„ã«AIãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’ã—ã¦ã„ã“ã†ï¼ ã€Œæ™‚ç³»åˆ—åˆ†æžã€ã®ã‚³ãƒ¼ã‚¹ä¸€è¦§ã‚’è¦‹ã‚‹ æœ¬è¨˜äº‹ã§ã¯ã€è¿‘å¹´ã®äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ãƒ–ãƒ¼ãƒ ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®åŸºæœ¬â€¦ æ©Ÿæ¢°å­¦ç¿’ã®ç”»åƒèªè­˜ã®å­¦ç¿’ã«ãŠã„ã¦ã€åˆå¿ƒè€…ã§ã‚‚ä½¿ã„ã‚„ã™ãã€ã‚µâ€¦ AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã¨ã„ã†è¨€è‘‰ã‚’ã‚ˆãç›®ã«ã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚æœ€â€¦ 2045å¹´ã«ã‚·ãƒ³ã‚®ãƒ¥ãƒ©ãƒªãƒ†ã‚£ã®åˆ°æ¥ã«ã‚ˆã£ã¦ã€äººå·¥çŸ¥èƒ½ã®çŸ¥æ€§ãŒå…¨â€¦ "
https://www.sapien.io/blog/natural-language-processing-vs-generative-ai-expert-insights,"EdTech Logistics Insurance Finance Autonomous Vehicles Large Language Models Image Annotation Document Annotation Data Collection Test & Evalutation Sapien Blog Case Studies Careers Contact us News The evolution of artificial intelligence has led to the development of multiple new specialized branches, each contributing uniquely to the field. Among these, Natural Language Processing (NLP) and Generative AI have emerged as two technologies receiving the most funding for development, driving advancements in human-computer interactions. As AI continues to be adopted by different industries, understanding the differences between NLP and Generative AI becomes increasingly important for professionals looking to integrate these technologies effectively. There are several key differences between NLP and generative AI applications that makes them distinctly useful and more effective in their respective domains. Natural Language Processing (NLP) and Generative AI, while often mentioned together, serve fundamentally different purposes within the broader AI ecosystem. NLP vs Generative AI highlights these distinctions, as NLP is primarily concerned with the comprehension and processing of human language. It encompasses a range of tasks, including text analysis, sentiment analysis, machine translation, and speech recognition. NLP relies on linguistic rules, statistical models, and machine learning algorithms to interpret and respond to human language accurately. The core technology behind NLP includes tokenization, parsing, named entity recognition (NER), and part-of-speech tagging, which are crucial for breaking down and understanding text. Generative AI, on the other hand, focuses on the creation of new content. It uses deep learning models, particularly Generative Adversarial Networks (GANs) and Transformer-based models like GPT, to produce text, images, or other forms of data that mimic human creativity. A generative AI model for language can generate entirely new outputs that were not part of the original dataset. This capability is increasingly used in content creation, design, and even drug discovery, where novel molecules are generated based on existing chemical properties. The difference between NLP and Generative AI is not just in their definitions but also in their underlying technologies and use cases. NLP is built on understanding and reacting to human language, whereas Generative AI is about innovation and creation. NLP is predominantly utilized in applications that require understanding, processing, and generating human language. Some of the key application areas include: Generative AI finds its applications in areas where creativity and innovation are paramount. Key application areas include: Despite their differences, NLP and Generative AI share several methodologies and tools, especially in the foundational aspects of AI. Machine learning algorithms:Both NLP and Generative AI rely heavily on machine learning, particularly deep learning models, to achieve their objectives. For instance, Transformer architectures like BERT (Bidirectional Encoder Representations from Transformers) are used in NLP for tasks such as text classification and question-answering, while similar architectures, like GPT (Generative Pre-trained Transformer), are used in Generative AI for creating human-like text. Natural language in artificial intelligence:NLP and Generative AI both involve the use of natural language models to process and generate language. In NLP, these models are used to understand and analyze text, whereas in Generative AI, they are used to produce new, coherent text based on the learned patterns. Data preprocessing techniques:Techniques such as tokenization, vectorization, and embedding are common to both NLP and Generative AI. These processes convert text into numerical data that can be fed into machine learning models for further analysis or generation. There are scenarios where NLP and Generative AI are used together, creating synergistic effects that enhance AI capabilities. Automated content generation and editing:NLP can analyze existing text to determine the style, tone, and structure, which Generative AI can then use to create new content that matches the analyzed characteristics. This combination is particularly useful in journalism and marketing, where large volumes of content need to be generated quickly. Chatbots and virtual assistants:Generative AI can create dynamic responses to user queries, while NLP ensures that these responses are contextually appropriate and grammatically correct. Together, they enable more sophisticated and human-like interactions in AI-powered customer service applications. Language translation and summarization:NLP can break down and understand the structure of a text, which Generative AI can then use to produce a summary or translation that maintains the original meaning while being concise and accurate. NLP and generative AI tools have become some of the fastest growing applications in history because of their novelty and the rate of improvement. Here are some of the areas and industries most influenced by these technologies. Natural Language Processing has become a critical technology in several industries, driving innovation and efficiency in the following areas: Customer service:NLP-driven chatbots and virtual assistants are transforming customer service by automating responses to common queries and providing instant support, significantly reducing the workload on human agents. Healthcare:In healthcare, NLP is used to analyze patient records, transcribe doctorâ€™s notes, and even assist in diagnosing conditions based on patient symptoms described in natural language. This technology is improving patient outcomes by enabling faster and more accurate data processing. Finance:The finance sector uses NLP for sentiment analysis, fraud detection, and algorithmic trading. By analyzing news articles, financial reports, and social media posts, NLP helps financial institutions make informed decisions and identify potential risks. Generative AI is making waves in several key sectors, driving groundbreaking innovations. Entertainment and media:Generative AI is revolutionizing content creation in the entertainment industry, from generating scripts to creating visual effects. In media, it is used to produce personalized content that resonates with individual audiences. Pharmaceuticals and healthcare:Generative AI is being used to design new drugs and treatment plans by generating novel molecular structures and simulating their effects. This is speeding up the drug discovery process and leading to more effective treatments. Marketing and advertising:Generative AI is enabling marketers to create highly targeted and personalized content for advertising campaigns. By analyzing consumer behavior and preferences, Generative AI can generate ads that are more likely to resonate with the target audience. While NLP has made significant strides, it faces several challenges and limitations in implementation. One of the biggest challenges in NLP is handling the ambiguity and context-dependence of human language. Words and phrases can have different meanings depending on the context, making it difficult for NLP systems to always interpret them correctly. NLP systems require large amounts of high-quality data to function effectively. However, obtaining and curating such data can be difficult, especially for languages and dialects that are less widely spoken. NLP models trained on data from one culture may not perform well when applied to another due to differences in language use, idioms, and cultural references. This limitation makes it challenging to create truly global NLP systems. Generativeunified AI, while promising, also faces its own set of limitations and challenges. Generative AI models can produce content that is realistic but not necessarily accurate or appropriate. Ensuring that generated content meets the required standards of quality and relevance is a significant challenge. Like NLP, Generative AI models can inherit biases present in the training data, leading to the generation of biased or discriminatory content. This issue is particularly concerning in applications like automated content creation and decision-making. The ability of Generative AI to create realistic images, videos, and text raises ethical concerns, particularly around misinformation, copyright infringement, and the potential misuse of technology for malicious purposes. These challenges show there is still a need for ongoing research and development to address the limitations and ethical concerns associated with both NLP and Generative AI. As AI continues to evolve, both NLP and Generative AI are expected to undergo significant advancements. In NLP, we can anticipate the development of more sophisticated models that better understand context and nuance. Future NLP systems may be able to comprehend and generate text that is indistinguishable from human writing, opening up new possibilities for human-computer interaction. Additionally, advancements in multilingual NLP will enable more accurate and seamless communication across different languages and cultures. Generative AI is expected to become even more powerful and versatile. As models continue to improve, they will be able to generate more complex and creative content, from entire novels to intricate designs. We may also see the emergence of Generative AI systems that can collaborate with humans in real-time, blending human creativity with machine-generated ideas. Our comparison of Natural Language Processing and Generative AI shows that while both technologies are needed for the advancement of AI, they serve different purposes and are applied in distinct ways. Understanding the difference between NLP and Generative AI is important for professionals looking to leverage AI effectively in their respective fields. To achieve the best results with AI, especially when dealing with large language models (LLMs), it is essential to have high-quality data labeling. At Sapien, we offer solutions for enhancingLLM alignmentand performance through high-quality data labeling, ensuring that your AI models are trained on the most accurate and relevant data. Whether you need document annotation or image annotation, our services are designed to optimize your data and improve the performance of your AI models. Explore ourLLM servicesto see how we can help you achieve your AI goals, schedule a consult to implement better NLP and generative AI applications in your AI models. NLP can be broadly classified into rule-based NLP, statistical NLP, and neural NLP, each with its own methods for processing and analyzing language data. What are the two techniques used in NLP? The two primary techniques in NLP are syntactic analysis (syntax) and semantic analysis (semantics). Syntax focuses on the structure of language, while semantics is concerned with meaning. How can businesses integrate NLP and Generative AI effectively? Businesses can integrate NLP and Generative AI by using NLP to analyze and understand customer data and Generative AI to create personalized content based on that analysis. Is NLP the future of AI? NLP is a critical component of AIâ€™s future, especially in applications involving human-computer interaction. However, it will likely evolve alongside other AI technologies, including Generative AI, to create more powerful and versatile systems. Schedule a consult with our team to learn how Sapienâ€™s data labeling and data collection services can advance your speech-to-text AI models"
https://www.geeksforgeeks.org/top-7-applications-of-natural-language-processing/,"In the past, did you ever imagine that you could talk to your phone and get things done?Or that your phone would talk back to you! This has become a pretty normal thing these days with Siri, Alexa, Google Assistant, etc. You can ask any possible questions ranging from â€œWhatâ€™s the weather outsideâ€ to â€œWhatâ€™s your favorite color?â€ from Siri and youâ€™ll get an answer. All of this and more is accomplished usingNatural Language Processing. And not only that, there are many other applications of Natural Language Processing these days including the translator on your phone or the grammar checker you use before sending Emails.  Natural Language Processing allows your device to hear what you say, then understand the hidden meaning in your sentence, and finally act on that meaning. And all of this is completed in 5 seconds! But the question this brings is What exactly is Natural Language Processing? And how does it work? So letâ€™s see the answer to this first. Natural Language Processing is a part of artificial intelligence that aims to teach the human language with all its complexities to computers. This is so that machines can understand and interpret the human language to eventually understand human communication in a better way. Natural Language Processing is a cross among many different fields such asartificial intelligence, computational linguistics, human-computer interaction, etc. There are many different methods in NLP to understand human language which include statistical and machine learning methods. And why is Natural Language Processing important, you wonder? Well, it allows computers to understand human language and then analyze huge amounts of language-based data in an unbiased way. This is the reason that Natural Language Processing has many diverse applications these days in fields ranging from IT to telecommunications to academics. So, letâ€™s see these applications now. Chatbots are a form of artificial intelligence that are programmed to interact with humans in such a way that they sound like humans themselves. Depending on the complexity of the chatbots, they can either just respond to specific keywords or they can even hold full conversations that make it tough to distinguish them from humans.Chatbotsare created using Natural Language Processing andMachine Learning, which means that they understand the complexities of the English language and find the actual meaning of the sentence and they also learn from their conversations with humans and become better with time. Chatbots work in two simple steps. First, they identify the meaning of the question asked and collect all the data from the user that may be required to answer the question. Then they answer the question appropriately. Have you noticed that search engines tend to guess what you are typing and automatically complete your sentences? For example, On typing â€œgameâ€ in Google, you may get further suggestions for â€œgame of thronesâ€, â€œgame of lifeâ€ or if you are interested in maths then â€œgame theoryâ€. All these suggestions are provided using autocomplete that uses Natural Language Processing to guess what you want to ask. Search engines use their enormous data sets to analyze what their customers are probably typing when they enter particular words and suggest the most common possibilities. They use Natural Language Processing to make sense of these words and how they are interconnected to form different sentences. These days voice assistants are all the rage! Whether its Siri, Alexa, or Google Assistant, almost everyone uses one of these to make calls, place reminders, schedule meetings, set alarms, surf the internet, etc. These voice assistants have made life much easier. But how do they work? They use a complex combination ofspeech recognition, natural language understanding, and natural language processing to understand what humans are saying and then act on it. The long term goal of voice assistants is to become a bridge between humans and the internet and provide all manner of services based on just voice interaction. However, they are still a little far from that goal seeing as Siri still canâ€™t understand what you are saying sometimes! Want to translate a text from English to Hindi but donâ€™t know Hindi? Well, Google Translate is the tool for you! While itâ€™s not exactly 100% accurate, it is still a great tool to convert text from one language to another. Google Translate and other translation tools as well as use Sequence to sequence modeling that is a technique in Natural Language Processing. Earlier, language translators used Â Statistical machine translation (SMT) which meant they analyzed millions of documents that were already translated from one language to another (English to Hindi in this case) and then looked for the common patterns and basic vocabulary of the language. However, this method was not that accurate as compared to Sequence to sequence modeling. Almost all the world is on social media these days! And companies can usesentiment analysisto understand how a particular type of user feels about a particular topic, product, etc. They can use natural language processing, computational linguistics, text analysis, etc. to understand the general sentiment of the users for their products and services and find out if the sentiment is good, bad, or neutral. Companies can use sentiment analysis in a lot of ways such as to find out the emotions of their target audience, to understand product reviews, to gauge their brand sentiment, etc. Grammar and spelling is a very important factor while writing professional reports for your superiors even assignments for your lecturers. After all, having major errors may get you fired or failed! Thatâ€™s why grammar and spell checkers are a very important tool for any professional writer. They can not only correct grammar and check spellings but also suggest better synonyms and improve the overall readability of your content. And guess what, they utilize natural language processing to provide the best possible piece of writing! The NLP algorithm is trained on millions of sentences to understand the correct format. Emails are still the most important method for professional communication. However, all of us still get thousands of promotional Emails that we donâ€™t want to read. Thankfully, our emails are automatically divided into 3 sections namely, Primary, Social, and Promotions which means we never have to open the Promotional section! But how does this work? Email services use natural language processing to identify the contents of each Email with text classification so that it can be put in the correct section. These are the most popular applications of Natural Language Processing and chances are you may have never heard of them! NLP is used in many other areas such as social media monitoring, translation tools, smart home devices, survey analytics, etc. Chances are you may have used Natural Language Processing a lot of times till now but never realized what it was. But now you know the insane amount of applications of this technology and how itâ€™s improving our daily lives. If you want to learn more about this technology, there are various online courses you can refer to. "
https://www.coursera.org/articles/natural-language-processing,"Natural language processing ensures that AI can understand the natural human languages we speak everyday. Learn more about this impactful AI subfield. Natural language processing (NLP) is a form of artificial intelligence (AI) that allows computers to understand human language, whether it be written, spoken, or even scribbled. As AI-powered devices and services become increasingly more intertwined with our daily lives and world, so too does the impact that NLP has on ensuring a seamless human-computer experience. In this article, youâ€™ll learn more about what NLP is, the techniques used to do it, and some of the benefits it provides consumers and businesses. At the end, youâ€™ll also learn about common NLP tools and explore some online, cost-effective courses that can introduce you to the fieldâ€™s most fundamental concepts.  Natural language processing (NLP)is a subset of artificial intelligence,computer science, and linguistics focused on making human communication, such as speech and text, comprehensible to computers. NLP is used in a wide variety of everyday products and services. Some of the most common ways NLP is used are through voice-activated digital assistants on smartphones, email-scanning programs used to identify spam, and translation apps that decipher foreign languages.      NLP encompasses a wide range of techniques to analyze human language. Some of the most common techniques you will likely encounter in the field include:  Sentiment analysis:An NLP technique that analyzes text to identify its sentiments, such as â€œpositive,â€ â€œnegative,â€ or â€œneutral.â€ Sentiment analysis is commonly used by businesses to better understand customer feedback.  Summarization:An NLP technique that summarizes a longer text, in order to make it more manageable for time-sensitive readers. Some common texts that are summarized include reports and articles.  Keyword extraction:An NLP technique that analyzes a text to identify the most important keywords or phrases. Keyword extraction is commonly used forsearch engine optimization (SEO), social media monitoring, and business intelligence purposes.  Tokenization:The process of breaking characters, words, or subwords down into â€œtokensâ€ that can be analyzed by a program. Tokenization undergirds common NLP tasks like word modeling, vocabulary building, and frequent word occurrence.     Whether itâ€™s being used to quickly translate a text from one language to another or producing business insights by running a sentiment analysis on hundreds of reviews, NLP provides both businesses and consumers with a variety of benefits. Unsurprisingly, then, we can expect to see more of it in the coming years. According to research by Fortune Business Insights, the North American market for NLP is projected to grow from $26.42 billion in 2022 to $161.81 billion in 2029 [1].  Some common benefits of NLP include:  The ability to analyze both structured and unstructured data, such as speech, text messages, and social media posts.  Improving customer satisfaction and experience by identifying insights using sentiment analysis.  Reducing costs by employing NLP-enabled AI to perform specific tasks, such as chatting with customers via chatbots or analyzing large amounts of text data.  Better understanding atarget marketor brand by conducting NLP analysis on relevant data like social media posts, focus group surveys, and reviews.  NLP can be used for a wide variety of applications but it's far from perfect. In fact, many NLP tools struggle to interpret sarcasm, emotion, slang, context, errors, and other types of ambiguous statements. This means that NLP is mostly limited to unambiguous situations that don't require a significant amount of interpretation.   Although natural language processing might sound like something out of a science fiction novel, the truth is that people already interact with countless NLP-powered devices and services every day. Online chatbots, for example, use NLP to engage with consumers and direct them toward appropriate resources or products. While chat bots canâ€™t answer every question that customers may have, businesses like them because they offer cost-effective ways to troubleshoot common problems or questions that consumers have about their products. Another common use of NLP is for text prediction and autocorrect, which youâ€™ve likely encountered many times before while messaging a friend or drafting a document. This technology allows texters and writers alike to speed-up their writing process and correct common typos.  ChatGPT is a chatbot powered by AI and natural language processing that produces unusually human-like responses. Recently, it has dominated headlines due to its ability to produce responses that far outperform what was previously commercially possible. If you'd like to learn more, the University of Michigan'sChatGPT Teach Outbrings together experts on communication technology, the economy, artificial intelligence, natural language processing, healthcare delivery, and law to discuss the impacts of the technology now and into the future. Read more:ChatGPT 101: What Is Generative AI (and How to Use It)    There are numerous natural language processing tools and services available to help you get started today. Some of the most common tools and services you might encounter include the following:  Google Cloud NLP API IBM Watson Amazon Comprehend Python is a programming language well-suited to NLP. Some common Python libraries and toolkits you can use to start exploring NLP include NLTK, Stanford CoreNLP, and Genism.   Read more:What Is Python Used For? A Beginnerâ€™s Guide   Natural language processing helps computers understand human language in all its forms, from handwritten notes to typed snippets of text and spoken instructions. Start exploring the field in greater depth by taking a cost-effective, flexible specialization on Coursera. DeepLearning.AIâ€™sNatural Language Processing Specializationwill prepare you to design NLP applications that perform question-answering and sentiment analysis, create tools to translate languages and summarize text, and even build chatbots. 

In DeepLearning.AIâ€™sMachine Learning Specialization, meanwhile, youâ€™ll master fundamental AI concepts and develop practical machine learning skills in the beginner-friendly, three-course program by AI visionary (and Coursera co-founder) Andrew Ng.            Business Fortune Insights. â€œThe global natural language processing (NLP) marketâ€¦,Â  https://www.fortunebusinessinsights.com/industry-reports/natural-language-processing-nlp-market-101933.â€ Accessed March 28, 2023.              Editorial Team Courseraâ€™s editorial team is comprised of highly experienced professional editors, writers, and fact... This content has been made available for informational purposes only. Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals.   Coursera Footer Get Started with AI Popular Career Certificates Popular Subjects Popular Resources Coursera Community More"
https://online.stanford.edu/courses/xcs224n-natural-language-processing-deep-learning,Error: 403 Client Error: Forbidden for url: https://online.stanford.edu/courses/xcs224n-natural-language-processing-deep-learning
https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP,"Natural language processing (NLP) is the ability of a computer program to understand human language as it's spoken and written -- referred to as natural language. It's a component of artificial intelligence (AI). NLP has existed for more than 50 years and has roots in the field of linguistics. It has a variety of real-world applications in numerous fields, including medical research, search engines and business intelligence. NLP uses eitherrule-based or machine learningapproaches to understand the structure and meaning of text. It plays a role inchatbots, voice assistants, text-based scanning programs, translation applications and enterprise software that aids in business operations, increases productivity and simplifies different processes. NLP uses many different techniques to enable computers to understand natural language as humans do. Whether the language is spoken or written, natural language processing can use AI to take real-world input, process it and make sense of it in a way a computer can understand. Just as humans have different sensors -- such as ears to hear and eyes to see -- computers have programs to read and microphones to collect audio. And just as humans have a brain to process that input, computers have a program to process their respective inputs. At some point in processing, the input is converted to code that the computer can understand.There are two main phases to natural language processing:data preprocessingand algorithm development. This article is part of Data preprocessing involves preparing andcleaningtext data so that machines can analyze it. Preprocessing puts data in a workable form and highlights features in the text that an algorithm can work with. There are several ways this can be done, including the following: Once the data has been preprocessed, an algorithm is developed to process it. There are many different natural language processing algorithms, but the following two main types are commonly used: Businesses use large amounts ofunstructured, text-heavy data and need a way to efficiently process it. Much of the information created online and stored in databases is natural human language, and until recently, businesses couldn't effectively analyze this data. This is where natural language processing is useful. The advantages of natural language processing can be seen when considering the following two statements: ""Cloud computing insurance should be part of every service-level agreement"" and ""A good SLA ensures an easier night's sleep -- even in the cloud."" If a user relies on natural language processing for search, the program will recognize thatcloud computingis an entity, thatcloudis an abbreviated form of cloud computing, and thatSLAis an industry acronym for service-level agreement. These are the types of vague elements that frequently appear in human language and thatmachine learning algorithmshave historically been bad at interpreting. Now, with improvements in deep learning and machine learning methods, algorithms can effectively interpret them. These improvements expand the breadth and depth of data that can be analyzed. Likewise, NLP is useful for the same reasons as when a person interacts with agenerative AIchatbot or AI voice assistant. Instead of needing to use specific predefined language, a user could interact with a voice assistant like Siri on their phone using their regular diction, and their voice assistant will still be able to understand them. Syntax and semantic analysis are two main techniques used in natural language processing. Syntaxis the arrangement of words in a sentence to make grammatical sense.NLP uses syntaxto assess meaning from a language based on grammatical rules. Syntax NLP techniques include the following: This is the grammatical analysis of a sentence. For example, a natural language processing algorithm is fed the sentence, ""The dog barked."" Parsing involves breaking this sentence into parts of speech -- i.e., dog = noun, barked = verb. This is useful for more complex downstream processing tasks. This is the act of taking a string of text and deriving word forms from it. For example, a person scans a handwritten document into a computer. The algorithm can analyze the page and recognize that the words are divided by white spaces. This places sentence boundaries in large texts. For example, a natural language processing algorithm is fed the text, ""The dog barked. I woke up."" The algorithm can use sentence breaking to recognize the period that splits up the sentences. This divides words into smaller parts called morphemes. For example, the worduntestablywould be broken into [[un[[test]able]]ly], where the algorithm recognizes ""un,"" ""test,"" ""able"" and ""ly"" as morphemes. This is especially useful in machine translation and speech recognition. This divides words with inflection in them into root forms. For example, in the sentence, ""The dog barked,"" the algorithm would recognize the root of the word ""barked"" is ""bark."" This is useful if a user is analyzing text for all instances of the word bark, as well as all its conjugations. The algorithm can see that they're essentially the same word even though the letters are different. Semanticsinvolves the use of and meaning behind words. Natural language processing applies algorithms to understand the meaning and structure of sentences. Semantic techniques include the following: This derives the meaning of a word based on context. For example, consider the sentence, ""The pig is in the pen."" The wordpenhas different meanings. An algorithm using this method can understand that the use of the word here refers to a fenced-in area, not a writing instrument. NERdetermines words that can be categorized into groups. For example, an algorithm using this method could analyze a news article and identify all mentions of a certain company or product. Using the semantics of the text, it could differentiate between entities that are visually the same. For instance, in the sentence, ""Daniel McDonald's son went to McDonald's and ordered a Happy Meal,"" the algorithm could recognize the two instances of ""McDonald's"" as two separate entities -- one a restaurant and one a person. NLGuses a database to determine the semantics behind words and generate new text. For example, an algorithm could automatically write a summary of findings from a business intelligence (BI) platform, mapping certain words and phrases to features of the data in the BI platform. Another example would be automatically generating news articles or tweets based on a certain body of text used for training. Current approaches to natural language processing are based on deep learning, a type of AI that examines and uses patterns in data to improve a program's understanding. Deep learning models require massive amounts of labeled data for the natural language processing algorithm to train on and identify relevant correlations, and assembling this kind ofbig dataset is one of the main hurdles to natural language processing. Earlier approaches to natural language processing involved a more rule-based approach, where simpler machine learning algorithms were told what words and phrases to look for in text and given specific responses when those phrases appeared. But deep learning is a more flexible, intuitive approach in which algorithms learn to identify speakers' intent from many examples -- almost like how a child would learn human language. Three open source tools commonly used for natural language processing include Natural Language Toolkit (NLTK), Gensim and NLP Architect by Intel. NLTK is aPythonmodule with data sets and tutorials. Gensim is a Python library for topic modeling and document indexing. NLP Architect by Intel is a Python library for deep learning topologies and techniques. Some of the main functions and NLP tasks that natural language processing algorithms perform include the following: The functions listed above are used in a variety of real-world applications, including the following: The main benefit of NLP is that it improves the way humans and computers communicate with each other. The most direct way to manipulate a computer is through code -- the computer's language. Enabling computers to understand human language makes interacting with computers much more intuitive for humans. Other benefits include the following: There are numerous challenges in natural language processing, and most of them boil down to the fact that natural language is ever-evolving and somewhat ambiguous. They include the following: NLP draws from a variety of disciplines, including computer science and computational linguistics developments dating back to the mid-20th century. Its evolution included the following major milestones: Natural language processing has its roots in this decade, when Alan Turing developed theTuring Testto determine whether or not a computer is truly intelligent. The test involves automated interpretation and the generation of natural language as a criterion of intelligence. NLP was largely rules-based, using handcrafted rules developed by linguists to determine how computers would process language. The Georgetown-IBM experiment in 1954 became a notable demonstration of machine translation, automatically translating more than 60 sentences from Russian to English. The 1980s and 1990s saw the development of rule-based parsing, morphology, semantics and other forms of natural language understanding. The top-down, language-first approach to natural language processing was replaced with a more statistical approach because advancements in computing made this a more efficient way of developing NLP technology. Computers were becoming faster and could be used to develop rules based on linguistic statistics without a linguist creating all the rules. Data-driven natural language processing became mainstream during this decade. Natural language processing shifted from a linguist-based approach to an engineer-based approach, drawing on a wider variety of scientific disciplines instead of delving into linguistics. Natural language processing saw dramatic growth in popularity as a term. NLP processes using unsupervised and semi-supervised machine learning algorithms were also explored. With advances in computing power, natural language processing has also gained numerous real-world applications. NLP also began powering other applications like chatbots and virtual assistants. Today, approaches to NLP involve a combination of classical linguistics and statistical methods. Natural language processing plays a vital part in technology and the way humans interact with it. Though it has its challenges, NLP is expected to become more accurate with more sophisticated models, more accessible and more relevant in numerous industries. NLP will continue to be an important part of both industry and everyday life. As natural language processing is making significant strides in new fields, it's becoming more important for developers to learn how it works. Learn how to develop your skills increating NLP programs. To help you find the right BI software for your analytics needs, here's a look at 20 top tools and the key features that ... Data preparation is a crucial but complex part of analytics applications. Don't let seven common challenges send your data prep ... A year after getting acquired by private equity firms amid declining revenue growth and a slow transition to the cloud, the ... President-elect Donald Trump has been vocal in his criticisms of big tech's content censorship power and President Joe Biden's ... Internal tech procurement can be a risky undertaking. Tech pilot programs can potentially reduce some of that risk, but IT ... Apart from President-elect Donald Trump's promise to take a strong stance on goods imported from China, collaboration might be ... Based on its AI development and expansion within data management, the record $10 billion the vendor raised shows it is viewed ... The data lakehouse pioneer has expanded into AI development and plans to use the funding to fuel further investments in AI, make ... The complementary partnership combines the data integration vendor's discovery and retrieval capabilities with the tech giant's ... The supply chain might be losing its seat at the executive table, but the work must continue to provide more resilience and ... Nothing is certain except death, taxes and supply chain disruptions. Companies should plan accordingly, especially as the ... Essential TMS features are real-time tracking, route optimization, carrier management, load planning, fleet management, automated... All Rights Reserved,Copyright 2018 - 2024, TechTargetPrivacy PolicyCookie PreferencesCookie PreferencesDo Not Sell or Share My Personal Information"
https://www.scribbledata.io/blog/genai-vs-llms-vs-nlp-a-complete-guide/,"Resources/Blogs/GenAI vs. LLMs vs. NLP: A Complete Guide In the early light of artificial intelligence, the world was simple. Machines were taught to mimic basic human tasks. As time moved, so did the ambition of those who programmed these machines. The first whispers of understanding human language emerged in what we now callNatural Language Processing (NLP). It was a modest beginning, a foundation on which something greater could be built. Years passed, and the machines learned to speak more fluently, more like us. They evolved into what we termLarge Language Models (LLMs). These models didnâ€™t just mimic â€“ they began to understand, to respond with a depth that was once thought impossible for cold circuits and silicon.  Now, we stand on the precipice of a new era.Generative AI, the latest offspring of this technological lineage, paints not just in words but in ideas. It is a leap from understanding to creating, from repeating to innovating. In the quiet hum of their processors, these machines now generate art, write poetry, and compose music, encroaching upon realms we once thought were exclusively human. In this article, we will explore how these concepts â€“ NLP, LLMs, and Generative AI â€“ intertwine and diverge, shaping the path of modern AI. Table of Contents  The history of AIâ€™s mastery over language is a relentless pursuit of understanding. It began in the earnest post-war years, the 1950s, marked by theGeorgetown-IBM experiment in 1954. Here, for the first time, a computer translated 60 Russian sentences into English. It was a modest output, but its implications were profound. It suggested a future where machines could bridge language barriers. The 1960s introduced a pivotal figure,Noam Chomsky. His theory of â€˜universal grammarâ€™ provided a structured approach to understanding language â€“ a framework that could be encoded into machines. This era saw NLP focusing on the rules and structures of language, evolving from simple word-for-word substitution to understanding syntax and grammar. As we moved into the 1980s and 1990s, the field of NLP expanded, incorporating statistical methods. This period marked a shift from rule-based systems to those that learned from large datasets. The â€™90s were a renaissance for NLP, with the advent of machine learning techniques that allowed for more nuanced language understanding and generation. Parallel to the evolution of NLP, the concept of Generative AI began to take shape. The 1960s had already seen early examples likeELIZA, a simple chatbot that could mimic human conversation. It was in the 2010s that Generative AI truly blossomed, fueled by advancements in neural networks and an explosion in computational power. This era saw the creation of AI models that could generate realistic images, compose music, and even write coherent pieces of text. The journey of Large Language Models (LLMs) began in earnest in the late â€™80s and â€™90s, with companies like IBM leading the development of smaller language models. These early models laid the groundwork for what was to come. However, it was in 2001 that a significant leap occurred with theintroduction of the first neural language model. This model used neural networks to process and generate language, marking a departure from the rule-based systems of the past. As we entered the 2010s, LLMs likeOpenAIâ€™s GPT 3.5/4andGoogleâ€™s BERTrepresented monumental leaps. These models, trained on vast datasets, could not only understand and generate language with unprecedented fluency but also perform a variety of language tasks, from translation to question-answering. GenAI, LLMs), and NLP each play a role in the grander scheme of machine intelligence, like unique instruments in an orchestra, creating a symphony of digital language and creativity.  NLP is the bedrock, the foundational element in this trinity. It is the domain of AI that enables machines to understand, interpret, and respond to human language. At its core, NLP uses algorithms to process and analyze human language data â€“ turning the sprawling, chaotic wilderness of our words into structured, understandable information. Early NLP systems were rule-based, relying on sets of hand-coded rules to interpret language. However, the field has evolved. Modern NLP uses statistical and machine learning techniques, allowing machines to learn language patterns from vast datasets. This learning enables NLP systems to perform tasks like sentiment analysis, language translation, and speech recognition.  Enter LLMs, the next evolution. These are highly sophisticated models, a step above basic NLP. LLMs like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) are built upon the foundation laid by NLP. They use deep learning, a subset of machine learning, to process and generate human language on a vast scale. What sets LLMs apart is their size and scope. They are trained on enormous datasets, encompassing a wide swath of human language from books, articles, and websites. This extensive training allows them to generate text that is remarkably coherent and contextually relevant, handling tasks that range from writing articles to engaging in conversations.  Generative AI is the creative apex of this trio. While it encompasses LLMs, its domain extends beyond language. Generative AI refers to AI systems that can generate new content, be it text, images, music, or even video. It uses advanced algorithms, often a form of deep learning known as Generative Adversarial Networks (GANs), to create new content that is original yet plausible. In language, Generative AI takes the capabilities of LLMs and pushes them into the realm of creativity. Itâ€™s not just about understanding or generating coherent text â€“ it is about creating something new, be it a story, a poem, or a block of code. While they share a common thread, NLP, LLMs, and Generative AI have distinct roles. NLP is about understanding and processing language, a fundamental necessity. LLMs build on this, using the capabilities of NLP to generate language that is not only coherent but contextually relevant. Generative AI, meanwhile, takes the baton and runs further. It uses the language generation capabilities of LLMs but extends into creating novel content across various mediums. Its scope is broader, and its potential more far-reaching. Yet, these distinctions are not clear-cut. LLMs are a subset of NLP, and Generative AI often relies on the language capabilities of LLMs. They are interconnected, each building on the advancements of the others. Each of these technologies has its role, serving distinct purposes. Let us delve into the practical applications of each, exploring how they transform various industries and activities.   Each of these technologies, with its unique capabilities, is reshaping how we interact with language, create content, and analyze data. Various tools and platforms have emerged, each specializing in NLP, LLMs, and Generative AI. Here, we dive into a comparative analysis, exploring their unique functionalities, strengths, and weaknesses.  Other Notable Tools:TextBlobis excellent for quick and easy text processing tasks.CoreNLPoffers comprehensive linguistic analysis, ideal for researchers.  Other Notable Platforms:Googleâ€™s T5is adept at converting text inputs into different desired outputs.Microsoftâ€™s Turing-NLGis a significant player in large-scale language modeling.  Other Notable Tools:DALL-Eexcels in creative image generation, pushing the boundaries of AI-driven art.WaveNetby DeepMind is renowned for generating realistic and human-like speech, a breakthrough in speech synthesis. While each tool and platform has its niche, the choice often boils down to the specific requirements of a project. NLTK and spaCy, for instance, are excellent for foundational NLP tasks, but integrating them with deep learning models is essential for more complex applications. In contrast, GPT-3.5/4 and BERT, while powerful, require careful handling to mitigate issues like bias and inaccuracies. For creative endeavors, tools like DeepArt and RunwayML demonstrate the potential of Generative AI in art and multimedia, though they serve different user needs in terms of simplicity versus customization.  Letâ€™s venture into the potential advancements and ongoing research in these fields and try to see what lies ahead. The collective trajectory of NLP, LLMs, and Generative AI points towards a more interconnected, intelligent, and creative AI landscape. The potential advancements in these fields promise not only technological breakthroughs but also a deeper understanding and enhancement of human capabilities. As research continues to push the boundaries, the future of these AI domains is poised to be as exciting as it is transformative, shaping the way we interact with technology and each other. Your email address will not be published.Required fields are marked* Comment* Name* Email* Website  Table of Contents Imagine telling an insurance executive in the 1970s that, in the not-so-distant future, they would be crafting group benefit plans that include coverage for mindfulness app subscriptions, pet insurance, or even student loan repayment assistance. They might have chuckled at the absurdityâ€”or marveled at the complexity. Yet here we are in 2024, navigating a landscape [â€¦] The insurance industry stands at a crossroads. The global protection gap, a measure of uninsured risk, looms large. By 2025, it will reach $1.86 trillion. This is not just a number. It represents real people and businesses exposed to financial ruin. The old models of insurance are failing to keep pace with a rapidly changing [â€¦] As the sun rose on a crisp autumn morning in 2022, pension fund managers worldwide awoke to a startling new reality. The gilts crisis that had rocked UK financial markets had not only sent shockwaves through the economy but also dramatically reshaped the landscape of defined benefit pension schemes. For many, the path to buyoutâ€”once [â€¦] Sign up to our newsletter and get exclusive access to our launches and updates CanadaScribble Data Inc55, York Street,#401, Toronto ONM5J 1R7 India2074 (#17) 16 D Main,H.A.L 2nd Stage,Indiranagar,Bangalore 560008 United States447 Broadway,2nd Floor Suite #563,New York 10013 Scribble Data builds for trust at the system,organization, and product levels.Talk to us to learn morehello@scribbledata.io Copyright Â© 2024 ScribbleData. All rights reserved."
https://online.york.ac.uk/the-role-of-natural-language-processing-in-ai/,Error: 403 Client Error: Forbidden for url: https://online.york.ac.uk/the-role-of-natural-language-processing-in-ai/
https://www.datacamp.com/blog/what-is-natural-language-processing,Error: 403 Client Error: Forbidden for url: https://www.datacamp.com/blog/what-is-natural-language-processing
https://www.geeksforgeeks.org/phases-of-natural-language-processing-nlp/,"Natural Language Processing (NLP) is a field within artificial intelligence that allows computers to comprehend, analyze, and interact with human language effectively. The process of NLP can be divided into five distinct phases: Lexical Analysis, Syntactic Analysis, Semantic Analysis, Discourse Integration, and Pragmatic Analysis. Each phase plays a crucial role in the overall understanding and processing of natural language. In this article, we are going to explore the phases of Natural Language Processing in detail. Phases of Natural Language Processing (NLP) The lexical phase inNatural Language Processing (NLP)involves scanning text and breaking it down into smaller units such as paragraphs, sentences, and words. This process, known astokenization, converts raw text into manageable units called tokens or lexemes. Tokenization is essential for understanding and processing text at the word level. In addition to tokenization, various data cleaning and feature extraction techniques are applied, including: These steps enhance the comprehensibility of the text, making it easier to analyze and process. Morphological analysis is another critical phase in NLP, focusing on identifying morphemes, the smallest units of a word that carry meaning and cannot be further divided. Understanding morphemes is vital for grasping the structure of words and their relationships. Morphological analysis is crucial in NLP for several reasons: By identifying and analyzing morphemes, the system can interpret text correctly at the most fundamental level, laying the groundwork for more advanced NLP applications. Syntactic analysis, also known as parsing, is the second phase of Natural Language Processing (NLP). This phase is essential for understanding the structure of a sentence and assessing its grammatical correctness. It involves analyzing the relationships between words and ensuring their logical consistency by comparing their arrangement against standard grammatical rules. Parsing examines the grammatical structure and relationships within a given text. It assignsParts-Of-Speech (POS) tagsto each word, categorizing them as nouns, verbs, adverbs, etc. This tagging is crucial for understanding how words relate to each other syntactically and helps in avoiding ambiguity. Ambiguity arises when a text can be interpreted in multiple ways due to words having various meanings. For example, the word â€œbookâ€ can be a noun (a physical book) or a verb (the action of booking something), depending on the sentence context. Consider the following sentences: Despite using the same words, only the first sentence is grammatically correct and makes sense. The correct arrangement of words according to grammatical rules is what makes the sentence meaningful. During parsing, each word in the sentence is assigned a POS tag to indicate its grammatical category. Hereâ€™s an example breakdown: Assigning POS tags correctly is crucial for understanding the sentence structure and ensuring accurate interpretation of the text. By analyzing and ensuring proper syntax, NLP systems can better understand and generate human language. This analysis helps in various applications, such as machine translation, sentiment analysis, and information retrieval, by providing a clear structure and reducing ambiguity. Semantic Analysisis the third phase of Natural Language Processing (NLP), focusing on extracting the meaning from text. Unlike syntactic analysis, which deals with grammatical structure, semantic analysis is concerned with the literal and contextual meaning of words, phrases, and sentences. Semantic analysis aims to understand the dictionary definitions of words and their usage in context. It determines whether the arrangement of words in a sentence makes logical sense. This phase helps in finding context and logic by ensuring the semantic coherence of sentences. Consider the following examples: Semantic analysis is essential for various NLP applications, including machine translation, information retrieval, and question answering. By ensuring that sentences are not only grammatically correct but also meaningful, semantic analysis enhances the accuracy and relevance of NLP systems. Discourse Integration is the fourth phase of Natural Language Processing (NLP). This phase deals with comprehending the relationship between the current sentence and earlier sentences or the larger context. Discourse integration is crucial for contextualizing text and understanding the overall message conveyed. Discourse integration examines how words, phrases, and sentences relate to each other within a larger context. It assesses the impact a word or sentence has on the structure of a text and how the combination of sentences affects the overall meaning. This phase helps in understanding implicit references and the flow of information across sentences. In conversations and texts, words and sentences often depend on preceding or following sentences for their meaning. Understanding the context behind these words and sentences is essential to accurately interpret their meaning. Consider the following examples: Discourse integration is vital for various NLP applications, such as machine translation, sentiment analysis, and conversational agents. By understanding the relationships and context within texts, NLP systems can provide more accurate and coherent responses. Pragmatic Analysis is the fifth and final phase of Natural Language Processing (NLP), focusing on interpreting the inferred meaning of a text beyond its literal content. Human language is often complex and layered with underlying assumptions, implications, and intentions that go beyond straightforward interpretation. This phase aims to grasp these deeper meanings in communication. Pragmatic analysis goes beyond the literal meanings examined in semantic analysis, aiming to understand what the writer or speaker truly intends to convey. In natural language, words and phrases can carry different meanings depending on context, tone, and the situation in which they are used. In human communication, people often do not say exactly what they mean. For instance, the word â€œHelloâ€ can have various interpretations depending on the tone and context in which it is spoken. It could be a simple greeting, an expression of surprise, or even a signal of anger. Thus, understanding the intended meaning behind words and sentences is crucial. Consider the following examples: Pragmatic analysis is essential for applications like sentiment analysis, conversational AI, and advanced dialogue systems. By interpreting the deeper, inferred meanings of texts, NLP systems can understand human emotions, intentions, and subtleties in communication, leading to more accurate and human-like interactions. The phases of NLPâ€”Lexical Analysis, Syntactic Analysis, Semantic Analysis, Discourse Integration, and Pragmatic Analysisâ€”each play a critical role in enabling computers to process and understand human language. By breaking down the text into manageable parts and analyzing them in different ways, NLP systems can perform complex tasks such as machine translation, sentiment analysis, and information retrieval, making significant advancements in human-computer interaction. V "
https://www.geeksforgeeks.org/ai-tools-for-natural-language-processing/,"A Natural Language Processing (NLP)is a form of computation concerned with free AI Tools for NLP whereby any form of signal, statistics, or machine learning program from human language combines them into text or voice data. AI Tools for NLP perform a set of functionalities such as processing data on its own and understanding the context with the generation of data as well. It is a collection of linguistic data, breaking down texts into readable forms or tokens by assigning grammatical tokens and thus performing a running analysis.  There are certainAI Tools for NLPto perform such tasks which we are discussing in this article best AI Tools for NLP along with their features, pros & cons, etc. AI tools work asNatural Language ProcessingToolsand it has a rapid growth in this field. In the early 1950s, these systems were introduced and certain linguistic rules were formed but had very limited features. It advanced in the year 2000 when various new models were introduced and theHidden Markov Modelwas one of them, which allowed the NLP system. They were known for their analytical power with automatic learning patterns. It continued to be supervised as Support Vector Machines were launched. They became fairer withLarge Language modelsin 2019.With deep learning sequence tasks applied, in 2020 multimodal was introduced to incorporate new features in a holistic approach marking AIâ€™s Evolution in NLP Tools. Top 12 AI Tools for Natural Language Processing (NLP): 2024 AI tools for NLP play a key role in language translation, text summarization, fraud detection, searching certain information, speed detection, and answering certain questions as well. They have been a great help to the forward generation for authentic information. Let us look into theTop 12 AI tools for Natural Language Processing in 2024and their features: MonkeyLearn is considered as a solution that helps a person to extract data that are inside any Gmail, tweets, or from any sentence that is in written form.The extracted data is further converted into visualization which is to be presented to the user for picture-directed work.  Features: Pros: Cons: Pricing: The most famous AI tool for NLP is spaCY is considered an open-source library that helps in natural language processing in Python.This platform helps in the extraction of information and provides it for NLP which is written in Python.  Features: Pros: Cons: Pricing: Stanford CoreNLP is a type of backup download page that is also used in language analysis tools in Java. It takes the raw input of human language and analyzes the data into different sentences in terms of phrases or dependencies.  Features: Pros: Cons: Pricing: MindMeld is considered a language conversation platform that assists in having a conversational understanding of the domain and other algorithms.  Features: Pros: Cons: Pricing: Amazon Comprehend has the feature of AI on NLP offers natural language processing, PII detection and redaction, Custom Classification and Entity detection, and topic modeling, allowing a wide range of applications.  Features: Pros: Cons: Pricing: OpenAI is advanced AI tool on NLP with machine learning, NLP, robotics, and deep learning programs.It is responsible for developing generative models with solutions.  Features: Pros: Cons: Pricing: It is a leading AI on NLP with cloud storage features processing diverse applications within. It also acts as a text analyst with sentiment analysis and speech recognition. NLP is offered with generating text and understanding languages.  Features: Pros: Cons: Pricing: Google Cloud has the same infrastructure as Google with its developed applications and offers a platform for custom services for cloud computing.It helps in storage with Google Kubernetes Engines. It also has advanced features of AI for NLP.  Features: Pros: Cons: Pricing: One of the common AI tools for NLP is IBM Watson the service developed by IBM for NLP for comprehension of texts in various languages.It is accurate an highly focused on transfer learning and deep learning techniques.  Features: Pros: Cons: Pricing: Gensim is used by data scientists as an open source with a variety of algorithms and random projections.It is known for data streaming with process large corpora. It relies on Python implementations, and considered as one of the top AI tool for NLP.  Features: Pros: Cons: Pricing: PyTorch is an optimizer with dynamic features assuming static behavior, and recompiling data sizes.It has compatibility with Python and helps in the Bfloat inference path acting as AI on NLP.  Features: Pros: Cons: Pricing: The last AI tool on NLP is FireEye Helix offers a pipeline and is software with features of a tokenizer and summarizer.It has a memory with certain requirements such as name entity tagging and uses cached copy to run fast.  Features: Pros: Cons: Pricing: AI tools play a key role in using various techniques with deep learning, machine learning, and statistical models.The tools are highly advanced and well worse with the training on large datasheets with certain patterns. Thus, they help in tasks such as translation, analysis, text summarization, and sentiment analysis. SpaCy is the best AI Cybersecurity tool as it provides accuracy and reliability with an open library designed for processing data analysis and entity recognition.It is also known for its speech tagging and pre-trained models. AI on NLP has undergone evolution and development as they become an integral part of building accuracy in multilingual models. They combine languages and help in image, text, and video processing. They are revolutionary models or tools helpful for human language in many ways such as in the decision-making process, automation and hence shaping the future as well. These AI Tools for NLP are continuously being refined for future endeavors and with the expansion of capabilities, it becomes more user friendly. The accuracy of the tool depends on the said feature and control or the functioning which is given to the tool. It also includes the quality of training and data based on transformer architectures. NLP AI tools can understand the emotional rate expressed and hence identify positive or neutral tones based on the customerâ€™s given functions and operations. Applications shall be translating texts into various languages, text generation, text summarizations, performing analysis functions, and data extraction with chat boxes and virtual assistants. The power of analysis is developed to real-time;  They use training, language identifiers, fine tunings, Parallel Corpora, multilingual functionaries, and models as data and embedding help in translation in multiple languages. A N N A A S A J V A A "
https://www.geeksforgeeks.org/top-7-applications-of-natural-language-processing/,"In the past, did you ever imagine that you could talk to your phone and get things done?Or that your phone would talk back to you! This has become a pretty normal thing these days with Siri, Alexa, Google Assistant, etc. You can ask any possible questions ranging from â€œWhatâ€™s the weather outsideâ€ to â€œWhatâ€™s your favorite color?â€ from Siri and youâ€™ll get an answer. All of this and more is accomplished usingNatural Language Processing. And not only that, there are many other applications of Natural Language Processing these days including the translator on your phone or the grammar checker you use before sending Emails.  Natural Language Processing allows your device to hear what you say, then understand the hidden meaning in your sentence, and finally act on that meaning. And all of this is completed in 5 seconds! But the question this brings is What exactly is Natural Language Processing? And how does it work? So letâ€™s see the answer to this first. Natural Language Processing is a part of artificial intelligence that aims to teach the human language with all its complexities to computers. This is so that machines can understand and interpret the human language to eventually understand human communication in a better way. Natural Language Processing is a cross among many different fields such asartificial intelligence, computational linguistics, human-computer interaction, etc. There are many different methods in NLP to understand human language which include statistical and machine learning methods. And why is Natural Language Processing important, you wonder? Well, it allows computers to understand human language and then analyze huge amounts of language-based data in an unbiased way. This is the reason that Natural Language Processing has many diverse applications these days in fields ranging from IT to telecommunications to academics. So, letâ€™s see these applications now. Chatbots are a form of artificial intelligence that are programmed to interact with humans in such a way that they sound like humans themselves. Depending on the complexity of the chatbots, they can either just respond to specific keywords or they can even hold full conversations that make it tough to distinguish them from humans.Chatbotsare created using Natural Language Processing andMachine Learning, which means that they understand the complexities of the English language and find the actual meaning of the sentence and they also learn from their conversations with humans and become better with time. Chatbots work in two simple steps. First, they identify the meaning of the question asked and collect all the data from the user that may be required to answer the question. Then they answer the question appropriately. Have you noticed that search engines tend to guess what you are typing and automatically complete your sentences? For example, On typing â€œgameâ€ in Google, you may get further suggestions for â€œgame of thronesâ€, â€œgame of lifeâ€ or if you are interested in maths then â€œgame theoryâ€. All these suggestions are provided using autocomplete that uses Natural Language Processing to guess what you want to ask. Search engines use their enormous data sets to analyze what their customers are probably typing when they enter particular words and suggest the most common possibilities. They use Natural Language Processing to make sense of these words and how they are interconnected to form different sentences. These days voice assistants are all the rage! Whether its Siri, Alexa, or Google Assistant, almost everyone uses one of these to make calls, place reminders, schedule meetings, set alarms, surf the internet, etc. These voice assistants have made life much easier. But how do they work? They use a complex combination ofspeech recognition, natural language understanding, and natural language processing to understand what humans are saying and then act on it. The long term goal of voice assistants is to become a bridge between humans and the internet and provide all manner of services based on just voice interaction. However, they are still a little far from that goal seeing as Siri still canâ€™t understand what you are saying sometimes! Want to translate a text from English to Hindi but donâ€™t know Hindi? Well, Google Translate is the tool for you! While itâ€™s not exactly 100% accurate, it is still a great tool to convert text from one language to another. Google Translate and other translation tools as well as use Sequence to sequence modeling that is a technique in Natural Language Processing. Earlier, language translators used Â Statistical machine translation (SMT) which meant they analyzed millions of documents that were already translated from one language to another (English to Hindi in this case) and then looked for the common patterns and basic vocabulary of the language. However, this method was not that accurate as compared to Sequence to sequence modeling. Almost all the world is on social media these days! And companies can usesentiment analysisto understand how a particular type of user feels about a particular topic, product, etc. They can use natural language processing, computational linguistics, text analysis, etc. to understand the general sentiment of the users for their products and services and find out if the sentiment is good, bad, or neutral. Companies can use sentiment analysis in a lot of ways such as to find out the emotions of their target audience, to understand product reviews, to gauge their brand sentiment, etc. Grammar and spelling is a very important factor while writing professional reports for your superiors even assignments for your lecturers. After all, having major errors may get you fired or failed! Thatâ€™s why grammar and spell checkers are a very important tool for any professional writer. They can not only correct grammar and check spellings but also suggest better synonyms and improve the overall readability of your content. And guess what, they utilize natural language processing to provide the best possible piece of writing! The NLP algorithm is trained on millions of sentences to understand the correct format. Emails are still the most important method for professional communication. However, all of us still get thousands of promotional Emails that we donâ€™t want to read. Thankfully, our emails are automatically divided into 3 sections namely, Primary, Social, and Promotions which means we never have to open the Promotional section! But how does this work? Email services use natural language processing to identify the contents of each Email with text classification so that it can be put in the correct section. These are the most popular applications of Natural Language Processing and chances are you may have never heard of them! NLP is used in many other areas such as social media monitoring, translation tools, smart home devices, survey analytics, etc. Chances are you may have used Natural Language Processing a lot of times till now but never realized what it was. But now you know the insane amount of applications of this technology and how itâ€™s improving our daily lives. If you want to learn more about this technology, there are various online courses you can refer to. "
https://online.york.ac.uk/the-role-of-natural-language-processing-in-ai/,Error: 403 Client Error: Forbidden for url: https://online.york.ac.uk/the-role-of-natural-language-processing-in-ai/
https://toloka.ai/blog/difference-between-ai-ml-llm-and-generative-ai/,"Products Success Stories Resources Impact on AI Company Talk to us Log in Log in Log in Test your LLM's math skills with our benchmark for complex problems and step-by-step reasoning Learn more Test your LLM's math skills with our benchmark for complex problems and step-by-step reasoning Learn more Test your LLM's math skills with our benchmark for complex problems and step-by-step reasoning Learn more Toloka Team Aug 27, 2023 Aug 27, 2023 Essential ML Guide Essential ML Guide For those who are new to the field of artificial intelligence, grasping the many complex terms associated with it can prove to be quite overwhelming. Artificial Intelligence (AI), Machine Learning (ML), Large Language Models (LLMs), and Generative AI are all related concepts in the field of computer science, but there are important distinctions between them. They have significant differences in their functionality and applications. We will take a closer look at these concepts and gain a better understanding of their distinctions further. AI belongs to the field of computer science that deals with the development of computer systems that can perform tasks that typically require human intelligence, such as speech recognition, natural language processing (NLP), text generation and translation, video, sound, and image generation, decision making, planning, and more. AI, in general, refers to the development of intelligent systems that can mimic human behavior and decision-making processes. It encompasses techniques and approaches enabling machines to perform tasks, analyze visual and textual data, and respond or adapt to their environment. One of the key advantages of artificial intelligence is its ability to process large amounts of data and find patterns in it. AI tools are designed to make decisions or take actions based on that knowledge. AI has applications in many fields including marketing, medicine, finance, science, education, industry, and many others. For example, in marketing it is applied to generate marketing materials, in medicine it is utilized to diagnose diseases, and in finance, it is used to analyze financial markets and make investment decisions. There are a handful of types and classifications of AI, including one based on the so-called AI evolution. According to this hypothetical evolution classification, all forms of AI existing now are considered weak AI because they are limited to a specific or narrow area of cognition. Weak AI lacks human consciousness, although it can simulate it in some situations. The next stage of AI development may be a conceptual (so far) form called strong AI or artificial general intelligence, endowed with human consciousness and capable of performing human tasks, constructing mental abilities, reasoning, and learning from experience. It will no longer â€œmimicâ€ human behavior, it will practically become a real thinking being. The peak of AI development may result in Super AI, which would outperform humans in all areas and may even become the cause of human extinction. But for now, this is only a hypothesis. Artificial Intelligence can also be categorized into discriminative and generative. Discriminative and generative AI are two different approaches to building AI systems. Discriminative AI focuses on learning the boundaries that separate different classes or categories in the training data. These models do not aim to generate new samples, but rather to classify or label input data based on what class it belongs to. Discriminative models are trained to identify the patterns and features that are specific to each class and make predictions based on those patterns. Discriminative models are often used for tasks like classification or regression, sentiment analysis, and object detection. Examples of discriminative AI include algorithms like logistic regression, decision trees, random forests and so on. In contrast to discriminative AI, Generative AI focuses on building models that can generate new data similar to the training data it has seen. Generative models learn the underlying probability distribution of the training data and can then generate new samples from this learned distribution. Generative AI tools are capable of image synthesis, text generation, or even music. Such systems typically involve deep learning and neural networks to learn patterns and relationships in the training data. They use that knowledge to create new content. Examples of generative AI models include Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), transformer and diffusion models, and many more. Generative AI is inconceivable without foundation models, that play a significant role in advancing it. They are large-scale algorithms that serve as the backbone of AI systems. By leveraging the learned knowledge of foundation models, generative AI systems can generate high-quality and contextually relevant content. These models have seen tremendous progress recently, allowing them to generate human-like text, answer questions, write essays, create stories, and much more. Through the utilization of a foundational model, we have the capacity to craft more specialized and advanced models that are specifically designed for particular domains or use cases. For instance, generative AI can utilize foundation models as a core for creating large language models. By leveraging the knowledge learned from training on vast amounts of text data, generative AI can generate coherent and contextually relevant text, often resembling human-generated content. Generative AI, which can generate new content or create new information, is becoming increasingly valuable in today's business landscape. It can be used to create high-quality marketing materials, and various business documents ranging from official email templates to annual reports, social media posts, product descriptions, articles, and so on. Generative AI can help businesses automate content creation and achieve scalability without compromising on quality. Such systems are already being incorporated into numerous business applications. Machine Learning is a specific subset or application of AI that focuses on providing systems the ability to learn and improve from experience without being explicitly programmed. ML is a critical component of many AI systems. ML algorithms are used to train AI models by providing them with datasets containing labeled examples or historical data. The model then learns the underlying patterns in the training data, enabling it to make accurate predictions or decisions on new, unseen data. By continuously feeding data to ML models, they can adapt and improve their performance over time. AI encompasses the broader concept of developing intelligent machines, while ML focuses on training systems to learn and make predictions from data. AI aims to replicate human-like behavior, while ML enables machines to automatically learn patterns from data. A machine learning model in AI is a mathematical representation or algorithm that is trained on a dataset to make predictions or take actions without being explicitly programmed. It is a fundamental component of AI systems as it enables computers to learn from data and improve performance over time. Generative AI is a broad concept encompassing various forms of content generation, while LLM is a specific application of generative AI. Large language models serve as foundation models, providing a basis for a wide range of natural language processing (NLP) tasks. Generative AI can encompass a range of tasks beyond language generation, including image and video generation, music composition, and more. Large language models, as one specific application of generative AI, are specifically designed for tasks revolving around natural language generation and comprehension. Large language models operate by using extensive datasets to learn patterns and relationships between words and phrases. They have been trained on vast amounts of text data to learn the statistical patterns, grammar, and semantics of human language. This vast amount of text may be taken from the Internet, books, and other sources to develop a deep understanding of human language. An LLM can take a given input (a sentence or a prompt) and generate a response: coherent and contextually relevant sentences or even paragraphs based on a given prompt or input. The model uses various techniques, including attention mechanisms, transformers, and neural networks, to process the input and generate an output that aims to be coherent and contextually appropriate. Both generative AI and large language models involve the use of deep learning and neural networks. While generative AI aims to create original content across various domains, large language models specifically concentrate on language-based tasks and excel in understanding and generating human-like text. Large language models can perform a wide range of language tasks, including answering questions, writing articles, translating languages, and creating conversational agents, making them extremely valuable tools for various industries and applications. By providing prompt or specific instructions, developers can utilize these large language models as code generation tools to write code snippets, functions, or even entire programs. This can be useful for automating repetitive tasks, prototyping, or exploring new ideas quickly. Code generation with large language models has the potential to greatly assist developers, saving time and effort in generating boilerplate code, exploring new techniques, or assisting with knowledge transfer. However, it's important to judiciously use these models in software development, validate the output, and maintain a balance between automation and human expertise. Companies are employing large language models to develop intelligent chatbots. They can enhance customer service by offering quick and accurate responses, improving customer satisfaction, and reducing human workload. Large language models can help businesses automate content creation processes, as well as save time and resources. Additionally, language models assist in content arrangement by analyzing and summarizing large volumes of information from various sources. Businesses process and analyze unstructured text data more effectively with the help of large language models. They can fulfill tasks like text classification, information extraction, sentiment analysis, and more. All of this plays a big role in understanding customer behavior and predicting market trends. Here are some popular large language models which have revolutionized many NLP tasks and have applications in chatbots, virtual assistants, content creation, and machine translation, among others: Developed by OpenAI, GPT-4 is one of the largest publicly available LLM models. It is a language model which is an extension of the GPT-3. It has been trained on a large amount of data and has higher accuracy and ability to generate text than previous models. The system can read, analyze or generate up to 25,000 words of text. The exact number of GPT-4 parameters is unknown, but according to some researchers it has approximately 1.76 trillion of them. GLaM is an advanced conversational AI model with 1.2 trillion parameters developed by Google. It is designed to generate human-like responses to user prompts and simulate text-based conversations. GLaM is trained on a wide range of internet text data, making it capable of understanding and generating responses on various topics. It aims to produce coherent and contextually relevant responses, leveraging the vast knowledge it has learned from its training data. Developed by Google, BERT is another widely-used LLM model with 340 million parameters. BERT is a pre-trained model that excels at understanding and processing natural language data. It has been used in various applications, including text classification, entity recognition, and question-answering systems. LLaMA (Large Language Model Meta AI) NLP model with billions of parameters and trained in 20 languages released by Meta. The model is accessible to all for non-commercial use. LLaMA has the capability to have conversations and engage in creative writing, making it a versatile language model. Overall, the operation of LLMs involves complex computations and sophisticated algorithms to generate coherent and contextually relevant text based on the given input. Such systems have a wide range of applications, including text completion, translation, chatbots, content generation, and more. Artificial Intelligence (AI), Machine Learning (ML), Large Language Models (LLMs), and Generative AI are all related concepts in the field of computer science, but there are important distinctions between them. Understanding the differences between these terms is crucial as they represent different vital aspects and features in AI. In summary, AI is a broad field covering the development of systems that simulate intelligent behavior. It encompasses various techniques and approaches, while machine learning is a subfield of AI that focuses on designing algorithms that enable systems to learn from data. Large language models are a specific type of ML model trained on text data to generate human-like text, and generative AI refers to the broader concept of AI systems capable of generating various types of content. ML, LLMs, Generative AI: these are just a few of the many terms used in AI. Gaining insight into these distinctions is essential for comprehending the unique characteristics and uses of AI, ML, LLMs, and Generative AI within the constantly changing world of technology. As the AI landscape continues to evolve, new concepts will inevitably appear and the terminology we employ to characterize these systems will transform in the future. Toloka is a European company based in Amsterdam, the Netherlands that provides data for Generative AI development. Toloka empowers businesses to build high quality, safe, and responsible AI. We are the trusted data partner for all stages of AI development from training to evaluation. Toloka has over a decade of experience supporting clients with its unique methodology and optimal combination of machine learning technology and human expertise, offering the highest quality and scalability in the market. ArticleÂ writtenÂ by: Toloka Team Updated: Aug 27, 2023 Case studies, product news, and other articles straight to your inbox. Subscribe Case studies, product news, and other articles straight to your inbox. Subscribe Case studies, product news, and other articles straight to your inbox. Subscribe Back to top View all articles LLM observability Dec 17, 2024 Toloka Platform Relaunch Dec 17, 2024 LLM fine-tuning: unlocking the true potential of large language models Dec 13, 2024 What is Tolokaâ€™s mission? Where is Toloka located? What is Tolokaâ€™s key area of expertise? How long has Toloka been in the AI market? How does Toloka ensure the quality and accuracy of the data collected? How does Toloka source and manage its experts and AI tutors? What types of projects or tasks does Toloka typically handle? What industries and use cases does Toloka focus on? What is Tolokaâ€™s mission? Where is Toloka located? What is Tolokaâ€™s key area of expertise? How long has Toloka been in the AI market? How does Toloka ensure the quality and accuracy of the data collected? How does Toloka source and manage its experts and AI tutors? What types of projects or tasks does Toloka typically handle? What industries and use cases does Toloka focus on? What is Tolokaâ€™s mission? Where is Toloka located? What is Tolokaâ€™s key area of expertise? How long has Toloka been in the AI market? How does Toloka ensure the quality and accuracy of the data collected? How does Toloka source and manage its experts and AI tutors? What types of projects or tasks does Toloka typically handle? What industries and use cases does Toloka focus on? Products Data for LLM Post-Training Data Labeling AI Evaluation AI Safety & Red Teaming Data types Image Video Text Audio ReSources Blog Events Success Stories Security and Privacy Pricing Impact on AI Toloka Research Responsible AI Education Partnerships Company About Us Partnerships Newsroom Careers Contact Brand Guidelines Â© 2024 Toloka AI BV Manage cookies Privacy Notice Terms of Use Code of Conduct Products Data for LLM Post-Training Data Labeling AI Evaluation AI Safety & Red Teaming Data types Image Video Text Audio ReSources Blog Events Success Stories Security and Privacy Pricing Impact on AI Toloka Research Responsible AI Education Partnerships Company About Us Partnerships Newsroom Careers Contact Brand Guidelines Â© 2024 Toloka AI BV Manage cookies Privacy Notice Terms of Use Code of Conduct Products Data for LLM Post-Training Data Labeling AI Evaluation AI Safety & Red Teaming Data types Image Video Text Audio ReSources Blog Events Success Stories Security and Privacy Pricing Impact on AI Toloka Research Responsible AI Education Partnerships Company About Us Partnerships Newsroom Careers Contact Brand Guidelines Â© 2024 Toloka AI BV Manage cookies Privacy Notice Terms of Use Code of Conduct"
https://en.wikipedia.org/wiki/Natural_language_processing,"Natural language processing(NLP) is a subfield ofcomputer scienceand especiallyartificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded innatural languageand is thus closely related toinformation retrieval,knowledge representationandcomputational linguistics, a subfield oflinguistics. Typically data is collected intext corpora, using either rule-based, statistical or neural-based approaches inmachine learninganddeep learning. Major tasks in natural language processing arespeech recognition,text classification,natural-language understanding, andnatural-language generation. Natural language processing has its roots in the 1950s.[1]Already in 1950,Alan Turingpublished an article titled ""Computing Machinery and Intelligence"" which proposed what is now called theTuring testas a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. The premise of symbolic NLP is well-summarized byJohn Searle'sChinese roomexperiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction ofmachine learningalgorithms for language processing.  This was due to both the steady increase in computational power (seeMoore's law) and the gradual lessening of the dominance ofChomskyantheories of linguistics (e.g.transformational grammar), whose theoretical underpinnings discouraged the sort ofcorpus linguisticsthat underlies the machine-learning approach to language processing.[8] In 2003,word n-gram model, at the time the best statistical algorithm, was outperformed by amulti-layer perceptron(with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster inlanguage modelling) byYoshua Bengiowith co-authors.[9] In 2010,TomÃ¡Å¡ Mikolov(then a PhD student atBrno University of Technology) with co-authors applied a simplerecurrent neural networkwith a single hidden layer to language modelling,[10]and in the following years he went on to developWord2vec. In the 2010s,representation learninganddeep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12]can achieve state-of-the-art results in many natural language tasks, e.g., inlanguage modeling[13]and parsing.[14][15]This is increasingly importantin medicine and healthcare, where NLP helps analyze notes and text inelectronic health recordsthat would otherwise be inaccessible for study when seeking to improve care[16]or protect patient privacy.[17] Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19]such as by writing grammars or devising heuristic rules forstemming. Machine learningapproaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance ofLLMsin 2023. Before that they were commonly used: In the late 1980s and mid-1990s, the statistical approach ended a period ofAI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21] The earliestdecision trees, producing systems of hardifâ€“then rules, were still very similar to the old rule-based approaches.
Only the introduction of hiddenMarkov models, applied to part-of-speech tagging, announced the end of the old rule-based approach. A major drawback of statistical methods is that they require elaboratefeature engineering. Since 2015,[22]the statistical approach has been replaced by theneural networksapproach, usingsemantic networks[23]andword embeddingsto capture semantic properties of words. Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. Neural machine translation, based on then-newly inventedsequence-to-sequencetransformations, made obsolete the intermediate steps, such as word alignment, previously necessary forstatistical machine translation. The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[46] Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognitionrefers to ""the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.""[47]Cognitive scienceis the interdisciplinary, scientific study of the mind and its processes.[48]Cognitive linguisticsis an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[49]Especially during the age ofsymbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. As an example,George Lakoffoffers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50]with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53]functional grammar,[54]construction grammar,[55]computational psycholinguistics and cognitive neuroscience (e.g.,ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56]of theACL). More recently, ideas of cognitive NLP have been revived as an approach to achieveexplainability, e.g., under the notion of ""cognitive AI"".[57]Likewise, ideas of cognitive NLP are inherent to neural modelsmultimodalNLP (although rarely made explicit)[58]and developments inartificial intelligence, specifically tools and technologies usinglarge language modelapproaches[59]and new directions inartificial general intelligencebased on thefree energy principle[60]by British neuroscientist and theoretician at University College LondonKarl J. Friston."
https://aws.amazon.com/what-is/nlp/,"Natural language processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language. Organizations today have large volumes of voice and text data from various communication channels like emails, text messages, social media newsfeeds, video, audio, and more. They use NLP software to automatically process this data, analyze the intent or sentiment in the message, and respond in real time to human communication. Natural language processing (NLP) is critical to fully and efficiently analyze text and speech data. It can work through the differences in dialects, slang, and grammatical irregularities typical in day-to-day conversations. Companies use it for several automated tasks, such as to:â€¢Â Â  Â Process, analyze, and archive large documentsâ€¢Â Â  Â Analyze customer feedback or call center recordingsâ€¢Â Â  Â Runchatbotsfor automated customer serviceâ€¢Â Â  Â Answer who-what-when-where questionsâ€¢Â Â  Â Classify and extract text You can also integrate NLP in customer-facing applications to communicate more effectively with customers. For example, a chatbot analyzes and sorts customer queries, responding automatically to common questions and redirecting complex queries to customer support. This automation helps reduce costs, saves agents from spending time on redundant queries, and improves customer satisfaction. Businesses use natural language processing (NLP) software and tools to simplify, automate, and streamline operations efficiently and accurately. We give some example use cases below. Sensitive data redaction Businesses in the insurance, legal, and healthcare sectors process, sort, and retrieve large volumes of sensitive documents like medical records, financial data, and private data. Instead of reviewing manually, companies use NLP technology to redact personally identifiable information and protect sensitive data. For example,Chisel AIhelps insurance carriers extract policy numbers, expiration dates, and other personal customer attributes from unstructured documents withAmazon Comprehend. Customer engagement NLP technologies allow chat and voice bots to be more human-like when conversing with customers. Businesses use chatbots to scale customer service capability and quality while keeping operational costs to a minimum.PubNub,which builds chatbot software, uses Amazon Comprehend to introduce localized chat functionality for its global customers.T-Mobile uses NLPto identify specific keywords in customers' text messages and offer personalized recommendations.Â Oklahoma State University deploysÂ aQ&A chatbot solutionto address student questions using machine learning technology. Business analytics Marketers use NLP tools like Amazon Comprehend andAmazon Lexto gain an educated perception of what customers feel toward a company's product or services. By scanning for specific phrases, they can gauge the customers' moods and emotions in written feedback. For example,Success KPIprovides natural language processing solutions that help businesses focus on targeted areas in sentiment analysis and help contact centers derive actionable insights from call analytics. Natural language processing (NLP) combines computational linguistics, machine learning, anddeep learningmodels to process human language. Computational linguistics Computational linguistics is the science of understanding and constructing human language models with computers and software tools. Researchers use computational linguistics methods, such as syntactic and semantic analysis, to create frameworks that help machines understand conversational human language. Tools like language translators,text-to-speechsynthesizers, and speech recognition software are based on computational linguistics. Machine learning Machine learningis a technology that trains a computer with sample data to improve its efficiency. Human language has several features like sarcasm, metaphors,Â variations in sentence structure, plus grammar and usage exceptions that take humans years to learn. Programmers use machine learning methods to teach NLP applications to recognize and accurately understand these features from the start. Deep learning Deep learning is a specific field of machine learning which teaches computers to learn and think like humans. It involves aneural networkthat consists of data processing nodes structured to resemble the human brain. With deep learning, computers recognize, classify, and co-relate complex patterns in the input data. NLP implementation steps Typically, NLP implementation begins by gathering and preparing unstructured text or speech data from sources like cloud data warehouses, surveys, emails, or internal business process applications. Pre-processing The NLP software uses pre-processing techniques such as tokenization, stemming, lemmatization, and stop word removal to prepare the data for various applications. Here's a description of these techniques: Training Researchers use the pre-processed data and machine learning to train NLP models to perform specific applications based on the provided textual information. Training NLP algorithms requires feeding the software with large data samples to increase the algorithms' accuracy. Deployment and inference Machine learning experts then deploy the model or integrate it into an existing production environment. The NLP model receives input and predicts an output for the specific use case the model's designed for. You can run the NLP application on live data and obtain the required output. Natural language processing (NLP) techniques, or NLP tasks, break down human text or speech into smaller parts that computer programs can easily understand. Common text processing and analyzing capabilities in NLP are given below. Part-f-speech tagging This is a process where NLP software tags individual words in a sentence according to contextual usages, such as nouns, verbs, adjectives, or adverbs. It helps the computer understand how words form meaningful relationships with each other. Word-sense disambiguation Some words may hold different meanings when used in different scenarios. For example, the wordÂ ""bat""means different things in these sentences: With word sense disambiguation, NLP software identifies a word's intended meaning, either by training its language model or referring to dictionary definitions. Speech recognition Speech recognition turns voice data into text. The process involves breaking words into smaller parts and understandingaccents, slurs, intonation, and nonstandard grammar usage in everyday conversation.Â A key application of speech recognition is transcription, which can be done using speech-to-text services likeAmazon Transcribe. Machine translation Machine translation software uses natural language processing to convert text or speech from one language to another while retaining contextual accuracy.Â The AWS service that supports machine translation isAmazon Translate. Named-entity recognition This process identifies unique names for people, places, events, companies, and more. NLP software uses named-entity recognition to determine the relationship between different entities in a sentence. Consider the following example: ""Jane went on a vacation to France, and she indulged herself in the local cuisines."" The NLP software will pickÂ ""Jane""andÂ ""France""as the special entities in the sentence. This can be further expanded by co-reference resolution, determining if different words are used to describe the same entity. In the above example, bothÂ ""Jane""andÂ ""she""pointed to the same person. Sentiment analysis Sentiment analysis is an artificial intelligence-based approach to interpreting the emotion conveyed by textual data. NLP software analyzes the text for words or phrases that show dissatisfaction, happiness, doubt, regret, and other hidden emotions. We give some common approaches to natural language processing (NLP) below. Supervised NLP Supervised NLP methods train the software with a set of labeled or known input and output. The program first processes large volumes of known data and learns how to produce the correct output from any unknown input. For example, companies train NLP tools to categorize documents according to specific labels. Unsupervised NLP Unsupervised NLP uses a statistical language model to predict the pattern that occurs when it is fed a non-labeled input. For example, the autocomplete feature in text messaging suggests relevant words that make sense for the sentence by monitoring the user's response. Natural language understanding Natural language understanding (NLU) is a subset of NLP that focuses on analyzing the meaning behind sentences. NLU allows the software to find similar meanings in different sentences or to process words that have different meanings. Natural language generation Natural language generation (NLG) focuses on producing conversational text like humans do based on specific keywords or topics. For example, an intelligent chatbot with NLG capabilities can converse with customers in similar ways tocustomer support personnel. AWS provides the broadest and most complete set ofartificial intelligenceandmachine learning(AI/ML) services for customers of all levels of expertise. These services are connected to a comprehensive set of data sources. For customers that lack ML skills, need faster time to market, or want to add intelligence to an existing process or an application,Â AWS offers a range ofML-based language services. These allow companies to easily add intelligence to their AI applications through pre-trained APIs for speech, transcription, translation, text analysis, and chatbot functionality. Here's a list of AWS ML-based language services: For customers who want to create a standard natural language processing (NLP) solution across their business,Â considerAmazon SageMaker.SageMakerÂ makes it easy to prepare data and build, train, and deploy ML models for any use case with fully managed infrastructure, tools, and workflows, including no-code offerings for business analysts. WithHugging Face on Amazon SageMaker, you can deploy and fine-tune pre-trained models from Hugging Face, an open-source provider ofNLP models known as Transformers. This reduces the time it takes to set up and use these NLP models from weeks to minutes. Get started with NLP by creating anAWS accounttoday. Instantly get access to the AWS free tier. Get started building in the AWS Management Console."
https://www.ibm.com/think/topics/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks,"6Â min read 6Â min read These computer science terms are often used interchangeably, but what differences make each a unique technology? Technology is becoming more embedded in our daily lives by the minute. To keep up with the pace of consumer expectations, companies are relying more heavily on machine learning algorithms to make things easier. You can see its application in social media (through object recognition in photos) or in talking directly toÂ devices (such as Alexa or Siri). Whileartificial intelligence(AI),machine learning(ML),deep learningandneural networksare related technologies, the terms are often used interchangeably, which frequently leads to confusion about their differences. This blog post clarifies some of the ambiguity. The easiest way to think about AI, machine learning, deep learning and neural networks is to think of them as a series of AI systems from largest to smallest, each encompassing the next. AI is the overarching system. Machine learning is a subset of AI. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. Itâ€™s the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three. Artificial intelligenceor AI, the broadest term of the three, is used to classify machines that mimic human intelligence and human cognitive functions like problem-solving and learning. AI uses predictions and automation to optimize and solve complex tasks that humans have historically done, such as facial and speech recognition, decision-making and translation.  The three main categories of AI are: ANI is considered â€œweakâ€ AI, whereas the other two types are classified as â€œstrongâ€ AI. We define weak AI by its ability to complete a specific task, like winning a chess game or identifying a particular individual in a series of photos. Natural language processing and computer vision, which let companies automate tasks and underpinchatbotsand virtual assistants such as Siri and Alexa, are examples of ANI. Computer vision is a factor in the development of self-driving cars. Stronger forms of AI, like AGI and ASI, incorporate human behaviors more prominently, such as the ability to interpret tone and emotion. Strong AI is defined by its ability compared to humans. AGI would perform on par with another human, while ASIâ€”also known as superintelligenceâ€”would surpass a humanâ€™s intelligence and ability. Neither form of Strong AI exists yet, but research in this field is ongoing.  An increasing number of businesses, about35%globally, are using AI, and another 42% are exploring the technology. The development ofgenerative AI, which uses powerful foundation models that train on large amounts of unlabeled data, can be adapted to new use cases and bring flexibility and scalability that is likely to accelerate the adoption of AI significantly. In early tests, IBM has seen generative AI bring time to value up to 70% faster than traditional AI. Whether you use AI applications based on ML or foundation models, AI can give your business a competitive advantage. Integrating customized AI models into your workflows and systems, and automating functions such as customer service, supply chain management and cybersecurity, can help a business meet customersâ€™ expectations, both today and as they increase in the future. The key is identifying the right data sets from the start to help ensure that you use quality data to achieve the most substantial competitive advantage. Youâ€™ll also need to create a hybrid, AI-ready architecture that can successfully use data wherever it livesâ€”on mainframes, data centers, in private and public clouds and at the edge. Your AI must be trustworthy because anything less means risking damage to a companyâ€™s reputation and bringing regulatory fines. Misleading models and those containing bias or thathallucinate(link resides outside ibm.com)Â can come at a high cost to customersâ€™ privacy, data rights and trust. Your AI must be explainable, fair and transparent. Machine learning is a subset of AI that allows for optimization. When set up correctly, it helps you make predictions that minimize the errors that arise from merely guessing. For example, companies like Amazon use machine learning to recommend products to a specific customer based on what theyâ€™ve looked at and bought before. Classic or â€œnondeepâ€ machine learning depends on human intervention to allow a computer system to identify patterns, learn, perform specific tasks and provide accurate results. Human experts determine the hierarchy of features to understand the differences between data inputs, usually requiring more structured data to learn. For example, letâ€™s say I showed you a series of images of different types of fast food: â€œpizza,â€ â€œburgerâ€ and â€œtaco.â€ A human expert working on those images would determine the characteristics distinguishing each picture as a specific fast food type. The bread in each food type might be a distinguishing feature. Alternatively, they might use labels, such as â€œpizza,â€ â€œburgerâ€ or â€œtacoâ€ to streamline the learning process through supervised learning. While the subset of AI called deep machine learning can leverage labeled data sets to inform its algorithm in supervised learning, it doesnâ€™t necessarily require a labeled data set. It can ingest unstructured data in its raw form (for example, text, images), and it can automatically determine the set of features that distinguish â€œpizza,â€ â€œburgerâ€ and â€œtacoâ€ from one another. As we generate more big data, data scientists use more machine learning. For a deeper dive into the differences between these approaches, check outSupervised versus unsupervised learning: Whatâ€™s the difference? A third category of machine learning is reinforcement learning, where a computer learns by interacting with its surroundings and getting feedback (rewards or penalties) for its actions. And online learning is a type of ML where a data scientist updates the ML model as new data becomes available. To learn more about machine learning, check out the following video: As our article ondeep learningexplains, deep learning is a subset of machine learning. The primary difference between machine learning and deep learning is how each algorithm learns and how much data each type of algorithm uses. Deep learning automates much of the feature extraction piece of the process, eliminating some of the manual human intervention required. It also enables the use of large data sets, earning the title ofscalable machine learning. That capability is exciting as we explore the use of unstructured data further, particularly sinceover 80% of an organizationâ€™s data is estimated to be unstructured(link resides outside ibm.com). Observing patterns in the data allows a deep-learning model to cluster inputs appropriately. Taking the same example from earlier, we might group pictures of pizzas, burgers and tacos into their respective categories based on the similarities or differences identified in the images. A deep-learning model requires more data points to improve accuracy, whereas a machine-learning model relies on less data given its underlying data structure. Enterprises generally use deep learning for more complex tasks, like virtual assistants or fraud detection. Neural networks, also called artificial neural networks or simulated neural networks, are a subset of machine learning and are the backbone of deep learning algorithms. They are called â€œneuralâ€ because they mimic how neurons in the brain signal one another. Neural networks are made up of node layersâ€”an input layer, one or more hidden layers and an output layer. Each node is an artificial neuron that connects to the next, and each has a weight and threshold value. When one nodeâ€™s output is above the threshold value, that node is activated and sends its data to the networkâ€™s next layer. If itâ€™s below the threshold, no data passes along. Training data teach neural networks and help improve their accuracy over time. Once the learning algorithms are fined-tuned, they become powerful computer science and AI tools because they allow us to quickly classify and cluster data. Using neural networks, speech and image recognition tasks can happen in minutes instead of the hours they take when done manually. Googleâ€™s search algorithm is a well-known example of a neural network. As mentioned in the explanation of neural networks above, but worth noting more explicitly, the â€œdeepâ€ in deep learning refers to the depth of layers in a neural network. A neural network of more than three layers, including the inputs and the output, can be considered a deep-learning algorithm. That can be represented by the following diagram: Most deep neural networks are feed-forward, meaning they only flow in one direction from input to output. However, you can also train your model through backpropagation, meaning moving in the opposite direction, from output to input. Backpropagation allows us to calculate and attribute the error that is associated with each neuron, allowing us to adjust and fit the algorithm appropriately. While all these areas of AI can help streamline areas of your business and improve your customer experience, achieving AI goals can be challenging because youâ€™ll first need to ensure that you have the right systems to construct learning algorithms to manage your data. Data management is more than merely building the models that you use for your business. You need a place to store your data and mechanisms for cleaning it and controlling for bias before you can start building anything. At IBM we are combining the power of machine learning and artificial intelligence in our new studio for foundation models, generative AI and machine learning, IBMÂ® watsonx.aiâ„¢.Subscribe to the Think Newsletter  Learn how to choose the right approach in preparing datasets and employing foundation models. We surveyed 2,000 organizations about their AI initiatives to discover what's working, what's not and how you can get ahead. IBMÂ® Graniteâ„¢ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options. Access our full catalog of over 100 online courses by purchasing an individual or multi-user subscription today, enabling you to expand your skills across a range of our products at one low price. Led by top IBM thought leaders, the curriculum is designed to help business leaders gain the knowledge needed to prioritize the AI investments that can drive growth. Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions. Learn how to confidently incorporate generative AI and machine learning into your business. Dive into the 3 critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI. Get one-stop access to capabilities that span the AI development lifecycle. Produce powerful AI solutions with user-friendly interfaces, workflows and access to industry-standard APIs and SDKs."
https://www.oracle.com/artificial-intelligence/what-is-natural-language-processing/,"Caroline Eppright | Content Strategist | March 25, 2021 In This Article Natural language processing (NLP) is a branch ofartificial intelligence (AI)that enables computers to comprehend, generate, and manipulate human language. Natural language processing has the ability to interrogate the data with natural language text or voice. This is also called â€œlanguage in.â€ Most consumers have probably interacted with NLP without realizing it. For instance, NLP is the core technology behind virtual assistants, such as the Oracle Digital Assistant (ODA), Siri, Cortana, or Alexa. When we ask questions of these virtual assistants, NLP is what enables them to not only understand the userâ€™s request, but to also respond in natural language. NLP applies both to written text and speech, and can be applied to all human languages. Other examples of tools powered by NLP include web search, email spam filtering, automatic translation of text or speech, document summarization, sentiment analysis, and grammar/spell checking. For example, some email programs can automatically suggest an appropriate reply to a message based on its contentâ€”these programs use NLP to read, analyze, and respond to your message. There are several other terms that are roughly synonymous with NLP. Natural language understanding (NLU) and natural language generation (NLG) refer to using computers to understand and produce human language, respectively. NLG has the ability to provide a verbal description of what has happened. This is also called â€œlanguage outâ€ by summarizing by meaningful information into text using a concept known as ""grammar of graphics."" In practice, NLU is used to mean NLP. The understanding by computers of the structure and meaning of all human languages, allowing developers and users to interact with computers using natural sentences and communication. Computational linguistics (CL) is the scientific field that studies computational aspects of human language, while NLP is the engineering discipline concerned with building computational artifacts that understand, generate, or manipulate human language. Research on NLP began shortly after the invention of digital computers in the 1950s, and NLP draws on both linguistics and AI. However, the major breakthroughs of the past few years have been powered by machine learning, which is a branch of AI that develops systems that learn and generalize from data.Deep learningis a kind ofmachine learningthat can learn very complex patterns from large datasets, which means that it is ideally suited to learning the complexities of natural language from datasets sourced from the web. Automate routine tasks:Chatbotspowered by NLP can process a large number of routine tasks that are handled by human agents today, freeing up employees to work on more challenging and interesting tasks. For example, chatbots andDigital Assistantscan recognize a wide variety of user requests, match them to the appropriate entry in a corporate database, and formulate an appropriate response to the user. Improve search:NLP can improve on keyword matching search for document and FAQ retrieval by disambiguating word senses based on context (for example, â€œcarrierâ€ means something different in biomedical and industrial contexts), matching synonyms (for example, retrieving documents mentioning â€œcarâ€ given a search for â€œautomobileâ€), and taking morphological variation into account (which is important for non-English queries). Effective NLP-powered academic search systems can dramatically improve access to relevant cutting-edge research for doctors, lawyers, and other specialists. Search engine optimization:NLP is a great tool for getting your business ranked higher in online search by analyzing searches to optimize your content. Search engines use NLP to rank their resultsâ€”and knowing how to effectively use these techniques makes it easier to be ranked above your competitors. This will lead to greater visibility for your business. Analyzing and organizing large document collections:NLP techniques such as document clustering and topic modeling simplify the task of understanding the diversity of content in large document collections, such as corporate reports, news articles, or scientific documents. These techniques are often used in legal discovery purposes. Social media analytics:NLP can analyze customer reviews and social media comments to make better sense of huge volumes of information. Sentiment analysis identifies positive and negative comments in a stream of social-media comments, providing a direct measure of customer sentiment in real time. This can lead to huge payoffs down the line, such as increased customer satisfaction and revenue. Market insights:With NLP working to analyze the language of your businessâ€™ customers, youâ€™ll have a better handle on what they want, and also a better idea of how to communicate with them. Aspect-oriented sentiment analysis detects the sentiment associated with specific aspects or products in social media (for example, â€œthe keyboard is great, but the screen is too dimâ€), providing directly actionable information for product design and marketing. Moderating content:If your business attracts large amounts of user or customer comments, NLP enables you to moderate whatâ€™s being said in order to maintain quality and civility by analyzing not only the words, but also the tone and intent of comments. NLP simplifies and automates a wide range of business processes, especially ones that involve large amounts of unstructured text like emails, surveys, social media conversations, and more. With NLP, businesses are better able to analyze their data to help make the right decisions. Here are just a few examples of practical applications of NLP: Machine learning models for NLP:We mentioned earlier that modern NLP relies heavily on an approach to AI called machine learning. Machine learning make predictions by generalizing over examples in a dataset. This dataset is called the training data, and machine learning algorithms train on this training data to produce a machine learning model that accomplishes a target task. For example, sentiment analysis training data consists of sentences together with their sentiment (for example, positive, negative, or neutral sentiment). A machine-learning algorithm reads this dataset and produces a model which takes sentences as input and returns their sentiments. This kind of model, which takes sentences or documents as inputs and returns a label for that input, is called a document classification model. Document classifiers can also be used to classify documents by the topics they mention (for example, as sports, finance, politics, etc.). Another kind of model is used to recognize and classify entities in documents. For each word in a document, the model predicts whether that word is part of an entity mention, and if so, what kind of entity is involved. For example, in â€œXYZ Corp shares traded for $28 yesterdayâ€, â€œXYZ Corpâ€ is a company entity, â€œ$28â€ is a currency amount, and â€œyesterdayâ€ is a date. The training data for entity recognition is a collection of texts, where each word is labeled with the kinds of entities the word refers to. This kind of model, which produces a label for each word in the input, is called a sequence labeling model. Sequence to sequence modelsare a very recent addition to the family of models used in NLP. A sequence to sequence (or seq2seq) model takes an entire sentence or document as input (as in a document classifier) but it produces a sentence or some other sequence (for example, a computer program) as output. (A document classifier only produces a single symbol as output). Example applications of seq2seq models include machine translation, which for example, takes an English sentence as input and returns its French sentence as output; document summarization (where the output is a summary of the input); and semantic parsing (where the input is a query or request in English, and the output is a computer program implementing that request). Deep learning, pretrained models, and transfer learning:Deep learningis the most widely-used kind of machine learning in NLP. In the 1980s, researchers developed neural networks, in which a large number of primitive machine learning models are combined into a single network: by analogy with brains, the simple machine learning models are sometimes called â€œneurons.â€ These neurons are arranged in layers, and a deep neural network is one with many layers. Deep learning is machine learning using deep neural network models. Because of their complexity, generally it takes a lot of data to train a deep neural network, and processing it takes a lot of compute power and time. Modern deep neural network NLP models are trained from a diverse array of sources, such as all of Wikipedia and data scraped from the web. The training data might be on the order of 10 GB or more in size, and it might take a week or more on a high-performance cluster to train the deep neural network. (Researchers find that training even deeper models from even larger datasets have even higher performance, so currently there is a race to train bigger and bigger models from larger and larger datasets). The voracious data and compute requirements of Deep Neural Networks would seem to severely limit their usefulness. However, transfer learning enables a trained deep neural network to be further trained to achieve a new task with much less training data and compute effort. The simplest kind of transfer learning is called fine tuning. It consists simply of first training the model on a large generic dataset (for example, Wikipedia) and then further training (â€œfine-tuningâ€) the model on a much smaller task-specific dataset that is labeled with the actual target task. Perhaps surprisingly, the fine-tuning datasets can be extremely small, maybe containing only hundreds or even tens of training examples, and fine-tuning training only requires minutes on a single CPU. Transfer learning makes it easy to deploy deep learning models throughout the enterprise. There is now an entire ecosystem of providers delivering pretrained deep learning models that are trained on different combinations of languages, datasets, and pretraining tasks. These pretrained models can be downloaded and fine-tuned for a wide variety of different target tasks. Learn how establishing an AI center of excellence (CoE) can boost your success with NLP technologies. Our ebook provides tips for building a CoE and effectively using advanced machine learning models. Tokenization:Tokenization splits raw text (for example., a sentence or a document) into a sequence of tokens, such as words or subword pieces. Tokenization is often the first step in an NLP processing pipeline. Tokens are commonly recurring sequences of text that are treated as atomic units in later processing. They may be words, subword units called morphemes (for example, prefixes such as â€œun-â€œ or suffixes such as â€œ-ingâ€ in English), or even individual characters. Bag-of-words models:Bag-of-words models treat documents as unordered collections of tokens or words (a bag is like a set, except that it tracks the number of times each element appears). Because they completely ignore word order, bag-of-words models will confuse a sentence such as â€œdog bites manâ€ with â€œman bites dog.â€ However, bag-of-words models are often used for efficiency reasons on large information retrieval tasks such as search engines. They can produce close to state-of-the-art results with longer documents. Stop word removal:A â€œstop wordâ€ is a token that is ignored in later processing. They are typically short, frequent words such as â€œa,â€ â€œthe,â€ or â€œan.â€ Bag-of-words models and search engines often ignore stop words in order to reduce processing time and storage within the database. Deep neural networks typically do take word-order into account (that is, they are not bag-of-words models) and do not do stop word removal because stop words can convey subtle distinctions in meaning (for example, â€œthe package was lostâ€ and â€œa package is lostâ€ donâ€™t mean the same thing, even though they are the same after stop word removal). Stemming and lemmatization:Morphemes are the smallest meaning-bearing elements of language. Typically morphemes are smaller than words. For example, â€œrevisitedâ€ consists of the prefix â€œre-â€œ, the stem â€œvisit,â€ and the past-tense suffix â€œ-ed.â€ Stemming and lemmatization map words to their stem forms (for example, â€œrevisitâ€ + PAST). Stemming and lemmatization are crucial steps in pre-deep learning models, but deep learning models generally learn these regularities from their training data, and so do not require explicit stemming or lemmatization steps. Part-of-speech tagging and syntactic parsing:Part-of-speech (PoS) tagging is the process of labeling each word with its part of speech (for example, noun, verb, adjective, etc.). A Syntactic parser identifies how words combine to form phrases, clauses, and entire sentences. PoS tagging is a sequence labeling task, syntactic parsing is an extended kind of sequence labeling task, and deep neural Nntworks are the state-of-the-art technology for both PoS tagging and syntactic parsing. Before deep learning, PoS tagging and syntactic parsing were essential steps in sentence understanding. However, modern deep learning NLP models generally only benefit marginally (if at all) from PoS or syntax information, so neither PoS tagging nor syntactic parsing are widely used in deep learning NLP. The NLP Libraries and toolkits are generally available in Python, and for this reason by far the majority of NLP projects are developed in Python. Pythonâ€™s interactive development environment makes it easy to develop and test new code. For processing large amounts of data, C++ and Java are often preferred because they can support more efficient code. Here are examples of some popular NLP libraries. TensorFlow and PyTorch:These are the two most popular deep learning toolkits. They are freely available for research and commercial purposes. While they support multiple languages, their primary language is Python. They come with large libraries of prebuilt components, so even very sophisticated deep learning NLP models often only require plugging these components together. They also support high-performance computing infrastructure, such as clusters of machines with graphical processor unit (GPU) accelerators. They have excellent documentation and tutorials. AllenNLP:This is a library of high-level NLP components (for example, simple chatbots) implemented in PyTorch and Python. The documentation is excellent. HuggingFace:This company distributes hundreds of different pretrained Deep Learning NLP models, as well as a plug-and-play software toolkit in TensorFlow and PyTorch that enables developers to rapidly evaluate how well different pretrained models perform on their specific tasks. Spark NLP:Spark NLP is an open source text processing library for advanced NLP for the Python, Java, and Scala programming languages. Its goal is to provide an application programming interface (API) for natural language processing pipelines. It offers pretrained neural network models, pipelines, and embeddings, as well as support for training custom models. SpaCy NLP:SpaCy is a free, open source library for advanced NLP in Python, and it is specifically designed to help build applications that can process and understand large volumes of text. SpaCy is known to be highly intuitive and can handle many of the tasks needed in common NLP projects. In summary, Natural language processing is an exciting area of artificial intelligence development that fuels a wide range of new products such as search engines, chatbots, recommendation systems, and speech-to-text systems. As human interfaces with computers continue to move away from buttons, forms, and domain-specific languages, the demand for growth in natural language processing will continue to increase. For this reason, Oracle Cloud Infrastructure is committed to providing on-premises performance with our performance-optimized compute shapes and tools for NLP. Oracle Cloud Infrastructure offersan array of GPU shapesthat you can deploy in minutes to begin experimenting with NLP."
https://www.sciencedirect.com/journal/natural-language-processing-journal,Error: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/journal/natural-language-processing-journal
https://www.ibm.com/think/topics/nlp-vs-nlu-vs-nlg,"4min read While natural language processing (NLP), natural language understanding (NLU), and natural language generation (NLG) are all related topics, they are distinct ones. At a high level, NLU and NLG are just components of NLP. Given how they intersect, they are commonly confused within conversation, but in this post, weâ€™ll define each term individually and summarize their differences to clarify any ambiguities. Natural language processing, which evolved from computational linguistics, uses methods from various disciplines, such as computer science, artificial intelligence, linguistics, and data science, to enable computers to understand human language in both written and verbal forms. While computational linguistics has more of a focus on aspects of language, natural language processing emphasizes its use of machine learning and deep learning techniques to complete tasks, like language translation or question answering. Natural language processing works by taking unstructured data and converting it into a structured data format. It does this through the identification of named entities (a process called named entity recognition) and identification of word patterns, using methods like tokenization, stemming, and lemmatization, which examine the root forms of words. For example, the suffix -ed on a word, like called, indicates past tense, but it has the same base infinitive (to call) as the present tense verb calling. While a number of NLP algorithms exist, different approaches tend to be used for different types of language tasks. For example, hidden Markov chains tend to be used for part-of-speech tagging. Recurrent neural networks help to generate the appropriate sequence of text. N-grams, a simple language model (LM), assign probabilities to sentences or phrases to predict the accuracy of a response. These techniques work together to support popular technology such as chatbots, or speech recognition products like Amazonâ€™s Alexa or Appleâ€™s Siri. However, its application has been broader than that, affecting other industries such as education and healthcare. Natural language understandingis a subset of natural language processing, which uses syntactic and semantic analysis of text and speech to determine the meaning of a sentence. Syntax refers to the grammatical structure of a sentence, while semantics alludes to its intended meaning. NLU also establishes a relevant ontology: a data structure which specifies the relationships between words and phrases. While humans naturally do this in conversation, the combination of these analyses is required for a machine to understand the intended meaning of different texts. Our ability to distinguish between homonyms and homophones illustrates the nuances of language well. For example, letâ€™s take the following two sentences: In the first sentence, the word, current is a noun. The verb that precedes it, swimming, provides additional context to the reader, allowing us to conclude that we are referring to the flow of water in the ocean. The second sentence uses the word current, but as an adjective. The noun it describes, version, denotes multiple iterations of a report, enabling us to determine that we are referring to the most up-to-date status of a file. These approaches are also commonly used in data mining to understand consumer attitudes. In particular, sentiment analysis enables brands to monitor their customer feedback more closely, allowing them to cluster positive and negative social media comments and track net promoter scores. By reviewing comments with negative sentiment, companies are able to identify and address potential problem areas within their products or services more quickly. Natural language generation is another subset of natural language processing. While natural language understanding focuses on computer reading comprehension, natural language generation enables computers to write. NLG is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services. NLG also encompasses text summarization capabilities that generate summaries from in-put documents while maintaining the integrity of the information. Extractive summarization is the AI innovation poweringKey Point Analysisused in Thatâ€™s Debatable. Initially, NLG systems used templates to generate text. Based on some data or query, an NLG system would fill in the blank, like a game of Mad Libs. But over time, natural language generation systems have evolved with the application of hidden Markov chains, recurrent neural networks, and transformers, enabling more dynamic text generation in real time. As with NLU, NLG applications need to consider language rules based on morphology, lexicons, syntax and semantics to make choices on how to phrase responses appropriately. They tackle this in three stages: Natural language processingand its subsets have numerous practical applications within todayâ€™s world, like healthcare diagnoses or online customer service. Explore some of the latestNLP researchat IBM or take a look at some of IBMâ€™s product offerings, likeWatson Natural Language Understanding. Its text analytics service offers insight into categories, concepts, entities, keywords, relationships, sentiment, and syntax from your textual data to help you respond to user needs quickly and efficiently. Help your business get on the right track to analyze and infuse your data at scale for AI. Program Manager VP and Head,  IBM Software,  Cloud and AI Acceleration Learn about the five key orchestration capabilities that can help organizations address the challenges of implementing generative AI effectively. IBMÂ® Graniteâ„¢ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options. Discover how natural language processing can help you to converse more naturally with computers. We surveyed 2,000 organizations about their AI initiatives to discover what's working, what's not and how you can get ahead. Explore IBM Developer's website to access blogs, articles, newsletters and learn more about IBM embeddable AI. Learn fundamental concepts and build your skills with hands-on labs, courses, guided projects, trials and more. Easily design scalable AI assistants and agents, automate repetitive tasks and simplify complex processes withÂ IBMÂ® watsonxâ„¢ OrchestrateÂ®. Accelerate the business value of artificial intelligence with a powerful and flexible portfolio of libraries, services and applications. Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value. Easily design scalable AI assistants and agents, automate repetitive tasks and simplify complex processes withÂ IBMÂ® watsonxâ„¢ OrchestrateÂ®."
https://www.copy.ai/blog/natural-language-processing,"Nathan is a revenue-focused marketing leader. By day he manages Demand Gen right here at Copy.ai, by night he enjoys family time in the Rocky Mountains! Imagine a world where computers can understand, interpret, and generate human language. This is the reality that Natural Language Processing (NLP) is making possible. NLP, a branch of artificial intelligence, is radically changing the way businesses operate by enabling machines to comprehend and respond to text and speech in a manner that is natural for humans. This comprehensive guide explores the world of NLP and how it can transform your business processes. From understanding the fundamentals of NLP and its key components to exploring its benefits and implementation strategies, this post provides the knowledge needed to apply language AI effectively. This guide focuses on how NLP can enhance various aspects of your business, particularly in sales and marketing. NLP automates tasks, improves data analysis, and enables more natural interactions with customers, boosting efficiency, productivity, and customer satisfaction. This guide is for business owners who want to maintain a competitive edge, sales professionals who aim to optimize processes, and marketing enthusiasts curious about the latest trends in AI. Unlock the full potential of Natural Language Processing and discover how it can propel your business to new heights. Natural Language Processing, or NLP for short, is a fascinating branch of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. It combines the power of linguistics and computer science to bridge the gap between how humans communicate and how computers process information. At its core, NLP involves teaching machines to make sense of the unstructured data found in text and speech, which is no small feat considering the complexity and nuance of human language. NLP systems use machine learning algorithms and statistical models to break down language into its fundamental components, analyze the relationships between words, and extract meaning from even the most complex sentences. Why is NLP important for businesses? It can automate and optimize countless processes that rely on language. From analyzing customer feedback and social media sentiment to generating personalized content and improving chatbot interactions, NLP is transforming the way businesses operate. NLP algorithms can analyze vast amounts of customer data to help sales teams identify high-quality leads, predict customer behavior, and craft more persuasive pitches. Marketers can also use NLP to create targeted campaigns, optimize content for search engines, and gain deep insights into customer preferences and trends. The importance of NLP in today's business landscape cannot be overstated. As the volume of unstructured data continues to grow at an unprecedented rate, the ability to effectively process and analyze language has become a critical competitive advantage. In fact, the global NLP market is expected to experience significant growth in the coming years, with some estimates projecting amarket size of over $43 billion by 2025. As we continue to explore the vast potential of NLP, it's clear that this technology will play an increasingly vital role in shaping the future of business. Companies that embrace language AI unlock new levels of efficiency, insights, and customer engagement, ultimately driving growth and success in an ever-evolving digital landscape. The rise of Natural Language Processing has brought about a multitude of benefits for businesses across various industries. NLP allows companies to streamline their operations, gain valuable insights, and deliver exceptional customer experiences. One of the most significant advantages of NLP is its ability to automate tasks that were once time-consuming and labor-intensive. For example, NLP-powered chatbots can handle customer inquiries 24/7, freeing up human agents to focus on more complex issues. Similarly, NLP algorithms can automatically categorize and route customer support tickets, reducing response times and improving overall efficiency. NLP also enables businesses to extract valuable insights from vast amounts of unstructured data. Analyzing customer reviews, social media posts, and other text-based data sources, companies can gain a deeper understanding of customer sentiment, preferences, and pain points. This information can then inform product development, marketing strategies, and customer service initiatives. NLP is also changing the way businesses interact with their customers. With the help of NLP-powered virtual assistants and conversational AI, companies can provide personalized, human-like experiences at scale. These intelligent systems can understand natural language queries, provide relevant responses, and even anticipate customer needs based on previous interactions. The benefits of NLP are not just theoreticalâ€”they are already driving tangible results for businesses around the world. For instance, a case study by IBM found that their NLP-powered Watson Assistant helped reduce customer service costs by 30% while improving customer satisfaction scores. Similarly, a global e-commerce company reported a 25% increase in conversion rates after implementing an NLP-based product recommendation system. Businesses are recognizing the value of NLP, and the market is poised for significant growth in the coming years. According to recent projections, the NLP market is expected to grow at aCompound Annual Growth Rate (CAGR) of 18.4% from 2020 to 2027, reaching a staggering $28.6 billion by the end of the forecast period. Processing and analyzing language effectively has become a critical success factor. NLP technologies enable companies to unlock new levels of efficiency, gain deeper customer insights, and deliver truly personalized experiences. As the NLP market continues to expand and evolve, businesses that embrace this transformative technology will be well-positioned to thrive in the years ahead. Integrating Natural Language Processing into your business processes may seem daunting at first, but with the right approach and tools, it can be a seamless and rewarding experience. This section explores how to effectively implement NLP, particularly in the context of sales and marketing. By following these guidelines and best practices, you can successfully implement NLP in your sales and marketing efforts, unlocking new levels of efficiency, insights, and customer engagement. Stay agile, data-driven, and customer-centric to realize the full potential of this transformative technology. Implementing Natural Language Processing can be a complex undertaking, but fortunately, numerous tools and resources are available to streamline the process. This section explores some of the most valuable software, platforms, and resources that can aid in your NLP implementation journey. In addition to these tools and platforms, numerous online resources, tutorials, and communities are dedicated to NLP. Some notable examples include: These tools and resources can accelerate your NLP implementation and help you unlock the full potential of natural language processing for your business. Whether you're a seasoned developer or just getting started with NLP, these resources will provide the guidance and support you need to succeed. Throughout this post, we've explored the transformative potential of Natural Language Processing in the business world. From automating routine tasks to extracting valuable insights from unstructured data, NLP offers a wide range of benefits that can help organizations streamline their operations, improve customer experiences, and gain a competitive edge. Implementing NLP may seem daunting at first, but with the right tools, resources, and strategies, any business can harness the power of this technology. Follow the step-by-step guide outlined in this post and utilize the various software, platforms, and resources available to effectively integrate NLP into your sales and marketing processes and start reaping the benefits. At Copy.ai, we're committed to helping businesses unlock the full potential of AI and NLP. Our suite of tools is designed to streamline your content creation process, enhance your go-to-market strategies, and ultimately drive business growth. Byachieving AI content efficiencywith Copy.ai, you can create compelling, data-driven content at scale, freeing up your team to focus on higher-level strategic initiatives. As the business landscape continues to evolve, the importance of NLP will only continue to grow. Embracing this technology positions your organization for long-term success in an increasingly competitive and data-driven world. Why wait? Start exploring the possibilities of NLP today and discover how it can transform your business operations. With the right tools and mindset, you can unlock new levels of efficiency, insights, and growth, and stay ahead of the competition in the years to come. Write 10x faster, engage your audience, & never struggle with the blank page again."
https://aws.amazon.com/what-is/nlp/,"Natural language processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language. Organizations today have large volumes of voice and text data from various communication channels like emails, text messages, social media newsfeeds, video, audio, and more. They use NLP software to automatically process this data, analyze the intent or sentiment in the message, and respond in real time to human communication. Natural language processing (NLP) is critical to fully and efficiently analyze text and speech data. It can work through the differences in dialects, slang, and grammatical irregularities typical in day-to-day conversations. Companies use it for several automated tasks, such as to:â€¢Â Â  Â Process, analyze, and archive large documentsâ€¢Â Â  Â Analyze customer feedback or call center recordingsâ€¢Â Â  Â Runchatbotsfor automated customer serviceâ€¢Â Â  Â Answer who-what-when-where questionsâ€¢Â Â  Â Classify and extract text You can also integrate NLP in customer-facing applications to communicate more effectively with customers. For example, a chatbot analyzes and sorts customer queries, responding automatically to common questions and redirecting complex queries to customer support. This automation helps reduce costs, saves agents from spending time on redundant queries, and improves customer satisfaction. Businesses use natural language processing (NLP) software and tools to simplify, automate, and streamline operations efficiently and accurately. We give some example use cases below. Sensitive data redaction Businesses in the insurance, legal, and healthcare sectors process, sort, and retrieve large volumes of sensitive documents like medical records, financial data, and private data. Instead of reviewing manually, companies use NLP technology to redact personally identifiable information and protect sensitive data. For example,Chisel AIhelps insurance carriers extract policy numbers, expiration dates, and other personal customer attributes from unstructured documents withAmazon Comprehend. Customer engagement NLP technologies allow chat and voice bots to be more human-like when conversing with customers. Businesses use chatbots to scale customer service capability and quality while keeping operational costs to a minimum.PubNub,which builds chatbot software, uses Amazon Comprehend to introduce localized chat functionality for its global customers.T-Mobile uses NLPto identify specific keywords in customers' text messages and offer personalized recommendations.Â Oklahoma State University deploysÂ aQ&A chatbot solutionto address student questions using machine learning technology. Business analytics Marketers use NLP tools like Amazon Comprehend andAmazon Lexto gain an educated perception of what customers feel toward a company's product or services. By scanning for specific phrases, they can gauge the customers' moods and emotions in written feedback. For example,Success KPIprovides natural language processing solutions that help businesses focus on targeted areas in sentiment analysis and help contact centers derive actionable insights from call analytics. Natural language processing (NLP) combines computational linguistics, machine learning, anddeep learningmodels to process human language. Computational linguistics Computational linguistics is the science of understanding and constructing human language models with computers and software tools. Researchers use computational linguistics methods, such as syntactic and semantic analysis, to create frameworks that help machines understand conversational human language. Tools like language translators,text-to-speechsynthesizers, and speech recognition software are based on computational linguistics. Machine learning Machine learningis a technology that trains a computer with sample data to improve its efficiency. Human language has several features like sarcasm, metaphors,Â variations in sentence structure, plus grammar and usage exceptions that take humans years to learn. Programmers use machine learning methods to teach NLP applications to recognize and accurately understand these features from the start. Deep learning Deep learning is a specific field of machine learning which teaches computers to learn and think like humans. It involves aneural networkthat consists of data processing nodes structured to resemble the human brain. With deep learning, computers recognize, classify, and co-relate complex patterns in the input data. NLP implementation steps Typically, NLP implementation begins by gathering and preparing unstructured text or speech data from sources like cloud data warehouses, surveys, emails, or internal business process applications. Pre-processing The NLP software uses pre-processing techniques such as tokenization, stemming, lemmatization, and stop word removal to prepare the data for various applications. Here's a description of these techniques: Training Researchers use the pre-processed data and machine learning to train NLP models to perform specific applications based on the provided textual information. Training NLP algorithms requires feeding the software with large data samples to increase the algorithms' accuracy. Deployment and inference Machine learning experts then deploy the model or integrate it into an existing production environment. The NLP model receives input and predicts an output for the specific use case the model's designed for. You can run the NLP application on live data and obtain the required output. Natural language processing (NLP) techniques, or NLP tasks, break down human text or speech into smaller parts that computer programs can easily understand. Common text processing and analyzing capabilities in NLP are given below. Part-f-speech tagging This is a process where NLP software tags individual words in a sentence according to contextual usages, such as nouns, verbs, adjectives, or adverbs. It helps the computer understand how words form meaningful relationships with each other. Word-sense disambiguation Some words may hold different meanings when used in different scenarios. For example, the wordÂ ""bat""means different things in these sentences: With word sense disambiguation, NLP software identifies a word's intended meaning, either by training its language model or referring to dictionary definitions. Speech recognition Speech recognition turns voice data into text. The process involves breaking words into smaller parts and understandingaccents, slurs, intonation, and nonstandard grammar usage in everyday conversation.Â A key application of speech recognition is transcription, which can be done using speech-to-text services likeAmazon Transcribe. Machine translation Machine translation software uses natural language processing to convert text or speech from one language to another while retaining contextual accuracy.Â The AWS service that supports machine translation isAmazon Translate. Named-entity recognition This process identifies unique names for people, places, events, companies, and more. NLP software uses named-entity recognition to determine the relationship between different entities in a sentence. Consider the following example: ""Jane went on a vacation to France, and she indulged herself in the local cuisines."" The NLP software will pickÂ ""Jane""andÂ ""France""as the special entities in the sentence. This can be further expanded by co-reference resolution, determining if different words are used to describe the same entity. In the above example, bothÂ ""Jane""andÂ ""she""pointed to the same person. Sentiment analysis Sentiment analysis is an artificial intelligence-based approach to interpreting the emotion conveyed by textual data. NLP software analyzes the text for words or phrases that show dissatisfaction, happiness, doubt, regret, and other hidden emotions. We give some common approaches to natural language processing (NLP) below. Supervised NLP Supervised NLP methods train the software with a set of labeled or known input and output. The program first processes large volumes of known data and learns how to produce the correct output from any unknown input. For example, companies train NLP tools to categorize documents according to specific labels. Unsupervised NLP Unsupervised NLP uses a statistical language model to predict the pattern that occurs when it is fed a non-labeled input. For example, the autocomplete feature in text messaging suggests relevant words that make sense for the sentence by monitoring the user's response. Natural language understanding Natural language understanding (NLU) is a subset of NLP that focuses on analyzing the meaning behind sentences. NLU allows the software to find similar meanings in different sentences or to process words that have different meanings. Natural language generation Natural language generation (NLG) focuses on producing conversational text like humans do based on specific keywords or topics. For example, an intelligent chatbot with NLG capabilities can converse with customers in similar ways tocustomer support personnel. AWS provides the broadest and most complete set ofartificial intelligenceandmachine learning(AI/ML) services for customers of all levels of expertise. These services are connected to a comprehensive set of data sources. For customers that lack ML skills, need faster time to market, or want to add intelligence to an existing process or an application,Â AWS offers a range ofML-based language services. These allow companies to easily add intelligence to their AI applications through pre-trained APIs for speech, transcription, translation, text analysis, and chatbot functionality. Here's a list of AWS ML-based language services: For customers who want to create a standard natural language processing (NLP) solution across their business,Â considerAmazon SageMaker.SageMakerÂ makes it easy to prepare data and build, train, and deploy ML models for any use case with fully managed infrastructure, tools, and workflows, including no-code offerings for business analysts. WithHugging Face on Amazon SageMaker, you can deploy and fine-tune pre-trained models from Hugging Face, an open-source provider ofNLP models known as Transformers. This reduces the time it takes to set up and use these NLP models from weeks to minutes. Get started with NLP by creating anAWS accounttoday. Instantly get access to the AWS free tier. Get started building in the AWS Management Console."
https://www.oracle.com/artificial-intelligence/what-is-natural-language-processing/,"Caroline Eppright | Content Strategist | March 25, 2021 In This Article Natural language processing (NLP) is a branch ofartificial intelligence (AI)that enables computers to comprehend, generate, and manipulate human language. Natural language processing has the ability to interrogate the data with natural language text or voice. This is also called â€œlanguage in.â€ Most consumers have probably interacted with NLP without realizing it. For instance, NLP is the core technology behind virtual assistants, such as the Oracle Digital Assistant (ODA), Siri, Cortana, or Alexa. When we ask questions of these virtual assistants, NLP is what enables them to not only understand the userâ€™s request, but to also respond in natural language. NLP applies both to written text and speech, and can be applied to all human languages. Other examples of tools powered by NLP include web search, email spam filtering, automatic translation of text or speech, document summarization, sentiment analysis, and grammar/spell checking. For example, some email programs can automatically suggest an appropriate reply to a message based on its contentâ€”these programs use NLP to read, analyze, and respond to your message. There are several other terms that are roughly synonymous with NLP. Natural language understanding (NLU) and natural language generation (NLG) refer to using computers to understand and produce human language, respectively. NLG has the ability to provide a verbal description of what has happened. This is also called â€œlanguage outâ€ by summarizing by meaningful information into text using a concept known as ""grammar of graphics."" In practice, NLU is used to mean NLP. The understanding by computers of the structure and meaning of all human languages, allowing developers and users to interact with computers using natural sentences and communication. Computational linguistics (CL) is the scientific field that studies computational aspects of human language, while NLP is the engineering discipline concerned with building computational artifacts that understand, generate, or manipulate human language. Research on NLP began shortly after the invention of digital computers in the 1950s, and NLP draws on both linguistics and AI. However, the major breakthroughs of the past few years have been powered by machine learning, which is a branch of AI that develops systems that learn and generalize from data.Deep learningis a kind ofmachine learningthat can learn very complex patterns from large datasets, which means that it is ideally suited to learning the complexities of natural language from datasets sourced from the web. Automate routine tasks:Chatbotspowered by NLP can process a large number of routine tasks that are handled by human agents today, freeing up employees to work on more challenging and interesting tasks. For example, chatbots andDigital Assistantscan recognize a wide variety of user requests, match them to the appropriate entry in a corporate database, and formulate an appropriate response to the user. Improve search:NLP can improve on keyword matching search for document and FAQ retrieval by disambiguating word senses based on context (for example, â€œcarrierâ€ means something different in biomedical and industrial contexts), matching synonyms (for example, retrieving documents mentioning â€œcarâ€ given a search for â€œautomobileâ€), and taking morphological variation into account (which is important for non-English queries). Effective NLP-powered academic search systems can dramatically improve access to relevant cutting-edge research for doctors, lawyers, and other specialists. Search engine optimization:NLP is a great tool for getting your business ranked higher in online search by analyzing searches to optimize your content. Search engines use NLP to rank their resultsâ€”and knowing how to effectively use these techniques makes it easier to be ranked above your competitors. This will lead to greater visibility for your business. Analyzing and organizing large document collections:NLP techniques such as document clustering and topic modeling simplify the task of understanding the diversity of content in large document collections, such as corporate reports, news articles, or scientific documents. These techniques are often used in legal discovery purposes. Social media analytics:NLP can analyze customer reviews and social media comments to make better sense of huge volumes of information. Sentiment analysis identifies positive and negative comments in a stream of social-media comments, providing a direct measure of customer sentiment in real time. This can lead to huge payoffs down the line, such as increased customer satisfaction and revenue. Market insights:With NLP working to analyze the language of your businessâ€™ customers, youâ€™ll have a better handle on what they want, and also a better idea of how to communicate with them. Aspect-oriented sentiment analysis detects the sentiment associated with specific aspects or products in social media (for example, â€œthe keyboard is great, but the screen is too dimâ€), providing directly actionable information for product design and marketing. Moderating content:If your business attracts large amounts of user or customer comments, NLP enables you to moderate whatâ€™s being said in order to maintain quality and civility by analyzing not only the words, but also the tone and intent of comments. NLP simplifies and automates a wide range of business processes, especially ones that involve large amounts of unstructured text like emails, surveys, social media conversations, and more. With NLP, businesses are better able to analyze their data to help make the right decisions. Here are just a few examples of practical applications of NLP: Machine learning models for NLP:We mentioned earlier that modern NLP relies heavily on an approach to AI called machine learning. Machine learning make predictions by generalizing over examples in a dataset. This dataset is called the training data, and machine learning algorithms train on this training data to produce a machine learning model that accomplishes a target task. For example, sentiment analysis training data consists of sentences together with their sentiment (for example, positive, negative, or neutral sentiment). A machine-learning algorithm reads this dataset and produces a model which takes sentences as input and returns their sentiments. This kind of model, which takes sentences or documents as inputs and returns a label for that input, is called a document classification model. Document classifiers can also be used to classify documents by the topics they mention (for example, as sports, finance, politics, etc.). Another kind of model is used to recognize and classify entities in documents. For each word in a document, the model predicts whether that word is part of an entity mention, and if so, what kind of entity is involved. For example, in â€œXYZ Corp shares traded for $28 yesterdayâ€, â€œXYZ Corpâ€ is a company entity, â€œ$28â€ is a currency amount, and â€œyesterdayâ€ is a date. The training data for entity recognition is a collection of texts, where each word is labeled with the kinds of entities the word refers to. This kind of model, which produces a label for each word in the input, is called a sequence labeling model. Sequence to sequence modelsare a very recent addition to the family of models used in NLP. A sequence to sequence (or seq2seq) model takes an entire sentence or document as input (as in a document classifier) but it produces a sentence or some other sequence (for example, a computer program) as output. (A document classifier only produces a single symbol as output). Example applications of seq2seq models include machine translation, which for example, takes an English sentence as input and returns its French sentence as output; document summarization (where the output is a summary of the input); and semantic parsing (where the input is a query or request in English, and the output is a computer program implementing that request). Deep learning, pretrained models, and transfer learning:Deep learningis the most widely-used kind of machine learning in NLP. In the 1980s, researchers developed neural networks, in which a large number of primitive machine learning models are combined into a single network: by analogy with brains, the simple machine learning models are sometimes called â€œneurons.â€ These neurons are arranged in layers, and a deep neural network is one with many layers. Deep learning is machine learning using deep neural network models. Because of their complexity, generally it takes a lot of data to train a deep neural network, and processing it takes a lot of compute power and time. Modern deep neural network NLP models are trained from a diverse array of sources, such as all of Wikipedia and data scraped from the web. The training data might be on the order of 10 GB or more in size, and it might take a week or more on a high-performance cluster to train the deep neural network. (Researchers find that training even deeper models from even larger datasets have even higher performance, so currently there is a race to train bigger and bigger models from larger and larger datasets). The voracious data and compute requirements of Deep Neural Networks would seem to severely limit their usefulness. However, transfer learning enables a trained deep neural network to be further trained to achieve a new task with much less training data and compute effort. The simplest kind of transfer learning is called fine tuning. It consists simply of first training the model on a large generic dataset (for example, Wikipedia) and then further training (â€œfine-tuningâ€) the model on a much smaller task-specific dataset that is labeled with the actual target task. Perhaps surprisingly, the fine-tuning datasets can be extremely small, maybe containing only hundreds or even tens of training examples, and fine-tuning training only requires minutes on a single CPU. Transfer learning makes it easy to deploy deep learning models throughout the enterprise. There is now an entire ecosystem of providers delivering pretrained deep learning models that are trained on different combinations of languages, datasets, and pretraining tasks. These pretrained models can be downloaded and fine-tuned for a wide variety of different target tasks. Learn how establishing an AI center of excellence (CoE) can boost your success with NLP technologies. Our ebook provides tips for building a CoE and effectively using advanced machine learning models. Tokenization:Tokenization splits raw text (for example., a sentence or a document) into a sequence of tokens, such as words or subword pieces. Tokenization is often the first step in an NLP processing pipeline. Tokens are commonly recurring sequences of text that are treated as atomic units in later processing. They may be words, subword units called morphemes (for example, prefixes such as â€œun-â€œ or suffixes such as â€œ-ingâ€ in English), or even individual characters. Bag-of-words models:Bag-of-words models treat documents as unordered collections of tokens or words (a bag is like a set, except that it tracks the number of times each element appears). Because they completely ignore word order, bag-of-words models will confuse a sentence such as â€œdog bites manâ€ with â€œman bites dog.â€ However, bag-of-words models are often used for efficiency reasons on large information retrieval tasks such as search engines. They can produce close to state-of-the-art results with longer documents. Stop word removal:A â€œstop wordâ€ is a token that is ignored in later processing. They are typically short, frequent words such as â€œa,â€ â€œthe,â€ or â€œan.â€ Bag-of-words models and search engines often ignore stop words in order to reduce processing time and storage within the database. Deep neural networks typically do take word-order into account (that is, they are not bag-of-words models) and do not do stop word removal because stop words can convey subtle distinctions in meaning (for example, â€œthe package was lostâ€ and â€œa package is lostâ€ donâ€™t mean the same thing, even though they are the same after stop word removal). Stemming and lemmatization:Morphemes are the smallest meaning-bearing elements of language. Typically morphemes are smaller than words. For example, â€œrevisitedâ€ consists of the prefix â€œre-â€œ, the stem â€œvisit,â€ and the past-tense suffix â€œ-ed.â€ Stemming and lemmatization map words to their stem forms (for example, â€œrevisitâ€ + PAST). Stemming and lemmatization are crucial steps in pre-deep learning models, but deep learning models generally learn these regularities from their training data, and so do not require explicit stemming or lemmatization steps. Part-of-speech tagging and syntactic parsing:Part-of-speech (PoS) tagging is the process of labeling each word with its part of speech (for example, noun, verb, adjective, etc.). A Syntactic parser identifies how words combine to form phrases, clauses, and entire sentences. PoS tagging is a sequence labeling task, syntactic parsing is an extended kind of sequence labeling task, and deep neural Nntworks are the state-of-the-art technology for both PoS tagging and syntactic parsing. Before deep learning, PoS tagging and syntactic parsing were essential steps in sentence understanding. However, modern deep learning NLP models generally only benefit marginally (if at all) from PoS or syntax information, so neither PoS tagging nor syntactic parsing are widely used in deep learning NLP. The NLP Libraries and toolkits are generally available in Python, and for this reason by far the majority of NLP projects are developed in Python. Pythonâ€™s interactive development environment makes it easy to develop and test new code. For processing large amounts of data, C++ and Java are often preferred because they can support more efficient code. Here are examples of some popular NLP libraries. TensorFlow and PyTorch:These are the two most popular deep learning toolkits. They are freely available for research and commercial purposes. While they support multiple languages, their primary language is Python. They come with large libraries of prebuilt components, so even very sophisticated deep learning NLP models often only require plugging these components together. They also support high-performance computing infrastructure, such as clusters of machines with graphical processor unit (GPU) accelerators. They have excellent documentation and tutorials. AllenNLP:This is a library of high-level NLP components (for example, simple chatbots) implemented in PyTorch and Python. The documentation is excellent. HuggingFace:This company distributes hundreds of different pretrained Deep Learning NLP models, as well as a plug-and-play software toolkit in TensorFlow and PyTorch that enables developers to rapidly evaluate how well different pretrained models perform on their specific tasks. Spark NLP:Spark NLP is an open source text processing library for advanced NLP for the Python, Java, and Scala programming languages. Its goal is to provide an application programming interface (API) for natural language processing pipelines. It offers pretrained neural network models, pipelines, and embeddings, as well as support for training custom models. SpaCy NLP:SpaCy is a free, open source library for advanced NLP in Python, and it is specifically designed to help build applications that can process and understand large volumes of text. SpaCy is known to be highly intuitive and can handle many of the tasks needed in common NLP projects. In summary, Natural language processing is an exciting area of artificial intelligence development that fuels a wide range of new products such as search engines, chatbots, recommendation systems, and speech-to-text systems. As human interfaces with computers continue to move away from buttons, forms, and domain-specific languages, the demand for growth in natural language processing will continue to increase. For this reason, Oracle Cloud Infrastructure is committed to providing on-premises performance with our performance-optimized compute shapes and tools for NLP. Oracle Cloud Infrastructure offersan array of GPU shapesthat you can deploy in minutes to begin experimenting with NLP."
https://www.geeksforgeeks.org/major-challenges-of-natural-language-processing/,"In this evolving landscape of artificial intelligence(AI), Natural Language Processing(NLP) stands out as an advanced technology that fills the gap between humans and machines. In this article, we will discover theMajor Challenges of Natural language Processing(NLP)faced by organizations. Understanding these challenges helps you explore the advanced  NLP but also leverages its capabilities to revolutionize How we interact with machines and everything from customer service automation to complicated data analysis. Natural Languageis a powerful tool ofArtificial Intelligencethat enables computers to understand, interpret and generate human readable text that is meaningful. NLP is a method used for processing and analyzing the text data. In Natural Language Processing the text is tokenized means the text is break into tokens, it could be words, phrases or character. It is the first step in NLP task. The text is cleaned and preprocessed before applying Natural Language Processing technique. Natural Language Processing technique is used inmachine translation, healthcare, finance, customer service, sentiment analysis and extracting valuable information from the text data. NLP is also used in text generation and language modeling. Natural Processing technique can also be used in answering the questions. Many companies uses Natural Language Processing technique to solve their text related problems. Tools such asChatGPT,Google Bardthat trained on large corpus of test of data uses Natural Language Processing technique to solve the user queries. Natural Language Processing (NLP) faces various challenges due to the complexity and diversity of human language. Let's discuss10 major challenges in NLP: The human language and understanding is rich and intricated and there many languages spoken by humans. Human language is diverse and thousand of human languages spoken around the world with having its own grammar, vocabular and cultural nuances. Human cannot understand all the languages and the productivity of human language is high. There is ambiguity in natural language since same words and phrases can have different meanings and different context. This is the major challenges in understating of natural language. There is acomplex syntacticstructures and grammatical rules of natural languages. The rules are such as word order, verb, conjugation, tense, aspect and agreement. There is rich semantic content in human language that allows speaker to convey a wide range of meaning through words and sentences. Natural Language is pragmatics which means that how language can be used in context to approach communication goals. The human language evolves time to time with the processes such aslexical change. Training data is a curated collection of input-output pairs, where the input represents the features or attributes of the data, and the output is the corresponding label or target.Training data is composed of both the features (inputs) and their corresponding labels (outputs). For NLP, features might include text data, and labels could be categories, sentiments, or any other relevant annotations. It helps the model generalize patterns from the training set to make predictions or classifications on new, previously unseen data. Development Time and Resource Requirements forNatural Language Processing (NLP)projects depends on various factors consisting the task complexity, size and quality of the data, availability of existing tools and libraries, and the team of expert involved. Here are some key points: It is a crucial aspect to navigate phrasing ambiguities because of the inherent complexity of human languages. The cause of phrasing ambiguities is when a phrase can be evaluated in multiple ways that leads to uncertainty in understanding the meaning. Here are some key points for navigating phrasing ambiguities in NLP: Overcoming Misspelling and Grammatical Error are the basic challenges in NLP, as there are different forms of linguistics noise that can impact accuracy of understanding and analysis. Here are some key points for solving misspelling and grammatical error in NLP: It is a crucial step of mitigating innate biases in NLP algorithm for conforming fairness, equity, and inclusivity in natural language processing applications. Here are some key points for mitigating biases in NLP algorithms. Words with multiple meaning plays a lexical challenge inNature Language Processingbecause of the ambiguity of the word. These words with multiple meaning are known as polysemous or homonymous have different meaning based on the context in which they are used. Here are some key points for representing the lexical challenge plays by words with multiple meanings in NLP: It is very important to address language diversity and multilingualism in Natural Language Processing to confirm that NLP systems can handle the text data in multiple languages effectively. Here are some key points to address language diversity and multilingualism: It is very crucial task to reduce uncertainty and false positives in Natural Language Process (NLP) to improve the accuracy and reliability of the NLP models. Here are some key points to approach the solution: Facilitating continuous conversations with NLP includes the development of system that understands and responds to human language in real-time that enables seamless interaction between users and machines. Implementing real time natural language processing pipelines gives to capability to analyze and interpret user input as it is received involving algorithms are optimized and systems for low latency processing to confirm quick responses to user queries and inputs. Building an NLP models that can maintain the context throughout a conversation. The understanding of context enables systems to interpret user intent, conversation history tracking, and generating relevant responses based on the ongoing dialogue. Apply intent recognition algorithm to find the underlying goals and intentions expressed by users in their messages. It requires a combination of innovative technologies, experts of domain, and methodological approached to over the challenges in NLP. Here are some key points to overcome the challenge of NLP tasks: Natural Language Processing (NLP) is a transformative field within data science, offering applications in areas like conversational agents, sentiment analysis, machine translation, and information extraction. Understanding and overcoming theChallenges of Natural Language Processingis crucial for businesses looking to leverage its power to drive innovation and improve user interactions. A "
https://www.researchgate.net/publication/376506354_Advancements_in_NLP_The_Role_of_AI_in_Language_Understanding,Error: 403 Client Error: Forbidden for url: https://www.researchgate.net/publication/376506354_Advancements_in_NLP_The_Role_of_AI_in_Language_Understanding
https://www.geeksforgeeks.org/nlp-techniques/,"Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way. Here, we will delve deeper into the various techniques and methodologies used in NLP, along with their applications and significance, accompanied by practical examples. Table of Content Text preprocessing is the first and one of the most crucial steps in any NLP task. It involves cleaning and preparing the text for analysis. TokenizationTokenization is the process of breaking down a text into smaller units called tokens. Tokens can be words, sentences, or subwords. Example: The sentence ""NLP is fascinating"" can be tokenized into [""NLP"", ""is"", ""fascinating""]. Stop Words RemovalStop words are common words that usually do not add significant meaning to a sentence and are often removed to reduce the dimensionality of the data. Example: From the sentence ""This is an example of stop words removal,"" the stop words ""This,"" ""is,"" ""an,"" and ""of"" can be removed, leaving [""example"", ""stop"", ""words"", ""removal""]. StemmingStemming is the process of reducing words to their root form. Example: The words ""running,"" ""runner,"" and ""runs"" can all be reduced to the root word ""run."" LemmatizationLemmatization also reduces words to their base or dictionary form but is more sophisticated than stemming. It considers the context and converts the word to its meaningful base form. Example: The word ""better"" would be reduced to ""good."" Syntactic analysis involves parsing the structure of sentences to understand their grammatical structure. Part-of-Speech Tagging (POS)POS tagging involves labeling each word in a sentence with its corresponding part of speech, such as nouns, verbs, adjectives, etc. Example: In the sentence ""The quick brown fox jumps over the lazy dog,"" POS tagging identifies ""quick"" and ""brown"" as adjectives, ""fox"" as a noun, and ""jumps"" as a verb. Dependency ParsingDependency parsing involves understanding the dependencies between words in a sentence. It establishes the relationships between ""head"" words and words that modify those heads. Example: In the sentence ""She enjoys reading books,"" dependency parsing identifies ""enjoys"" as the main verb, with ""She"" as the subject and ""reading books"" as the object. ChunkingChunking, also known as shallow parsing, groups words in a sentence into meaningful ""chunks."" These chunks typically represent noun phrases, verb phrases, etc. Example: In the sentence ""He bought a new car,"" chunking would group ""a new car"" as a noun phrase. Semantic analysis focuses on understanding the meaning of the text. Named Entity Recognition (NER)NERinvolves identifying and classifying named entities in text into predefined categories such as names of people, organizations, locations, dates, etc. Example: In the sentence ""Barack Obama was born in Hawaii,"" NER identifies ""Barack Obama"" as a person and ""Hawaii"" as a location. Sentiment AnalysisSentiment analysis determines the sentiment expressed in a text, which can be positive, negative, or neutral. Example: The sentence ""I love this product"" would be classified as positive sentiment. Word Sense Disambiguation (WSD)WSD identifies the correct meaning of a word based on its context. Example: The word ""bank"" can mean a financial institution or the side of a river. In the sentence ""He sat by the river bank,"" WSD determines that ""bank"" refers to the side of a river. Advanced NLP techniques involve more complex and nuanced processing. Topic ModelingTopic modeling is a technique to discover abstract topics within a collection of documents. One of the most popular algorithms for topic modeling is Latent Dirichlet Allocation (LDA). Example: Analyzing a set of news articles, LDA might identify topics such as politics, sports, and technology. Text ClassificationText classification involves categorizing text into predefined categories. Example: An email filtering system might classify emails into categories like ""spam"" and ""not spam."" Machine TranslationMachine translation is the task of translating text from one language to another. Example: Translating the English sentence ""Hello, how are you?"" to Spanish would result in ""Hola, Â¿cÃ³mo estÃ¡s?"" SummarizationSummarization involves producing a concise summary of a longer text while preserving the main ideas. Example: Summarizing a long news article about a political event might result in a few sentences highlighting the key points. Deep learning has revolutionized NLP, enabling more sophisticated models and applications. Recurrent Neural Networks (RNNs)RNNs are designed for sequential data like text, where the output depends on the previous computations. Example: An RNN can be used to predict the next word in a sentence based on the previous words. Long Short-Term Memory Networks (LSTMs)LSTMs are a type of RNN that can capture long-term dependencies and context in text. They are used in tasks like machine translation, text generation, and speech recognition. Example: An LSTM model could be used to generate text that follows a specific style, such as writing poetry. Transformer ModelsTransformers, such as BERT, GPT-3, and T5, use self-attention mechanisms to handle context more effectively. They have set new benchmarks in various NLP tasks, including translation, summarization, and question answering. Example: GPT-3 can generate coherent and contextually relevant text based on a given prompt, such as writing an essay or answering questions. Sequence-to-Sequence ModelsSequence-to-sequence models are used for tasks where the input and output are sequences, such as translation and summarization. These models often use encoder-decoder architectures, where the encoder processes the input sequence and the decoder generates the output sequence. Example: In machine translation, a sequence-to-sequence model can translate an entire sentence from English to French, such as ""How are you?"" to ""Comment Ã§a va?"" NLP techniques are applied in a wide range of practical applications that impact our daily lives. ChatbotsChatbots are automated conversation agents that use NLP to understand and respond to user queries. They are widely used in customer service, virtual assistants, and interactive websites. Example: A customer service chatbot on an e-commerce site can help users track their orders or find product information. Search EnginesNLP enhances search engines by improving the relevance of search results. Techniques like semantic search help in understanding user intent and providing more accurate answers. Example: When a user searches for ""best smartphone 2024,"" the search engine can understand the intent and provide relevant results, including reviews and comparisons. Voice AssistantsVoice assistants like Siri, Alexa, and Google Assistant use NLP to understand voice commands and respond appropriately. They rely on speech recognition, language understanding, and dialogue management. Example: Asking a voice assistant, ""What's the weather like today?"" will result in a weather forecast for the current day. Content RecommendationNLP is used in content recommendation systems to suggest articles, videos, and products based on user preferences and behaviors. These systems analyze text content to determine user interests. Example: A streaming service might recommend movies based on the user's viewing history and preferences, using NLP to analyze movie descriptions and reviews. Natural Language Processing is a rapidly evolving field with a wide array of techniques and applications. From basic text preprocessing to advanced deep learning models, NLP enables machines to understand and interact with human language in increasingly sophisticated ways. As technology continues to advance, NLP will play an even more critical role in bridging the gap between humans and machines, making interactions more seamless and intuitive. P "
https://paperswithcode.com/area/natural-language-processing,
https://www.nature.com/articles/s41598-022-14101-4,"Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript. Advertisement Scientific Reportsvolume12, ArticleÂ number:10228(2022)Cite this article 5886Accesses 7Citations 3Altmetric Metricsdetails Everybody claims to be ethical. However, there is a huge difference between declaring ethical behavior and living up to high ethical standards. In this paper, we demonstrate that â€œhidden honest signalsâ€ in the language and the use of â€œsmall wordsâ€ can show true moral values and behavior of individuals and organizations and that this ethical behavior is correlated to real-world success; however not always in the direction we might expect. Leveraging the latest advances of AI in natural language processing (NLP), we construct three different â€œtribesâ€ of ethical, moral, and non-ethical people, based on Twitter feeds of people of known high and low ethics and morals: fair and modest collaborators codified as ethical â€œbeesâ€; hard-working competitive workers as moral â€œantsâ€; and selfish, arrogant people as non-ethical â€œleechesâ€. Results from three studies involving a total of 49 workgroups and 281 individuals within three different industries (healthcare, business consulting, and higher education) confirm the validity of our model. Associating membership in ethical or unethical tribes with performance, we find that being ethical correlates positively or negatively with success depending on the context. German army sergeant Anton Schmid was executed as a traitor by the German army for saving 300 Jews by shielding them from the Ponary massacre. While Schmid was recognized by Israel right after the Second World War, Schmidâ€™s widow was refused a pension after the war, and her windows were smashed by the neighbors as the wife of a traitor. Schmid was a true â€œbeeâ€, while the army and the neighbors acted as â€œantsâ€. Human â€œbeesâ€, just like the real bees pollinating the plants on our planet, are doing good for everybody. However, just like real bees, human â€œbeesâ€ frequently get little recognition for their essential contributions to the good of society. Worldly recognition goes to human â€œantsâ€ and â€œleechesâ€. Just like real ants sacrifice their lives for their hive while fighting to the death with ants from competing hives, human â€œantsâ€ are competitive workers who are well-embedded in their in-group and work hard to get ahead. It took the human ants in the German army over 50 years to change the moral code of their in-group and give Sergeant Schmid recognition for his ethical behavior by renaming a military base after him. While human ants value loyalty within their in-group, human â€œleechesâ€ are egoists. Just like real leeches, which steal their victimâ€™s blood for themselves, human â€œleechesâ€ only care about their benefits with little regard for the welfare of others. â€œBeesâ€ are ethical, â€œantsâ€ might have firm morals, while â€œleechesâ€ are un-ethical. While colloquially the terms ethics and morality are frequently used interchangeably, many philosophers, going back to Aristotle and Spinoza1,2, see ethics as the standard for discerning â€œgood vs. badâ€ or â€œright vs. wrongâ€ based on societal values, while they associate morality with the personal attitude of individuals towards others3. This means that ethical people are universally good, in the sense of â€œuniversalistsâ€ as defined in the Theory of Basic Human Values by Schwartz et al.4. Moral people care for the welfare of members of their in-group while having limited tolerance for behavior that deviates from their norms5. For instance, even people who support gay marriage think that gay sex is immoral6. Morals thus define a personal value system. People who share similar morals aggregate in virtual tribes, such as pro or contra abortion, or pro or contra vaccines7. While social pressure gets human â€œbeesâ€, â€œantsâ€, and â€œleechesâ€ to claim to act by high ethical standards, their underlying value systems exhibit radical differences. Just pretending to be ethical does not make one ethical. Enron had the most beautifully written code of ethics, while its entire upper-level management definitively behaved highly unethical, following their own â€œmoral codeâ€ of personal greed8. Applying the Schwartz system9of personal values, a â€œbeeâ€ would be an ethical adherent of universalism and benevolence, understanding and protecting all peopleâ€™s welfare and nature. â€œAntsâ€ and â€œleechesâ€, on the other hand, are strongly motivated by self-enhancement, striving for achievement and power. The key difference between the two is that â€œantsâ€ highly value tradition and loyalty to other members of their in-group, while â€œleechesâ€ only care about their own interests, with no concern for the welfare of others. In other words, bees are â€œethicalâ€, ants are â€œmoralâ€, and leeches are â€œamoralâ€. Differently from â€œbeesâ€, â€œantsâ€ and â€œleechesâ€ will thus stick to the moral value systems of their in-groups which might be ethical or unethical, with little compassion for the rest of society. In this paper, we explore the following question: how are ethical values correlated with individual and company performance? Research is contradictory, with some researchers finding that ethical leaders will create higher-performing organizations, while others find that unethical individuals will be promoted faster. Although religion and the law want individuals and companies to restrict competitive behavior and act ethically and according to social and community expectations, the reality is quite different. In business, law, and medicine, the concept of ethics serves as a personal code of conduct for people working in those fields, and ethical decisions themselves are often contested and challenged10,11,12. Frequently, â€œwho breaks the law without being caughtâ€ wins. For instance, personality characteristics of psychopaths and CEOs show worrying similarities13,14. Frequently the most egotistical person is chosen as the leader of an organization15. On the other hand, ethical leaders are highly appreciated by their subordinates. While authoritative and inflexible leadership might have worked in an earlier era, todayâ€™s workers demand inclusiveness, empowerment, and a collaborative approach to problem-solving. Employees do not respond positively to top-down leadership, commonly considered outdated and counterproductive. Rather, they expect managers to follow humble, servant, and ethical leadership styles that are conducive to a work environment that enhances trust and builds positive relationships16,17. Leaders in ethical organizations adopt collaborative approaches to promote engagement and fair behaviors without using authoritative power18. In traditional bureaucratic organizational models, leaders issue commands and expect compliance from subordinates, often through authoritative power. In organizations dominated by a command-and-control style, employees are not empowered to change a course of action even when they witness unethical or unlawful behaviors. Empirical evidence has shown how ethical leadership models enable followers to make decisions moving away from domineering or self-centered approaches18. Ethical and humble leadership has been associated with the perceived effectiveness of leaders, employeesâ€™ job satisfaction and dedication, and their willingness to report problems to management. Ethical leaders encourage normative behavior and discourage unethical behavior of their subordinates by being an ethical example, treating people fairly, and actively managing morality19. Previous research has traditionally explored the association between ethical behaviors and outcomes by adopting qualitative methods, including surveys and self-report questionnaires. Our study contributes to this literature on ethical decision-making by providing a complementary methodology based on the digital traces that individuals leave as they interact online. In this study, we leverage the latest advances in natural language processing (NLP) and build â€œbeeâ€, â€œantâ€, and â€œleechâ€ â€œtribesâ€ of ethical, moral, and amoral people. Tribes are groups composed of members connected through a common belief or ideology. The concept has been used primarily in the marketing literature to describe consumer behavior20. Individuals in the same tribe share similar behaviors and similar ethical values and emotional responses to external stimuli21. In the rest of this paper, we will use the term â€œethical valuesâ€ as the goal to aspire to, distinguishing between ethical bees, moral ants, and amoral leeches. The relationship between ethical values and behavior has attracted the interest of social scientists for several decades22,23. Values are defined as desirable goals that act as guiding principles in peopleâ€™s lives. They are then translated and become visible through individual behaviors and concrete actions. Values may be important to some people and unimportant to others24. Ethical identity has been positively related to prosocial behaviors such as charitable giving and negatively related to unethical behaviors such as lying25. Ethical identity acts as a â€œself-regulatory mechanismâ€ embedded in peopleâ€™s internalized notions of right and wrong, influencing individual ethical behavior26. To help resolve important behavioral and ethical issuesâ€”including discrimination or sexual harassmentâ€”scholars have stressed the role of universal ethical values in defining corporate codes of ethics9. The importance of ethical values in organizations is clearly explained by studies that document significant and positive relationships between firmsâ€™ social responsibility and financial performance27. Ethical decision-making and ethical leadership have been associated with increased business performance measured at the individual level. For instance, a case study of supervisor-subordinate dyadic data from Taiwanese organizations showed that subordinatesâ€™ business ethical values are positively associated with job performance and employee engagement28. According to the social learning theory, individuals learn appropriate behaviors through a role-modeling process by observing the behaviors of others around them29. Studies show that team members exposed to similar cues regarding norms and ethical behaviors tend to behave homogeneously. Group norms are formed and reinforced by leadersâ€™ behaviors, as they communicate as role models the importance of ethical values and use punishment and reward systems to encourage behaviors that align with cultural and universal values30. Empirical studies across various countries show that the ethical behavior of peers has the most significant impact on both individual moral values31and group ethical behavior32. Ethical leaders will influence their subordinates to adjust their morals to be more ethical. A 2020 study on ethical leadership in business confirms that ethical values, especially when modeled by leaders, enhance both individual and business performance33. A few studies have focused on measuring ethical values and ethical behaviors through the lenses of the big five factors of personality, suggesting that conscientiousness, agreeableness, and emotional stability are most consistently related to ethical leadership and agreeableness with power-sharing and fairness34. Recent empirical studies of European and African managers found that fairness of performance evaluation is associated with job satisfaction and mediated by trust and organizational commitment35. Other research has shown that satisfied employees increase business success36. In combination, this demonstrates that adhering to ethical values such as fairness will increase business performance. For example, research by Bowen et al.37indicates that just and fair behaviors in the workplace translate into increased customer satisfaction. Other studies exploring the impact of organizational justice in HRM practice provide evidence that behaviors that â€œhonor the justice principlesâ€ positively impact both job satisfaction and overall job performance38. Traditional approaches to measuring ethical values and ethical decision-making rely on data collected through surveys, questionnaires, or focus groups. For instance, a study involving middle-level managers and engineers at an aviation center relied on questionnaires to demonstrate the impact of ethical behavior on turnover intention39. Knafo and Sagiv40conducted 603 phone interviews with Israeli families to explore the relationship between values and occupational environments. Schwartz et al.41developed the Portrait Values Questionnaire (PVQ) based on Schwartzâ€™s theoryÂ ofÂ values, which identified ten fundamental individual values influencing human actions42. However, the survey-based approach has considerable disadvantages, as individuals are notoriously bad at self-assessment, either seeing themselves in too positive a light or being overly critical of themselves. Researchers have repeatedly found that an individualâ€™s friends are much better at rating the individualâ€™s personality traits than the individual43,44. AI and machine learning put new tools at the disposal of behavioral and organizational researchers, allowing them to automatically analyze electronic traces of individuals to predict their personality characteristics. AI thus leverages the â€œwisdom of the swarmâ€ to extend the judgment of friends by aggregating the assessment of large groups of people of the personality traits of an individual. To overcome these limitations traditionally associated with survey methods45, we use a system called Tribefinder, which scans digital documentsâ€”including emails and social media postsâ€”through a deep learning algorithm and considers the use of similar words in similar contexts. Tribefinder identifies tribal affiliations of individuals based on the words used by â€œtribal leadersâ€46. Tribefinder builds models of different tribes using LSTM47and Tensorflow48and trains their models with the Twitter streams of tribal leaders. The Machine Learning system built into Tribefinder assigns tribal membership based on word usage of individual tribe members on social media. The system proved to reach high classification accuracy values and Cohenâ€™s Kappa49. It computes a dictionary of tribal words and their distribution in the text using a probabilistic distribution of a dictionary of millions of words called â€œword embeddingsâ€. Once a tribe is created, the tribe members are plotted in proximity to each other, based on word usage and how they fit in with the predefined tribes7,50,51. Researchers have been using machine learning to identify ethical tribe categories based on the content shared on Twitter46or via email21. For instance, Morgan and Gloor21analyzed the communication habits of three morality tribes, i.e., nerds, treehuggers, and fatherlanders, and found that these tribes significantly differ in how they communicate by email. Recent research has used digital traces such as emails and social media posts to predict emotional and behavioral traits from email communication. Gloor and Fronzetti Colladon51found that communication patterns measured through e-mail interaction correspond with the ethical values of a person. Motivated by the discussion on the impact of ethics on performance in the previous section, we explore ethical and unethical behavior via the words used by team members, categorizing individuals into three tribes, ethical â€œbeesâ€, moral â€œantsâ€, and amoral â€œleechesâ€. To identify automatically tribal affiliation of â€œbeesâ€, â€œantsâ€, and â€œleechesâ€, three tribes for Tribefinder were created, with the bee tribe leaders being open-source developers and artists, the ant tribe members being competitive athletes, and the leech tribe leaders being hedge fund managers and peddlers of â€œgetting rich quickâ€ schemes. In general, we relied on the procedure suggested by Gloor52, where AI-based methods are introduced to identify the personality, moral values, and ethics of individuals based on their body language and interaction with others. Additionally, six other â€œpersonality attribute tribesâ€ were created to cross-verify the bees, ants, and leeches. We have chosen the representatives of these personality attribute tribes based on their perception in newspapers such as USA Today and People magazine and on Websites such as quora. Indeed, it has been shown that the language that individuals use in blogs and online forums can be a strong signal of their personality53. For instance, for the â€œarroganceâ€ tribe, members were chosen from celebrities with a reputation for arrogance, such as Charlie Sheen or Will Smith. For the â€œmodestyâ€ tribe we chose celebrities with a reputation for modesty, such as the Dalai Lama and Emma Watson. For the â€œfairnessâ€ tribe we considered social advocates and human rights activists. Lastly, the â€œunfairnessâ€ tribe was built based on people like the editor of â€œBreitbart Newsâ€ and hedge fund managers. The last two tribes are the â€œinterestâ€ tribe â€“ subsuming curiosity, a passion for learning, and exploration of unknowns, with members such as Steven Pinker and Bill Gatesâ€”and a â€œdisinterestâ€ tribe of â€œcouch potatoesâ€, that are individuals who are primarily interested in their hedonistic pleasures with members expressing their boredom on their Twitter profiles. It was quite hard to identify exemplary members for each tribe as, for instance, Lady Gaga has a reputation for being a comparatively modest down-to-earth artist, but artists in general by nature are gregarious extroverts and anything but modest. We, therefore, carefully cross-checked each member of these tribes by looking at their tweets and making sure that the tweets of members of the modesty tribe showed a very low arrogance score, which helped eliminate celebrities like Lady Gaga from the tribe. To verify the validity of our approach, we carry out an email network and content analysis, considering three different e-mail archives. For each archive, we build a social network based on the email interaction of individuals and teams, and we analyze the content of email bodies or subject lines. In this network, each email account is represented as a node, with emails translating into one or more links connecting different nodes. The first dataset, called â€œCOINcourseâ€, consists of three cohorts of students enrolled in an international seminar on Collaborative Innovation Networks over three semesters, with a total of 89 students working in 21 groups. The performance metric is the final grade for each group, given by three instructors. The email archive consists of 89 students sending a total of 871 emails. The contents of emails sent by the 89 students were used to calculate their behavioral and emotional scores. The second dataset, called â€œHealthcare Innovationâ€, consists of emails exchanged by 101 group members working in 11 innovation teams in the healthcare environment. The performance, innovation, and learning behaviors of each team were rated every other month for a year by three supervisors, who individually rated team performance, the capability of a team to learn new things, and the innovativeness of problem-solving methods. The total email dataset includes 1782 actors (the outgroup) sending 286,029 emails, which was used for calculating the network metrics, while the content of the 191,519 emails sent by the 101 group members (the in-group) was used for calculating their behavioral and emotional scores. The third dataset, called â€œService Companyâ€, consists of 91 managers who are part of 17 groups serving 17 large international customers of a global services firm. The managers are rated individually by their supervisors using three categories: outstanding, excellent, and good. The group performance is rated using the Net Promoter Score (NPS) collected from each groupâ€™s customers. NPS is a measure of customersâ€™ loyalty to a company and is calculated using the answer to a key question â€œOn a scale from 0 to 10, how likely is it that you would recommend a company (or brand) to a friend or colleague?â€54. The total email dataset includes 1752 actors who sent 769,125 emails (the outgroup). This was used for calculating the network values, while the subject lines of the 126,978 emails sent by the 91 managers (the in-group) were used to calculate their behavioral and emotional scores. Note that, for this dataset, we were only able to obtain the subject line of emails, instead of the content of the entire email exchange, because of privacy restrictions. However, it has been shown in earlier work that for e-mail content analysis, metrics derived from the subject line are correlated with metrics derived from contents55. Research has shown an intrinsic connection between ethical behavior and emotional response to an event. To support our method, we relied on the Basic Emotion theory, which proposes that human beings have a limited number of â€œbiologically basicâ€ emotions, including fear, anger, joy, and sadness56. A different classification has been offered by the Dimensional Theory of emotion, which uses three dimensions: pleasant-unpleasant, tension-relaxation, and excitation-calm57, or various adaptations of the Circumplex model, where each emotion is located on a quadrant that reflects varying amounts of hedonic and arousal properties. Other researchers investigated the role of specific emotions, including shame and empathy, as they play a fundamental role in morality, with guilt being often considered the quintessential â€œmoral emotionâ€58. To improve the accuracy of the algorithm built within the Tribefinder, we chose the framework of the Basic Emotion Theory, as it proposes a basic classification of four fundamental emotions, namely fear, anger, joy, and sadness. These emotions have been preserved because of their biological and social functions are associated with an organized recurring pattern of behavioral components59,60. The Basic Emotion Theory was adopted by a recent study that used facial emotion recognition to predict emotional response to visual stimuli, which highlights the strong association between personality characteristics and moral values of individuals61. Based on an individualâ€™s moral values, the individual will show different emotional responses. Therefore, besides the personality tribes, we also compute the emotionality of the emails using four categories: anger, fear, happiness, and sadness. The Tribefinder was trained to recognize these emotions, following a procedure similar to that used to classify personality attributes, i.e., training an AI model. We focus on these basic emotions as they have been considered by many to be the prototypical ones62. A combination of these emotions leads to more complex ones, as shown by Ekmanâ€™s Basic Emotions Theory63. The Basic Emotion Theory represented an appropriate framework to train the AI algorithm behind the Tribefinder, as it offers a classification of a limited number of emotions (i.e., fear, anger, joy, sadness) that are biologically and psychologically â€œbasicâ€ to all human beings. In addition to analyzing the language used in email communication, we calculated key social network metrics, including degree centrality, betweenness centrality, and average response time64,65, to identify individual prominence (degree centrality) and information brokerage (betweenness centrality). The average response time (ART) indicates how fast an individual or a group responds to e-mails, offering insights into the degree of respect that an individual commands and the level of commitment they show66. We also distinguished between â€œalter ARTâ€ and â€œego ARTâ€, respectively indicating the time taken by recipients to answer an actorâ€™s emails and the time taken by that actor to answer the emails they receive. These metrics are part of the six honest signals of collaboration described by Gloor67. Table1briefly summarizes the study variables. Our analysis followed two steps. First, we examined the behavior of people classified as bees, ants, and leeches and then related these roles to performance with metrics at both the individual and group level. Figure1shows the average values for both emotional (i.e., anger, fear, happiness, and sadness) and behavioral scores (i.e., arrogance, fairness, interest) of ants, leeches, and beesâ€”while considering the three datasets described in the previous section. Average emotional and behavioral scores of ants, leeches and bees. To evaluate the significance of mean differences, we carried out an analysis of variance, as presented in Tables2,3and4. Instead of using a classic ANOVA, we used Welchâ€™s ANOVA as a robust alternative in the case of unequal group variancesâ€”as indicated by the results of the Leveneâ€™s tests that we performed for all groups. Accordingly, we also ran a robust post-hoc analysis to evaluate significant group differences through Games-Howell tests. As shown in Fig.1and Tables2,3and4, bees and ants are less arrogant than leeches. Bees are also more interested and less fearful than leeches. Surprisingly, ants seem to be the happiest group. Our post-hoc analysis reveals that the most significant differences are usually between ants and leeches and between bees and leeches. This is partially dependent on the datasets used in the study. For example, significant differences between ants and bees emerge in the Healthcare Innovation dataset. The analysis of social network metrics indicates the presence of different behavioral patterns, again depending on the dataset. For example, we find that bees are much more central in the email network while considering the COINcourse and Healthcare Innovation datasetâ€”both in terms of degree and betweenness centrality. On the other hand, leeches are more activeâ€”they send more messages and have a higher degreeâ€”in the Service Company dataset. As the second step of the analysis, we looked for a relationship between performance and the individual classification of participants as ant, leech, or bee. The regression analysis produced the models presented in Tables5,6and7, which show the best models for each dataset. All our models were tested to exclude multicollinearity problems. The Variance Inflation Factor (VIF) values were reasonably lowâ€”always lower than 2.5 and, in most cases, also lower than 2. In Table5, we present the effect of the three categories (ants, bees, and leeches) on the group and individual performance, only relating to the Service Company dataset. As already mentioned, individual performance was judged by the supervisors of the managers participating in the study, while group performance was evaluated by the companyâ€™s clients and measured as customer satisfaction through the NPS indicator. Results from the regression analysis (Table5) indicate that individual ratings are higher when managers are less arrogant and in the ant category. On the other hand, more variables contribute to group performance, i.e., client satisfaction. Groups that received higher evaluations answered emails faster, had a lower number of leeches, and comprised less arrogant employees. Employees in these groups were also characterized by a lower degree centrality and lower interest. In other words, these employees were more focused on a smaller number of key customers, to whom they gave preferential treatment by answering them more quickly and talking less about topics of general interest. In Table6, we present the analysis carried out on the Healthcare Innovation dataset, where 11 groups were evaluated with respect to performance, innovation, and learning skills. As Table6shows, the presence of bees is particularly relevant for a good group performance. For innovation tasks, on the other hand, it seems more important to have focused communication (having a lower degree) and as few leeches as possible. Groups that present high innovation skills are more emotional, exhibiting higher levels of happiness and fear. Lastly, the presence of bees (and a low number of leeches) seems to favor group learning. Surprisingly, learning abilities are also higher when group members are less fair and more arrogant. Table7shows the best regression models for the COINcourse dataset, where a group of teachers evaluated 21 groups of students. Grades had continuous values, ranging from 1 to 2â€”with 2 representing the highest grade and 1 the lowest. In the COINcourse dataset, student groups that achieved a higher grade had more bees and fewer leeches (see Table7)â€”which is aligned with the results obtained for Group Learning in the Healthcare Innovation dataset. In addition, it seems that having higher betweenness centrality (probably increasing the possibility of integrating knowledge coming from multiple sources) is beneficial to performance. Surprisingly, groups with higher average levels of arrogance achieved a higher grade. This might have to do with the studentsâ€™ self-esteem, in that groups that were more self-assured in their presentations got a higher grade from their instructors. The analysis of variance across the three datasets suggests that the most significant differences in terms of emotional and behavioral scores are between ants and leeches and between bees and leeches. Regardless of the datasets and related industries (healthcare, higher education, or service companies), leeches are systematically emerging as being more arrogant and with less curiosity and passion for learning than bees. Leeches are also the tribe with the lowest happiness levels. This is consistent with previous studies looking at narcissistic behaviors through the lenses of social media posting. For example, leeches in our study display partially similar traits to the â€œtakersâ€, a personality type described by Adam Grant68: these individuals tend to be more self-promoting, arrogant, boastful, prone to anger, and self-absorbed. Our study also found that bees are less fearful than leeches. A possible explanation is that bees are driven by collaborative values of helping others independently of what they can receive in exchange. An interesting result is that ants seem to be happier than leeches. This might be explained by their desire to conform to society and rely on social norms to feel accepted by other members of their organization. Ants may be happier because their behavior better aligns with the social norms of their community, making them feel cheerier and at ease in their community. As demonstrated by Helliwell69, people tend to be happier when they work together for a worthy, non-individualistic purpose. Results were not always consistent across datasets. In the service company, individual performance was higher when managers were less arrogant and displayed traits typical of the â€œantâ€ tribe, such as valuing conformity and security and being tendentially conscientious and fair. This can be explained by a tendency of managers to provide positive assessments to employees who â€œfit the moldâ€, who are more aligned with expectations and follow shared values and morals. Since â€œbeesâ€ tend to take more social risks and are open to trying new things, this could translate into less easy behaviors to manage and control. Not surprisingly, groups that received higher evaluations were the ones that answered emails faster. This is aligned with previous studies showing that responding to emails at a reasonably fast rate improves customer satisfaction70. The highest performing groups also had a lower number of leeches and were composed of fewer arrogant employees, which is consistent with previous studies demonstrating the importance of humility and its impact on performance71. For instance, Nevicka et al.72found that a leaderâ€™s narcissism inhibits information exchange between group members, negatively impacting group performance. In the Healthcare Innovation dataset, the presence of beesâ€”i.e., individuals who are open to learning, try new things, and care for othersâ€”has a positive impact on group performance. A possible explanation is that bees might be acting as motivators for the group promoting idea generation, thanks to their tendency to embrace social risks and be open to new things. Results from the healthcare innovation dataset indicate that having leeches in your group may decrease your collective ability to innovate and learn. This might be explained by the tendency of leeches to favor self-promotion and to advertise their accomplishments rather than advancing the groupâ€™s goal73. Groups focused on innovation tasks might benefit from having a lower degree centrality, reducing the number of connections to others, and inviting as few leeches as possible. As demonstrated by Buffardi and Cambell73, people who behave like leeches have significantly more friends, often establishing superficial connections with the mere goal of advertising themselves. Because innovation, in particular at the idea generation stage, is a process of trial and error that leads to fluctuating emotions, it was not surprising to see that highly innovative groups were more emotional. Throughout the creative process of idea generation, it is natural to go through happiness when groups push things forward and emotions can turn negative when new problems emerge. Experiencing fear is also a possible sign of commitment, as group members demonstrate attachment to the project and care about the group success. While negative emotions might sometimes act as distractors in the professional and personal sphere, discrete negative emotions like fear and anger may spark proactivity and can be expected during the chaotic innovation process, as shown by studies exploring the relationship between affect and creativity74. Another result from the Healthcare datasetâ€”consistent with the positive effect of bees on group performanceâ€”is that groups improve their learning outcome if there are more bees and fewer leeches among them. Again, bees are characterized by a desire for learning and caring for others which are roles often associated with open innovation75. The surprising result that learning abilities are higher when group members are less fair and more arrogant might be explained by the need to incorporate some level of competitiveness within and among groups in order for learning to occur. See Cagiltay et al.76for a detailed explanation of how competition enhances learning and motivation. The results of the analysis conducted on the COINcourse dataset confirm the same conclusion we found in the Health Innovation dataset regarding the importance of forming groups that have more bees and fewer leeches. In this case, groups increased their chance of getting a good grade if there were more caring students and fewer self-centered students. At the same time, a surprising result was that when groups acted more arrogantly, they had a higher chance to get a higher grade, which contrasts with other studies showing that arrogance is negatively related to self-and other-rated task performance77. The impact of arrogance on group performance might be explained by incorporating the natural level of competitiveness that students develop prior to any class presentation. Higher group performance was also associated with higher betweenness centrality, highlighting the important role of students who acted as spokesperson (i.e., team leaders) and knowledge brokers between group members, other groups, and the instructors. Figure2summarizes the number of significant relationships we find in our best regression models, linking individual and group performance with individual traits and social network dynamics. Variables impacting group and individual performance. In this study, we investigated how organizations can effectively manage individual and team ethical behaviorsâ€”exemplified by bees and antsâ€”while avoiding unethical onesâ€”as exemplified by leeches. To this purpose, we measured the impact of behavioral and emotional traits on group performance, with a focus on the role of ethical behaviors in determining real-world success. Based on three different contexts, our findings indicate that exhibiting moral values, being fair, being open to others and new things, and caring for others, correlate positively or negatively with success depending on the tasks. This study corroborates evidence from organizational psychology68and ethical leadership17by suggesting that building groups composed of individuals with traits and values typical of bees and ants can lead to success. In contexts where groups are asked to come up with innovative ideas and learn from others, the presence of arrogant leeches might be detrimental to success. In other contexts where groups are tasked with responding to customer questions and solving their issues, or when they are tasked with presenting results in a classroom environment, group performance is higher if individuals are more self-oriented, unfair, and take ethical risks. This is aligned with evidence provided by evolutionary biology studies, where social animals face recurrent opportunities to engage in nonzero-sum exchanges: humans and other mammals who engage in cheating rather than cooperative behaviors and react with emotions that induce them to play â€œtit for tatâ€ have been found to have an advantage over those who had to figure out their next move using their general intelligence23. While bees, ants, and leeches represent three fundamental styles of social interaction based on moral and behavioral values, the lines between them are not hard and fast. For example, having ants might be better for some tasks and detrimental for others, and different configurations might be better, depending on contexts and business cases. This study contributes to the theories and practice of ethical decision-making by proposing the adoption of a new methodology based on computational social science that links ethical behaviors with business outcomes. The limited sample of individuals represents the main limitation of the study. Future studies should consider larger datasets and incorporate additional control variables, such as age, gender, or tenure within the organization, which we could not consider in this study due to privacy agreements. To conclude, we hope that in todayâ€™s age of big data, aggregating the ethical understanding of large groups of people through machine learning will assist in recognizing and rewarding the ethical courage of todayâ€™s â€œAnton Schmidâ€ without a 50-year delay. The study was conducted according to the guidelines of the Declaration of Helsinki and approved by the Institutional Review Board of MIT (protocol code 170181783) on 16 February 2017. Informed consent was obtained from all subjects involved in the study. The datasets generated during and/or analyzed during the current study are not publicly available due to privacy agreements. Kelemen, M. & Peltonen, T. Ethics, morality and the subject: The contribution of Zygmunt Bauman and Michel Foucault to â€˜postmodernâ€™ business ethics.Scand. J. Manage.17, 151â€“166 (2001). ArticleGoogle Scholar Wolfson, H. A.Philosophy of Spinoza(Harvard University Press, 1934). Google Scholar Palese, E. Ethics without morality, morality without ethicsâ€”Politics, identity, responsibility in our contemporary world.Open J. Philos.03, 366â€“371 (2013). ArticleGoogle Scholar Schwartz, S. H.et al.Refining the theory of basic individual values.J. Pers. Soc. Psychol.103, 663â€“688 (2012). ArticlePubMedGoogle Scholar Graeber, D. & Wengrow, D.The Dawn of Everything: A New History of Humanity(Penguin UK, 2021). Google Scholar Green, E. Lots of people who support gay marriage think gay sex is immoral.The Atlantic(2014). De Oliveira, J. M. & Gloor, P. A. GalaxyScope: Finding the â€œtruth of tribesâ€ on social media. InCollaborative Innovation Networks(eds Grippa, F.et al.) 153â€“164 (Springer, 2018). ChapterGoogle Scholar Benston, G. J. & Hartgraves, A. L. Enron: What happened and what we can learn from it.J. Account. Public Policy21, 105â€“127 (2002). ArticleGoogle Scholar Schwartz, M. S. Universal moral values for corporate codes of ethics.J. Bus. Ethics59, 27â€“44 (2005). ArticleGoogle Scholar Nussbaum, M. C. Hiding from humanity: Disgust, shame, and the law.J. Appl. Philos.24, 329.https://doi.org/10.1111/j.1468-5930.2007.00384.x(2009). ArticleGoogle Scholar Shank, C. A. Deconstructing the corporate psychopath: An examination of deceptive behavior.Rev. Behav. Financ.10, 163 (2018). Frey, B. S. & Meier, S. Are political economists selfish and indoctrinated? Evidence from a natural experiment.Econom. Inquiry41, 448 (2003). ArticleGoogle Scholar Boddy, C. R. Psychopathic leadership a case study of a corporate psychopath CEO.J. Bus. Ethics145, 141â€“156 (2017). ArticleGoogle Scholar Landay, K., Harms, P. D. & CredÃ©, M. Shall we serve the dark lords? A meta-analytic review of psychopathy and leadership.J. Appl. Psychol.104, 183â€“196 (2019). ArticlePubMedGoogle Scholar Babiak, P., Neumann, C. S. & Hare, R. D. Corporate psychopathy: Talking the walk.Behav. Sci. Law28, 174â€“193 (2010). PubMedGoogle Scholar Parris, D. L. & Peachey, J. W. A systematic literature review of servant leadership theory in organizational contexts.J. Bus. Ethics113, 377â€“393 (2013). ArticleGoogle Scholar Schein, E. H. & Schein, P. A.Humble Leadership: The Power of Relationships, Openness, and Trust(Berrett-Koehler Publishers, 2018). Google Scholar Ancona, D., Backman, E. & Isaacs, K. Nimble leadership.Harv. Bus. Rev.97, 74â€“83 (2019). Google Scholar Brown, M. E., TreviÃ±o, L. K. & Harrison, D. A. Ethical leadership: A social learning perspective for construct development and testing.Organ. Behav. Hum. Decis. Process.97, 117â€“134 (2005). ArticleGoogle Scholar Cova, B. & Cova, V. Tribal aspects of postmodern consumption research: The case of French in-line roller skaters.J. Consum. Behav.1, 67â€“76 (2001). ArticleGoogle Scholar Morgan, L. & Gloor, P. A. Identifying virtual tribes by their language in enterprise email archives. InDigital Transformation of Collaboration(eds Przegalinska, A.et al.) 95â€“110 (Springer, 2020). ChapterGoogle Scholar Barnea, M. F. & Schwartz, S. H. Values and voting.Polit. Psychol.19, 17â€“40 (1998). ArticleGoogle Scholar Graham, J.et al.Moral foundations theory.Adv. Exp. Soc. Psychol.47, 55â€“130 (2013). ArticleGoogle Scholar Bardi, A. & Schwartz, S. H. Values and behavior: Strength and structure of relations.Pers. Soc. Psychol. Bull.29, 1207â€“1220 (2003). ArticlePubMedGoogle Scholar Aquino, K., Freeman, D., Reed, A., Lim, V. K. G. & Felps, W. Testing a social-cognitive model of moral behavior: The interactive influence of situations and moral identity centrality.J. Pers. Soc. Psychol.97, 123â€“141 (2009). ArticlePubMedGoogle Scholar Aquino, K. & Reed, A. The self-importance of moral identity.J. Pers. Soc. Psychol.83, 1423â€“1440 (2002). ArticlePubMedGoogle Scholar Hasan, I., Kobeissi, N., Liu, L. & Wang, H. Corporate social responsibility and firm financial performance: The mediating role of productivity.J. Bus. Ethics149, 671â€“688 (2018). ArticleGoogle Scholar Jiang, D.-Y., Lin, Y.-C. & Lin, L.-C. Business moral values of supervisors and subordinates and their effect on employee effectiveness.J. Bus. Ethics100, 239â€“252 (2011). ArticleGoogle Scholar Bandura, A. Self-efficacy: Toward a unifying theory of behavioral change.Psychol. Rev.84, 191â€“215 (1977). ArticleCASPubMedGoogle Scholar Holtz, B. C. & Harold, C. M. Effects of leadership consideration and structure on employee perceptions of justice and counterproductive work behavior.J. Organ. Behav.34, 492â€“519 (2013). ArticleGoogle Scholar Deshpande, S. P., Joseph, J. & Prasad, R. Factors impacting ethical behavior in hospitals.J. Bus. Ethics69, 207â€“216 (2006). ArticleGoogle Scholar Palanski, M. E., Kahai, S. S. & Yammarino, F. J. Team virtues and performance: An examination of transparency, behavioral integrity, and trust.J. Bus. Ethics99, 201â€“216 (2011). ArticleGoogle Scholar Seidman, D. Why moral leadership matters now more than ever.World Economic Forumhttps://www.weforum.org/agenda/2021/02/why-moral-leadership-matters-now-more-than-ever/. Accessed 24 Feb 2022 (2021). Kalshoven, K., Den Hartog, D. N. & De Hoogh, A. H. B. Ethical leader behavior and big five factors of personality.J. Bus. Ethics100, 349â€“366 (2011). ArticleGoogle Scholar Sholihin, M. & Pike, R. Fairness in performance evaluation and its behavioural consequences.Account. Bus. Res.39, 397â€“413 (2009). ArticleGoogle Scholar Lyubomirsky, S., King, L. & Diener, E. The benefits of frequent positive affect: Does happiness lead to success?Psychol. Bull.131, 803â€“855 (2005). ArticlePubMedGoogle Scholar Bowen, D. E., Gilliland, S. W. & Folger, R. HRM and service fairness: How being fair with employees spills over to customers.Organ. Dyn.27, 7â€“23 (1999). ArticleGoogle Scholar Cropanzano, R., Bowen, D. E. & Gilliland, S. W. The management of organizational justice.Acad. Manage. Perspect.21, 34â€“48 (2007). ArticleGoogle Scholar Demirtas, O. & Akdogan, A. A. The effect of ethical leadership behavior on ethical climate, turnover intention, and affective commitment.J. Bus. Ethics130, 59â€“67 (2015). ArticleGoogle Scholar Knafo, A. & Sagiv, L. Values and work environment: Mapping 32 occupations.Eur. J. Psychol. Educ.19, 255 (2004). ArticleGoogle Scholar Schwartz, S. H.et al.Extending the cross-cultural validity of the theory of basic human values with a different method of measurement.J. Cross Cult. Psychol.32, 519â€“542 (2001). ArticleGoogle Scholar Schwartz, S. H. Are there universal aspects in the structure and contents of human values?J. Soc. Issues50, 19â€“45 (1994). ArticleGoogle Scholar Kelly, E. L. Consistency of the adult personality.Am. Psychol.10, 659â€“681 (1955). ArticleGoogle Scholar Jackson, J. J., Connolly, J. J., Garrison, S. M., Leveille, M. M. & Connolly, S. L. Your friends know how long you will live: A 75-year study of peer-rated personality traits.Psychol. Sci.26, 335â€“340 (2015). ArticlePubMedGoogle Scholar Pinsonneault, A. & Kraemer, K. Survey research methodology in management information systems: An assessment.J. Manage. Inf. Syst.10, 75â€“105 (1993). ArticleGoogle Scholar Gloor, P., Fronzetti Colladon, A., de Oliveira, J. M. & Rovelli, P. Put your money where your mouth is: Using deep learning to identify consumer tribes from word usage.Int. J. Inf. Manage.51, 101924 (2020). ArticleGoogle Scholar Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural Comput.9, 1735â€“1780 (1997). ArticleCASPubMedGoogle Scholar Abadi, M. TensorFlow: Learning functions at scale. InProc. 21st ACM SIGPLAN International Conference on Functional Programming1â€“1 (ACM, 2016).https://doi.org/10.1145/2951913.2976746. Gloor, P.et al.Identifying tribes on twitter through shared context. InCollaborative Innovation Networks(eds Song, Y.et al.) 91â€“111 (Springer, 2019). ChapterGoogle Scholar Jones, J. T., Pelham, B. W., Carvallo, M. & Mirenberg, M. C. How do I love thee? Let me count the Js: Implicit egotism and interpersonal attraction.J. Pers. Soc. Psychol.87, 665 (2004). ArticlePubMedGoogle Scholar Gloor, P. A. & Fronzetti Colladon, A. Heart beats brain: Measuring moral beliefs through e-mail analysis. InDigital Transformation of Collaboration(eds Przegalinska, A.et al.) (Springer, 2020). Google Scholar Gloor, P. A.Happymetrics: Leveraging AI to Untangle the Surprising Link Between Ethics, Happiness, and Business Success(2022). Guadagno, R. E., Okdie, B. M. & Eno, C. A. Who blogs? Personality predictors of blogging.Comput. Hum. Behav.24, 1993â€“2004 (2008). ArticleGoogle Scholar Reichheld, F. F. The one number you need to grow.Harv. Bus. Rev.81, 46â€“54 (2003). PubMedGoogle Scholar Fronzetti Colladon, A. & Gloor, P. Measuring the impact of spammers on e-mail and Twitter networks.Int. J. Inf. Manage.48, 254â€“262 (2019). ArticleGoogle Scholar Wilson-Mendenhall, C. D., Barrett, L. F. & Barsalou, L. W. Neural evidence that human emotions share core affective properties.Psychol. Sci.24, 947â€“956 (2013). ArticlePubMedGoogle Scholar Posner, J., Russel, J. A., Peterson, B. S., Russell, J. A. & Peterson, B. S. The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology.Dev. Psychopathol.17, 715â€“734 (2005). ArticlePubMedPubMed CentralGoogle Scholar Eisenberg, N. Emotion, regulation, and moral development.Annu. Rev. Psychol.51, 665â€“697 (2000). ArticleCASPubMedGoogle Scholar Jack, R. E., Garrod, O. G. B. & Schyns, P. G. Dynamic facial expressions of emotion transmit an evolving hierarchy of signals over time.Curr. Biol.24, 187â€“192 (2014). ArticleCASPubMedGoogle Scholar Barrett, L. & Russel, J.The Psychological Construction of Emotion(The Guilford Press, 2014). Google Scholar Gloor, P. A.et al.Your face mirrors your deepest beliefsâ€”Predicting personality and morals through facial emotion recognition.Future Internet14, 5 (2022). ArticleGoogle Scholar Saif, M. & Turney, P. Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. InProc. NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text26â€“34 (Association for Computational Linguistics, 2010). Ekman, P. An argument for basic emotions.Cogn. Emot.6, 169â€“200 (1992). ArticleGoogle Scholar Merten, F. & Gloor, P. Too much E-mail decreases job satisfaction.Procedia Soc. Behav. Sci.2, 6457â€“6465 (2010). ArticleGoogle Scholar Wasserman, S. & Faust, K.Social Network Analysis: Methods and Applications(Cambridge University Press, 1994). BookMATHGoogle Scholar Gloor, P. A. & Grippa, F. Creating collaborative innovation networks (COINs) to reduce infant mortality. InCollaborative Innovation Networks(eds Grippa, F.et al.) 75â€“91 (Springer, 2018). ChapterGoogle Scholar Gloor, P.Sociometrics and Human Relationships: Analyzing Social Networks to Manage Brands, Predict Trends, and Improve Organizational Performance(Emerald Publishing Limited, 2017). BookGoogle Scholar Grant, A.Give and Take: A Revolutionary Approach to Success(Orion Publishing Group Ltd, 2014). Google Scholar Helliwell, J. F., Huang, H. & Wang, S. Social capital and well-being in times of crisis.J. Happiness Stud.15, 145â€“162 (2014). ArticleGoogle Scholar Gloor, P., Fronzetti Colladon, A., Giacomelli, G., Saran, T. & Grippa, F. The impact of virtual mirroring on customer satisfaction.J. Bus. Res.75, 67â€“76 (2017). ArticleGoogle Scholar Owens, B. P., Johnson, M. D. & Mitchell, T. R. Expressed humility in organizations: Implications for performance, teams, and leadership.Organ. Sci.24, 1517â€“1538 (2013). ArticleGoogle Scholar Nevicka, B., Ten Velden, F. S., De Hoogh, A. H. B. & Van Vianen, A. E. M. Reality at odds with perceptions.Psychol. Sci.22, 1259â€“1264 (2011). ArticlePubMedGoogle Scholar Buffardi, L. E. & Campbell, W. K. Narcissism and social networking web sites.Pers. Soc. Psychol. Bull.34, 1303â€“1314 (2008). ArticlePubMedGoogle Scholar James, K., Brodersen, M. & Eisenberg, J. Workplace affect and workplace creativity: A review and preliminary model.Hum. Perform.17, 169â€“194 (2004). ArticleGoogle Scholar GemÃ¼nden, H. G., Salomo, S. & HÃ¶lzle, K. Role models for radical innovations in times of open innovation.Creat. Innov. Manage.16, 408â€“421 (2007). ArticleGoogle Scholar Cagiltay, N. E., Ozcelik, E. & Ozcelik, N. S. The effect of competition on learning in games.Comput. Educ.87, 35â€“41 (2015). ArticleGoogle Scholar Johnson, R. E.et al.Acting superior but actually inferior?: Correlates and consequences of workplace arrogance.Hum. Perform.23, 403â€“427 (2010). ArticleGoogle Scholar Download references MIT Center for Collective Intelligence, 245 First Street, Cambridge, MA, 02142, USA Peter Gloor Department of Engineering, University of Perugia, Via G. Duranti 93, 06125, Perugia, Italy Andrea Fronzetti Colladon Northeastern University, 360 Huntington Avenue, Boston, MA, 02115, USA Francesca Grippa You can also search for this author inPubMedGoogle Scholar You can also search for this author inPubMedGoogle Scholar You can also search for this author inPubMedGoogle Scholar P.G.: Conceptualization; Methodology; Software; Writingâ€”Original Draft; Writingâ€”Review & Editing; Funding acquisition. A.F.C.: Methodology; Formal analysis; Data Curation; Writingâ€”Original Draft; Writingâ€”Review & Editing; Visualization. F.G.: Writingâ€”Original Draft; Writingâ€”Review & Editing; Funding acquisition. Correspondence toAndrea Fronzetti Colladon. The authors declare no competing interests. Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visithttp://creativecommons.org/licenses/by/4.0/. Reprints and permissions Gloor, P., Fronzetti Colladon, A. & Grippa, F. Measuring ethical behavior with AI and natural language processing to assess business success.Sci Rep12, 10228 (2022). https://doi.org/10.1038/s41598-022-14101-4 Download citation Received:25 February 2022 Accepted:01 June 2022 Published:17 June 2022 DOI:https://doi.org/10.1038/s41598-022-14101-4 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article.  Provided by the Springer Nature SharedIt content-sharing initiative Advertisement Scientific Reports (Sci Rep)ISSN2045-2322(online) Â© 2024 Springer Nature Limited Sign up for theNature Briefing: AI and Roboticsnewsletter â€” what matters in AI and robotics research, free to your inbox weekly."
https://www.geeksforgeeks.org/nlp-vs-nlu-vs-nlg/,"Natural Language Processing(NLP)is a subset of Artificial intelligence which involves communication between a human and a machine using a natural language than a coded or byte language. It provides the ability to give instructions to machines in a more easy and efficient manner. Natural Language Understanding(NLU)is an area of artificial intelligence to process input data provided by the user in natural language say text data or speech data. It is a way that enables interaction between a computer and a human in a way like humans do using natural languages like English, French, Hindi etc. Natural Language Generation(NLG)is a sub-component of Natural language processing that helps in generating the output in a natural language based on the input provided by the user. This component responds to the user in the same language in which the input was provided say the user asks something in English then the system will return the output in English.  Following is a table of differences between NLP and NLU and NLG: Natural Language Processing (NLP) Natural Language Understanding (NLU) Natural Language Generation (NLG) J "
https://github.com/topics/nlp-projects,"We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see ourdocumentation. 500 AI Machine learning Deep learning Computer vision NLP Projects with code This repository showcases a selection of machine learning projects undertaken to understand and master various ML concepts. Each project reflects commitment to applying theoretical knowledge to practical scenarios, demonstrating proficiency in machine learning techniques and tools. Enjoy major NLP Projects. I used NLP libraries,ML for NLP,DL for NLP....etc. In this repository, i will be making some cool projects in NLP from basic to intermediate level. This Repository Contain All the Artificial Intelligence Projects such as Machine Learning, Deep Learning and Generative AI that I have done while understanding Advanced Techniques & Concepts. A python knowledge-based chatbot application built with Tkinter 100+ AI Machine learning Deep learning Computer vision NLP Projects with code An Enthusiastic undergraduate with a passion for Data Science and Machine learning. With over a year of hands-on experience in the field, I'm constantly exploring the exciting world of AI and innovation. I've built a solid foundation in machine learning and Python. Deep Learning for NLP Projects related to Data Science This is my curated collection of notes, materials, projects on DNN, CNN and RNN. ""Welcome to my NLP mini-projects repository! Here, I'll share a collection of projects that explore various natural language processing (NLP) techniques and tools. From sentiment analysis to text classification, each project is designed to help you gain a better understanding of NLP and its applications. Whether you're new to NLP or an experienced The repository encompasses a collection of my Artificial Intelligence project Notebook files, ranging from Machine Learning to Deep Learning and Generative AI. These projects reflect my exploration and comprehension of advanced techniques and concepts within the field Important points and code from the famous book: Natural Language Processing using python by Steven Bird, Ewan Klein and Edward Loper A Playground for Natural Language Processing Practitioners - and a Professional Portfolio. Python implementation of different Natural Language Processing tasks NLP project demonstrating various techniques, including text preprocessing, feature extraction, and model training, without deployment. This repository contains the NLP and Text Mining R script and the generated charts namely Sentiment Pie chart, Emotion Bar chart and Word Cloud chart. Decoding Sentiments in Amazon Feedback Add a description, image, and links to thenlp-projectstopic page so that developers can more easily learn about it. Curate this topic To associate your repository with thenlp-projectstopic, visit your repo's landing page and select ""manage topics."" Learn more"
https://www.analyticssteps.com/blogs/ethical-considerations-natural-language-processing-nlp,"Natural Language Processing (NLP)has emerged as a powerful tool in modern society, impacting various fields such as healthcare, finance, andmarketing. NLP involves training machine learning models to process human language, enabling computers to understand, interpret, and generate human-like text. While NLP has shown promising results, it raises significant ethical concerns related to bias, privacy, and transparency.  Bias in NLP modelscan arise due to various reasons such as the quality of data used for training, the choice of algorithms used for processing, and the assumptions made by the developers. This can result in NLP models that are discriminatory towards certain groups, perpetuating social inequalities and hindering progress toward a fairer society.  Privacy concerns arise when NLP models are used to analyze personal data, such as emails, messages, or social media posts. This data can be used to infer sensitive information about individuals, leading to potential violations of privacy and confidentiality.  Transparency is critical in ensuring the responsible development and deployment of NLP models. The lack of transparency can make it difficult to understand how NLP models work, leading to a lack of trust in their decisions and outputs.  This article will explore the ethical considerations of NLP in detail, highlighting the challenges and potential solutions to address these issues. It will also discuss the importance of responsible AI and how stakeholders can work together to ensure the ethical use of NLP.   Natural Language Processing (NLP) has become increasingly popular over the past few years, enabling machines to understand and process human language. NLP technology has many practical applications, from chatbots and virtual assistants tosentiment analysisand machine translation. However, like any technology, NLP raises ethical considerations that must be addressed to ensure that it is used responsibly.  One of the most significantethical concerns associated with NLPis bias. NLP models are trained on large datasets, and the quality of their output depends on the quality and diversity of the data they are trained on. If thetraining data is biased, the NLP model may learn and perpetuate that bias, leading to unfair or discriminatory outcomes. For example, an NLP-based recruitment system may discriminate against candidates based on their race or gender, even if unintentionally.  To address this issue, it is essential to ensure that NLP models are designed and trained on diverse and representative datasets that are free from bias. Additionally, it is crucial to conduct regular audits of NLP systems to identify and address any bias that may exist in the models or the data they are trained on.  Another ethical concern related to NLP is privacy. NLP systems often rely on large amounts of personal data, such as text messages, emails, and social media posts, to provide insights and make predictions. This data can be sensitive and personal, and individuals may not be aware that it is being collected or used by NLP systems.  To protect privacy, it is crucial to ensure that NLP systems are designed with privacy in mind. This includes using data minimization techniques to reduce the amount of personal data collected, providing clear and transparent information about how data is being used, and implementing appropriate security measures to protect data from unauthorized access or theft.  Transparency is also an essential ethical consideration in NLP. It is often difficult to understand how NLP models arrive at their predictions or recommendations, which can lead to distrust and confusion among users. To address this issue, it is essential to ensure that NLP systems are transparent and explainable, with clear documentation and visualizations that enable users to understand how the models are making decisions.  In addition to these ethical concerns, NLP can also be used to promote ethical communication and empathy. For example, NLP-based chatbots can be used to provide mental health support and counseling, enabling individuals to access help and support when they need it most. NLP can also be used to analyze social media posts and identify instances of hate speech or bullying, enabling organizations to take action to promote social justice and equality.  NLP has manypractical applications, but it also raises significant ethical considerations related to bias, privacy, and transparency. To ensure that NLP is used responsibly, it is essential to design and train models on diverse and representative datasets, protect privacy, and ensure transparency and explainability. Additionally, NLP can be used to promote ethical communication and empathy, making a positive impact on society.  Also read |10 Top NLP Tools in 2022   The following are someexamples of ethical issuesin NLP:  Privacy concerns:NLP technology can analyze and extract personal information from text data, which can lead to potential privacy breaches. Companies and organizations need to ensure that they are complying with relevant privacy laws and regulations and that they are transparent with their customers about how their data is being used.  Bias in language models:NLP models are trained on large datasets, which can include biases that are present in the data. This can result in biased language models that perpetuate stereotypes and discrimination. It is important to identify and address bias in language models to ensure that they are fair and inclusive.  Misinformation and fake news:NLP models can be used to generate fake news and misinformation, which can have serious consequences for society. It is important to develop techniques that can identify and filter out fake news and misinformation from text data.  Ownership of text data:NLP models require large amounts of text data to be trained on, which can raise questions about ownership and control of the data. It is important to establish ethical guidelines for the collection, use, and sharing of text data to ensure that it is done in a responsible and transparent way.  Use of NLP in surveillance:NLP technology can be used to monitor and analyze large volumes of text data, which can raise concerns about privacy and surveillance. It is important to establish clear guidelines and regulations around the use of NLP technology in surveillance to ensure that it is used in a responsible and ethical manner.  Also read |8 NLP Techniques to Extract Information   Solving the ethical considerations in NLP requires a multi-pronged approach, involving both technical solutions and broader societal and regulatory changes. In this essay, I will outline somepotential solutionsto the ethical considerations raised by NLP, including strategies for addressing privacy concerns, mitigating biases, combating misinformation and manipulation, and promoting transparency and accountability.   To address privacy concerns in NLP, developers must take steps to ensure that NLP models are developed and deployed in a manner that protects individual privacy. This may include implementing data minimization techniques, such as limiting the amount of data collected and deleting data once it is no longer needed. It may also involve implementing strong encryption and access controls to prevent unauthorized access to sensitive data. Furthermore, developers should conduct regular privacy impact assessments to identify and mitigate potential privacy risks associated with their NLP models.   To mitigate biases in NLP, developers should focus on developing more diverse and representative datasets. This may involve collecting data from a range of sources, including underrepresented groups, and ensuring that the data is free from bias and discrimination. Developers should also implement bias mitigation techniques, such as algorithmic fairness measures, to ensure that their models are fair and equitable. Furthermore, developers should conduct regular audits and evaluations of their NLP models to identify and address potential biases.   To combat misinformation and manipulation in NLP, developers should implement robust validation and verification procedures to ensure that the data and models they are working with are accurate and reliable. This may involve implementing fact-checking mechanisms, using trusted sources of data, and ensuring that models are regularly updated to reflect changes in the data. Developers should also implement measures to detect and prevent the spread of fake news and misinformation, such as using natural language generation (NLG) models to produce counter-narratives.   To promote transparency and accountability in NLP, developers should focus on developing more interpretable models. This may involve using techniques such as explainable AI (XAI) to make NLP models more transparent and easier to understand. Developers should also implement measures to ensure that NLP models are accountable, such as providing users with clear explanations of how the model works and what its limitations are. Furthermore, developers should work to ensure that NLP models are subject to appropriate regulatory oversight and that users are aware of their rights and how to exercise them.  Solving the ethical considerations in NLP requires a multifaceted approach, involving both technical and societal solutions. Developers must focus on addressing privacy concerns, mitigating biases, combating misinformation and manipulation, and promoting transparency and accountability.  Furthermore, governments and regulators must work to ensure that NLP models are subject to appropriate regulatory oversight and that users are aware of their rights and how to exercise them. Only by addressing these ethical considerations can NLP be developed and deployed in a responsible and ethical manner.   In conclusion, ethical considerations in NLP are of utmost importance. While NLP has the potential to revolutionize the way we communicate and interact with technology, it also presents a range of ethical challenges. These challenges include privacy concerns, biases, misinformation and manipulation, and transparency and accountability.  Addressing these challenges requires a multifaceted approach, involving technical solutions such as data minimization, bias mitigation techniques, and explainable AI, as well as broader societal and regulatory changes.  It is crucial that developers, governments, and regulators work together to ensure that NLP is developed and deployed in a responsible and ethical manner. Only then can we fully harness the potential of NLP while minimizing the risks and maximizing the benefits for society as a whole. Or Be a part of ourInstagramcommunity 5 Factors Influencing Consumer Behavior Elasticity of Demand and its Types An Overview of Descriptive Analysis What is PESTLE Analysis? Everything you need to know about it What is Managerial Economics? Definition, Types, Nature, Principles, and Scope 5 Factors Affecting the Price Elasticity of Demand (PED) 6 Major Branches of Artificial Intelligence (AI) Scope of Managerial Economics Dijkstraâ€™s Algorithm: The Shortest Path Algorithm Different Types of Research Methods I URGENTLY NEED MY EX BACK 2023 EMAIL PRIEST OSAS ONÂ UNSURPASSED.SOLUTIO@GMAIL.COMÂ OR WHATSAPP HIM +1(419)3594367

My name is Stephanie Janet .This is a very joyful day of my life because of the help PRIEST OSAS has rendered to me by helping me get my ex husband back with his magic and love spell. i was married for 6 years and it was so terrible because my husband was really cheating on me and was seeking for a divorce but when i came across PRIEST OSAS email on the internet on how he help so many people to get theirÂ ex back and help fixing relationship and make people to be happy in their relationship. I explained my situation to him and then sought his help but to my greatest surprise he told me that he will help me with my case and here I am now celebrating because my Husband has changed totally for good. He always wants to be by me and can not do anything without my present. I am really enjoying my marriage, what a great celebration. I will keep on testifying on the internet because PRIEST OSASÂ  is truly a real spell caster.Â Â EMAIL: Unsurpassed.solution@gmail.comÂ or callÂ WhatsApp +1(419)3594367Â Â He is the only answer to your problem and makes you feel happy in your relationship
https://unsurpassedsolutionhome.website2.me/

https://unsurpassedsolution.blogspot.com/ I URGENTLY NEED MY EX BACK 2023 EMAIL PRIEST OSAS ONÂ UNSURPASSED.SOLUTIO@GMAIL.COMÂ OR WHATSAPP HIM +1(419)3594367

My name is Stephanie Janet .This is a very joyful day of my life because of the help PRIEST OSAS has rendered to me by helping me get my ex husband back with his magic and love spell. i was married for 6 years and it was so terrible because my husband was really cheating on me and was seeking for a divorce but when i came across PRIEST OSAS email on the internet on how he help so many people to get theirÂ ex back and help fixing relationship and make people to be happy in their relationship. I explained my situation to him and then sought his help but to my greatest surprise he told me that he will help me with my case and here I am now celebrating because my Husband has changed totally for good. He always wants to be by me and can not do anything without my present. I am really enjoying my marriage, what a great celebration. I will keep on testifying on the internet because PRIEST OSASÂ  is truly a real spell caster.Â Â EMAIL: Unsurpassed.solution@gmail.comÂ or callÂ WhatsApp +1(419)3594367Â Â He is the only answer to your problem and makes you feel happy in your relationship
https://unsurpassedsolutionhome.website2.me/

https://unsurpassedsolution.blogspot.com/ i want to share to the whole world how Dr Kachi the Great of all the Spell Caster, that helped me reunite my marriage back, my Ex Husband broke up with me 3months ago, I have been trying to get him back ever since then, i was worried and so confused because i love him so much. I was really going too much depressed, he left me with my kids and just ignored me constantly. I have begged him for forgiveness through text messages for him to come back home and the kids crying and miss their dad but he wont reply, I wanted him back desperately. we were in a very good couple and yet he just ignores me and get on with his life just like that, so i was looking for help after reading a post of Dr Kachi on the internet when i saw a lady name SHARRON testified that Dr Kachi cast a Pure love spell to stop divorce. and i also met with other, it was about how he brought back her Ex lover in less than 24 hours at the end of her testimony she dropped his email, I contacted Dr Kachi via email and explained my problem to Dr Kachi and he told me what went wrong with my husband and how it happen, that he will restored my marriage back, and to my greatest surprise my Ex husband came back to me, and he apologized for his mistake, and for the pain he caused me and my children. Then from that day our marriage is now stronger than how it was before, Dr Kachi you're a real spell caster, you can also get your Ex back and live with him happily: Contact Email drkachispellcast@gmail.comÂ his Text Number and Call:Â +1 (209) 893-8075 his Website: https://drkachispellcaster.wixsite.com/my-site i want to share to the whole world how Dr Kachi the Great of all the Spell Caster, that helped me reunite my marriage back, my Ex Husband broke up with me 3months ago, I have been trying to get him back ever since then, i was worried and so confused because i love him so much. I was really going too much depressed, he left me with my kids and just ignored me constantly. I have begged him for forgiveness through text messages for him to come back home and the kids crying and miss their dad but he wont reply, I wanted him back desperately. we were in a very good couple and yet he just ignores me and get on with his life just like that, so i was looking for help after reading a post of Dr Kachi on the internet when i saw a lady name SHARRON testified that Dr Kachi cast a Pure love spell to stop divorce. and i also met with other, it was about how he brought back her Ex lover in less than 24 hours at the end of her testimony she dropped his email, I contacted Dr Kachi via email and explained my problem to Dr Kachi and he told me what went wrong with my husband and how it happen, that he will restored my marriage back, and to my greatest surprise my Ex husband came back to me, and he apologized for his mistake, and for the pain he caused me and my children. Then from that day our marriage is now stronger than how it was before, Dr Kachi you're a real spell caster, you can also get your Ex back and live with him happily: Contact Email drkachispellcast@gmail.comÂ his Text Number and Call:Â +1 (209) 893-8075 his Website: https://drkachispellcaster.wixsite.com/my-site i want to share to the whole world how Dr Kachi the Great of all the Spell Caster, that helped me reunite my marriage back, my Ex Husband broke up with me 3months ago, I have been trying to get him back ever since then, i was worried and so confused because i love him so much. I was really going too much depressed, he left me with my kids and just ignored me constantly. I have begged him for forgiveness through text messages for him to come back home and the kids crying and miss their dad but he wont reply, I wanted him back desperately. we were in a very good couple and yet he just ignores me and get on with his life just like that, so i was looking for help after reading a post of Dr Kachi on the internet when i saw a lady name SHARRON testified that Dr Kachi cast a Pure love spell to stop divorce. and i also met with other, it was about how he brought back her Ex lover in less than 24 hours at the end of her testimony she dropped his email, I contacted Dr Kachi via email and explained my problem to Dr Kachi and he told me what went wrong with my husband and how it happen, that he will restored my marriage back, and to my greatest surprise my Ex husband came back to me, and he apologized for his mistake, and for the pain he caused me and my children. Then from that day our marriage is now stronger than how it was before, Dr Kachi you're a real spell caster, you can also get your Ex back and live with him happily: Contact Email drkachispellcast@gmail.comÂ his Text Number and Call:Â +1 (209) 893-8075 his Website: https://drkachispellcaster.wixsite.com/my-site THIS IS HOW YOU CAN RECOVER YOUR LOST CRYPTO? Are you a victim of Investment, BTC, Forex, NFT, Credit card, etc Scam? Do you want to investigate a cheating spouse? Do you desire credit repair (all bureaus)? Contact Hacker Steve (Funds Recovery agent) asap to get started. He specializes in all cases of ethical hacking, cryptocurrency, fake investment schemes, recovery scam, credit repair, stolen account, etc. Stay safe out there! 
Hackersteve911@gmail.com
https://hackersteve.great-site.net/ Copyright Â© Analytics Steps Infomedia LLP 2020-24. All Rights Reserved."
https://learn.microsoft.com/en-us/training/paths/explore-natural-language-processing/,"This browser is no longer supported. Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support. Natural language processing supports applications that can see, hear, speak with, and understand users. Using text analytics, translation, and language understanding services, Microsoft Azure makes it easy to build applications that support natural language. Ability to navigate the Azure portal. Achievement Code Would you like to request an achievement code? Explore Azure AI Language's natural language processing (NLP) features, which include sentiment analysis, key phrase extraction, named entity recognition, and language detection. Create a custom question answering knowledge base with Azure AI Language. In this module, we introduce you to conversational language understanding, and show how to create applications that understand language with Azure AI Language. Learn how to recognize and synthesize speech by using Azure AI Speech. Automated translation capabilities in an AI solution enable closer collaboration by removing language barriers."
https://www.mdpi.com/2076-3417/12/18/9207,"You are accessing a machine-readable page. In order to be human-readable, please install an RSS reader. All articles published by MDPI are made immediately available worldwide under an open access license. No special 
        permission is required to reuse all or part of the article published by MDPI, including figures and tables. For 
        articles published under an open access Creative Common CC BY license, any part of the article may be reused without 
        permission provided that the original article is clearly cited. For more information, please refer tohttps://www.mdpi.com/openaccess. Feature papers represent the most advanced research with significant potential for high impact in the field. A Feature 
        Paper should be a substantial original Article that involves several techniques or approaches, provides an outlook for 
        future research directions and describes possible research applications. Feature papers are submitted upon individual invitation or recommendation by the scientific editors and must receive 
        positive feedback from the reviewers. Editorâ€™s Choice articles are based on recommendations by the scientific editors of MDPI journals from around the world. 
        Editors select a small number of articles recently published in the journal that they believe will be particularly 
        interesting to readers, or important in the respective research area. The aim is to provide a snapshot of some of the 
        most exciting work published in the various research areas of the journal. Original Submission Date Received:. Find support for a specific problem in the support section of our website. Please let us know what you think of our products and services. Visit our dedicated information section to learn more about MDPI. Abstract:Introduction:The advances in the digital era have necessitated the adoption of communication as the main channel for modern business. In the past, business negotiations, profiling, seminars, shopping, and agreements were in-person but today everything is almost digitalized.Objectives:The study aims to examine how the Internet of things (IoTs) connects text-object as part of NLP and AI responding to human needs. Also, how precipitated changes in the business environment and modern applications such as NLP and AI embedded with IoTs services have changed business settings.Problem statement:As communication takes lead in the business environment, companies have developed sophisticated applications of NLP that take human desires and fulfill them instantly with the help of text, phone calls, smart records, and chatbots. The ease of communication and interaction has shown a greater influence on customer choice, desires, and needs. Modern service providers now use email, text, phone calls, smart records, and virtual assistants as first contact points for almost all of their dealings, customer inquiries, and most preferred trading channels.Method:The study uses text content as part of NLP and AI to demonstrate how companies capture customersâ€™ insight and how they use IoTs to influence customersâ€™ reactions, responses, and engagement with enterprise management in Industry 4.0. The â€œBehavior-oriented drive and influential function of IoTs on Customers in Industry 4.0â€ concept was used in this study to determine the influence of Industry 4.0 on customers.Results:The result indicates the least score of 12 out of 15 grades for all the measurements on a behavior-oriented drive and influential function of IoTs on customers.Conclusion:The study concluded that NLP and AI are the preferred system for enterprise management in the era of Industry 4.0 to understand customersâ€™ demands and achieve customer satisfaction. Therefore, NLP and AI techniques are a necessity to attain business goals.Keywords:natural language processing;artificial intelligence;Internet of Things;enterprise management;Industry 4.0 Mah, P.M.;                     Skalna, I.;                     Muzam, J.    
        Natural Language Processing and Artificial Intelligence for Enterprise Management in the Era of Industry 4.0.Appl. Sci.2022,12, 9207.
    https://doi.org/10.3390/app12189207 Mah PM,                                 Skalna I,                                 Muzam J.        
                Natural Language Processing and Artificial Intelligence for Enterprise Management in the Era of Industry 4.0.Applied Sciences. 2022; 12(18):9207.
        https://doi.org/10.3390/app12189207 Mah, Pascal Muam,                                 Iwona Skalna,                                 and John Muzam.        
                2022. ""Natural Language Processing and Artificial Intelligence for Enterprise Management in the Era of Industry 4.0""Applied Sciences12, no. 18: 9207.
        https://doi.org/10.3390/app12189207 Mah, P. M.,                                 Skalna, I.,                                 & Muzam, J.        
        
        (2022). Natural Language Processing and Artificial Intelligence for Enterprise Management in the Era of Industry 4.0.Applied Sciences,12(18), 9207.
        https://doi.org/10.3390/app12189207 Mah, P.M.;                     Skalna, I.;                     Muzam, J.    
        Natural Language Processing and Artificial Intelligence for Enterprise Management in the Era of Industry 4.0.Appl. Sci.2022,12, 9207.
    https://doi.org/10.3390/app12189207 Mah PM,                                 Skalna I,                                 Muzam J.        
                Natural Language Processing and Artificial Intelligence for Enterprise Management in the Era of Industry 4.0.Applied Sciences. 2022; 12(18):9207.
        https://doi.org/10.3390/app12189207 Mah, Pascal Muam,                                 Iwona Skalna,                                 and John Muzam.        
                2022. ""Natural Language Processing and Artificial Intelligence for Enterprise Management in the Era of Industry 4.0""Applied Sciences12, no. 18: 9207.
        https://doi.org/10.3390/app12189207 Mah, P. M.,                                 Skalna, I.,                                 & Muzam, J.        
        
        (2022). Natural Language Processing and Artificial Intelligence for Enterprise Management in the Era of Industry 4.0.Applied Sciences,12(18), 9207.
        https://doi.org/10.3390/app12189207 Subscribe to receive issue release notifications and newsletters from MDPI journals"
https://www.datacamp.com/blog/what-is-natural-language-processing,Error: 403 Client Error: Forbidden for url: https://www.datacamp.com/blog/what-is-natural-language-processing
https://github.com/nlpfromscratch/nlp-llms-resources,"We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see ourdocumentation. Master list of curated resources on NLP and LLMs  This is the master resource list forNLP from scratch. This is a living document and will continually be updated and so should always be considered a work in progress. If you find any dead links or other issues, feel free tosubmit an issue. This document is quite large, so you may wish to use the Table of Contents automatically generated by Github to find what you are looking for:  Thanks, and enjoy! These are not referral links. Master list of curated resources on NLP and LLMs"
https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP,"Natural language processing (NLP) is the ability of a computer program to understand human language as it's spoken and written -- referred to as natural language. It's a component of artificial intelligence (AI). NLP has existed for more than 50 years and has roots in the field of linguistics. It has a variety of real-world applications in numerous fields, including medical research, search engines and business intelligence. NLP uses eitherrule-based or machine learningapproaches to understand the structure and meaning of text. It plays a role inchatbots, voice assistants, text-based scanning programs, translation applications and enterprise software that aids in business operations, increases productivity and simplifies different processes. NLP uses many different techniques to enable computers to understand natural language as humans do. Whether the language is spoken or written, natural language processing can use AI to take real-world input, process it and make sense of it in a way a computer can understand. Just as humans have different sensors -- such as ears to hear and eyes to see -- computers have programs to read and microphones to collect audio. And just as humans have a brain to process that input, computers have a program to process their respective inputs. At some point in processing, the input is converted to code that the computer can understand.There are two main phases to natural language processing:data preprocessingand algorithm development. This article is part of Data preprocessing involves preparing andcleaningtext data so that machines can analyze it. Preprocessing puts data in a workable form and highlights features in the text that an algorithm can work with. There are several ways this can be done, including the following: Once the data has been preprocessed, an algorithm is developed to process it. There are many different natural language processing algorithms, but the following two main types are commonly used: Businesses use large amounts ofunstructured, text-heavy data and need a way to efficiently process it. Much of the information created online and stored in databases is natural human language, and until recently, businesses couldn't effectively analyze this data. This is where natural language processing is useful. The advantages of natural language processing can be seen when considering the following two statements: ""Cloud computing insurance should be part of every service-level agreement"" and ""A good SLA ensures an easier night's sleep -- even in the cloud."" If a user relies on natural language processing for search, the program will recognize thatcloud computingis an entity, thatcloudis an abbreviated form of cloud computing, and thatSLAis an industry acronym for service-level agreement. These are the types of vague elements that frequently appear in human language and thatmachine learning algorithmshave historically been bad at interpreting. Now, with improvements in deep learning and machine learning methods, algorithms can effectively interpret them. These improvements expand the breadth and depth of data that can be analyzed. Likewise, NLP is useful for the same reasons as when a person interacts with agenerative AIchatbot or AI voice assistant. Instead of needing to use specific predefined language, a user could interact with a voice assistant like Siri on their phone using their regular diction, and their voice assistant will still be able to understand them. Syntax and semantic analysis are two main techniques used in natural language processing. Syntaxis the arrangement of words in a sentence to make grammatical sense.NLP uses syntaxto assess meaning from a language based on grammatical rules. Syntax NLP techniques include the following: This is the grammatical analysis of a sentence. For example, a natural language processing algorithm is fed the sentence, ""The dog barked."" Parsing involves breaking this sentence into parts of speech -- i.e., dog = noun, barked = verb. This is useful for more complex downstream processing tasks. This is the act of taking a string of text and deriving word forms from it. For example, a person scans a handwritten document into a computer. The algorithm can analyze the page and recognize that the words are divided by white spaces. This places sentence boundaries in large texts. For example, a natural language processing algorithm is fed the text, ""The dog barked. I woke up."" The algorithm can use sentence breaking to recognize the period that splits up the sentences. This divides words into smaller parts called morphemes. For example, the worduntestablywould be broken into [[un[[test]able]]ly], where the algorithm recognizes ""un,"" ""test,"" ""able"" and ""ly"" as morphemes. This is especially useful in machine translation and speech recognition. This divides words with inflection in them into root forms. For example, in the sentence, ""The dog barked,"" the algorithm would recognize the root of the word ""barked"" is ""bark."" This is useful if a user is analyzing text for all instances of the word bark, as well as all its conjugations. The algorithm can see that they're essentially the same word even though the letters are different. Semanticsinvolves the use of and meaning behind words. Natural language processing applies algorithms to understand the meaning and structure of sentences. Semantic techniques include the following: This derives the meaning of a word based on context. For example, consider the sentence, ""The pig is in the pen."" The wordpenhas different meanings. An algorithm using this method can understand that the use of the word here refers to a fenced-in area, not a writing instrument. NERdetermines words that can be categorized into groups. For example, an algorithm using this method could analyze a news article and identify all mentions of a certain company or product. Using the semantics of the text, it could differentiate between entities that are visually the same. For instance, in the sentence, ""Daniel McDonald's son went to McDonald's and ordered a Happy Meal,"" the algorithm could recognize the two instances of ""McDonald's"" as two separate entities -- one a restaurant and one a person. NLGuses a database to determine the semantics behind words and generate new text. For example, an algorithm could automatically write a summary of findings from a business intelligence (BI) platform, mapping certain words and phrases to features of the data in the BI platform. Another example would be automatically generating news articles or tweets based on a certain body of text used for training. Current approaches to natural language processing are based on deep learning, a type of AI that examines and uses patterns in data to improve a program's understanding. Deep learning models require massive amounts of labeled data for the natural language processing algorithm to train on and identify relevant correlations, and assembling this kind ofbig dataset is one of the main hurdles to natural language processing. Earlier approaches to natural language processing involved a more rule-based approach, where simpler machine learning algorithms were told what words and phrases to look for in text and given specific responses when those phrases appeared. But deep learning is a more flexible, intuitive approach in which algorithms learn to identify speakers' intent from many examples -- almost like how a child would learn human language. Three open source tools commonly used for natural language processing include Natural Language Toolkit (NLTK), Gensim and NLP Architect by Intel. NLTK is aPythonmodule with data sets and tutorials. Gensim is a Python library for topic modeling and document indexing. NLP Architect by Intel is a Python library for deep learning topologies and techniques. Some of the main functions and NLP tasks that natural language processing algorithms perform include the following: The functions listed above are used in a variety of real-world applications, including the following: The main benefit of NLP is that it improves the way humans and computers communicate with each other. The most direct way to manipulate a computer is through code -- the computer's language. Enabling computers to understand human language makes interacting with computers much more intuitive for humans. Other benefits include the following: There are numerous challenges in natural language processing, and most of them boil down to the fact that natural language is ever-evolving and somewhat ambiguous. They include the following: NLP draws from a variety of disciplines, including computer science and computational linguistics developments dating back to the mid-20th century. Its evolution included the following major milestones: Natural language processing has its roots in this decade, when Alan Turing developed theTuring Testto determine whether or not a computer is truly intelligent. The test involves automated interpretation and the generation of natural language as a criterion of intelligence. NLP was largely rules-based, using handcrafted rules developed by linguists to determine how computers would process language. The Georgetown-IBM experiment in 1954 became a notable demonstration of machine translation, automatically translating more than 60 sentences from Russian to English. The 1980s and 1990s saw the development of rule-based parsing, morphology, semantics and other forms of natural language understanding. The top-down, language-first approach to natural language processing was replaced with a more statistical approach because advancements in computing made this a more efficient way of developing NLP technology. Computers were becoming faster and could be used to develop rules based on linguistic statistics without a linguist creating all the rules. Data-driven natural language processing became mainstream during this decade. Natural language processing shifted from a linguist-based approach to an engineer-based approach, drawing on a wider variety of scientific disciplines instead of delving into linguistics. Natural language processing saw dramatic growth in popularity as a term. NLP processes using unsupervised and semi-supervised machine learning algorithms were also explored. With advances in computing power, natural language processing has also gained numerous real-world applications. NLP also began powering other applications like chatbots and virtual assistants. Today, approaches to NLP involve a combination of classical linguistics and statistical methods. Natural language processing plays a vital part in technology and the way humans interact with it. Though it has its challenges, NLP is expected to become more accurate with more sophisticated models, more accessible and more relevant in numerous industries. NLP will continue to be an important part of both industry and everyday life. As natural language processing is making significant strides in new fields, it's becoming more important for developers to learn how it works. Learn how to develop your skills increating NLP programs. To help you find the right BI software for your analytics needs, here's a look at 20 top tools and the key features that ... Data preparation is a crucial but complex part of analytics applications. Don't let seven common challenges send your data prep ... A year after getting acquired by private equity firms amid declining revenue growth and a slow transition to the cloud, the ... President-elect Donald Trump has been vocal in his criticisms of big tech's content censorship power and President Joe Biden's ... Internal tech procurement can be a risky undertaking. Tech pilot programs can potentially reduce some of that risk, but IT ... Apart from President-elect Donald Trump's promise to take a strong stance on goods imported from China, collaboration might be ... Based on its AI development and expansion within data management, the record $10 billion the vendor raised shows it is viewed ... The data lakehouse pioneer has expanded into AI development and plans to use the funding to fuel further investments in AI, make ... The complementary partnership combines the data integration vendor's discovery and retrieval capabilities with the tech giant's ... The supply chain might be losing its seat at the executive table, but the work must continue to provide more resilience and ... Nothing is certain except death, taxes and supply chain disruptions. Companies should plan accordingly, especially as the ... Essential TMS features are real-time tracking, route optimization, carrier management, load planning, fleet management, automated... All Rights Reserved,Copyright 2018 - 2024, TechTargetPrivacy PolicyCookie PreferencesCookie PreferencesDo Not Sell or Share My Personal Information"
https://www.kellton.com/kellton-tech-blog/natural-language-processing-in-ai,"Reach out, we'd love to hear from you! NLP stands for Natural Language Processing, and this phenomenal innovation is, in many ways, driving the future of AI. To begin with, NLP technology is actively transforming how we interact with machines, automate tasks, and drive innovation. As machines and computers become more comfortable interacting with humans, it will set the stage for smoother human-machine interactions, seamless multilingual communication, and increased automation and innovation. Expect more advanced chatbots, improved healthcare diagnostics and treatments, and an increased thrust on ethical AI. Nearly all industries, from healthcare to insurance and retail to manufacturing, stand to benefit from the evolution of natural language processing in AI. In many ways, NLP and the rest of the AI stack are building a new world - a world where machines are trained to comprehend humans and respond appropriately. Forward-looking organizations, from tech startups to established enterprises, are increasingly investing in NLP-powered apps and systems to streamline operations and drive productivity and business results. In fact,Grand View Researchstates, â€œThe global NLP market size was close to USD 27.73 billion in 2022 and is likely to grow at an impressive CAGR of 40.4% from 2023 to 2030.â€ The rapid growth in the NLP space is a testament to the fact that businesses across the globe are willing to invest in this technology. The growth in NLP will also help push the existing boundaries of Artificial Intelligence and make AI a far more precious asset in the future. Thatâ€™s what weâ€™ll focus on in this blog. Weâ€™ll learn about the fundamentals of NLP. More importantly, weâ€™ll look into ways this technology will helpbuild a new era of AI. Letâ€™s start with what natural language processing (NLP) means. Natural language processing (NLP) is a pivotal innovation in modern AI. The simplest way to understand NLP is to imagine a bridge connecting humans with machines at a far deeper level than ever before. We use NLP in numerous real-life situations. So, when you interact with a chatbox installed on a website, with voice assistants such as Siri and Alexa, or with tools translating languages in real-time, you are using NLP-powered apps and systems. NLP uses an ever-increasing number of techniques to understand, process, and generate human language. The most common natural language processing techniques are tokenization, stemming and lemmatization, and named entity recognition (NER).  Revenues from the Natural Language Processing (NLP) market worldwide from 2017 to 2025 (in million U.S. dollars) The entire ecosystem of Natural language processing (NLP) thrives on a multitude of techniques, such as tokenization and transformer models. These natural language processing techniques supercharge an ever-growing number of use cases, fromhighly interactive chatbots to sentiment analysis. Letâ€™s take a quick look at some of the most common natural language processing techniques: In addition to tokenization, named entity recognition, stemming, and lemmatization, AI apps, and systems development companies use several other NLP techniques, such as text classification, sentiment analysis, and text summarization. Weâ€™ve now familiarized ourselves with some popular natural language processing techniques. Now, letâ€™s explore another key aspect of NLP: how it differs fromcore AI technology. Yes, NLP is a type of AI, but itâ€™s also evolved into a world of its own. NLP and AI are related in more than one way. One strengthens the other. However, it does not mean that NLP and AI are the same. Itâ€™s essential to understand what differentiates them from each other. Let us share a quick comparison table that explores the key differences between NLP and AI.  Whether you know it or not, NLP has entered our lives, and we use it like every day of our lives. Here are some of the examples of NLP in action: We have shared just the tip of the iceberg regarding how NLP is becoming an essential part of our lives. However, you must have a gist of how NLP impacts us all. Now, letâ€™s get down to the value that the proper applications of NLP solutions can generate. Natural language processing, or NLP, has numerous use cases across nearly all industries. However, the most common uses of NLP in the business world include:  Natural language processing is a powerful technology, which is increasingly driving innovation across the AI landscape. Nearly every industry stands to benefit from advancements in natural language processing in AI, which will eventually make machines more humane and beneficial for our world. To harness NLP's full value and drive business forward, you must strategically build, buy, and integrate NLP-powered solutions within your IT infrastructure. Thatâ€™s where an AI-first technology consulting partner, such as Kellton, can help you navigate the complex landscape of NLP with greater clarity and confidence. North America:+1.844.469.8900 Asia:+91.124.469.8900 Europe:+44.203.807.6911 Email:ask@kellton.com Â© 2024Â Kellton"
https://www.frontiersin.org/research-topics/48440/explainable-ai-in-natural-language-processing,"2,801 Total Downloads 16k Total Views and Downloads You will be redirected to our submission process. Submission closed Traditional Natural Language Processing (NLP) models (e.g., decision trees, Markov models, etc.) have primarily been based on techniques that are inherently interpretable models, referred to as white-box techniques. However, in recent years, NLP models have employed advanced neural approaches along with language embedding features. Using these advanced approaches, mostly referred to as black-box techniques, the NLP models have yielded state-of-art performance. Nonetheless, the level of interpretability (e.g., how the model arrives at its results) has reduced significantly. This obfuscated interpretability not only lowers the end usersâ€™ trust in the NLP models but also makes it challenging for the developers to debug or improve by analyzing the models for further improvement. Therefore, nowadays, researchers in the NLP community are giving significant attention to the emerging field called Explainable AI (XAI) to tackle the obfuscated complexity of AI systems for trust and improvement. Apart from academia, organizations and companies also have launched high-funding projects such as  DARPA XAI, People +AI Research (PAIR), etc.As XAI is still a growing field, there is plenty of room for innovation to improve the explainability of NLP systems. In recent works, explainable NLP models have captured linguistic knowledge of neural networks, explain predictions, stress-test models via challenge sets or adversarial examples, and interpret language embeddings.The goal of this Research Topic is to better understand the present status of the XAI in NLP by identifying: new dimensions for a better explanation, evaluation techniques used to measure the quality of explanations, approaches or developments of new software toolkits to explain XAI in NLP, and transparent deep learning models for different NLP task.The scope of this Research Topic covers (but is not restricted to) the following topics:â€¢ Survey of XAI in NLP in general or any particular NLP task such as NER, QA, Sentiment analysis, social media (SocialNLP), etc.â€¢ Explainable Neural models in Machine Translationâ€¢ Explainable Neural models in Named Entity Recognitionâ€¢ Explainable Neural models in Question Answeringâ€¢ Explainable Neural models in Sentiment Analysisâ€¢ Explainable Neural models in Opinion Miningâ€¢ Explainable Neural models in SocialNLPâ€¢ Evaluation techniques used to measure the quality of explanationsâ€¢ Tools for explaining explainabilityâ€¢ Resources related to XAI in the context of NLPThe Research Topic welcomes contributions toward interpretable models for efficient solutions to NLP research problems that explain the explainability of the proposed model using suitable explainability technique(s) (e.g., example-driven, provenance, feature importance, induction, surrogate models, etc.), visualization technique(s) (e.g., raw examples, saliency, raw declarative, etc.), and other aspects. Software toolkits or approaches that can help users express explainability to their models and ML pipelines are also welcome. Keywords:NLP, Explainable AI, Explainability, Interpretability, Deep Learning Important note:All contributions to this Research Topic must be within the scope of the section and journal to which they are submitted, as defined in their mission statements. Frontiers reserves the right to guide an out-of-scope manuscript to a more suitable section or journal at any stage of peer review. University of Tartu Tartu,Estonia University of Alicante Alicante,Spain Frontiers' Research Topics are collaborative hubs built around an emerging theme.Defined, managed, and led by renowned researchers, they bring communities together around a shared area of interest to stimulate collaboration and innovation. Unlike section journals, which serve established specialty communities, Research Topics are pioneer hubs, responding to the evolving scientific landscape and catering to new communities. The goal of Frontiers' publishing program is to empower research communities to actively steer the course of scientific publishing. Our program was implemented as a three-part unit with fixed field journals, flexible specialty sections, and dynamically emerging Research Topics, connecting communities of different sizes and maturity. Research Topics originate from the scientific community. Many of our Research Topics are suggested by existing editorial board members who have identified critical challenges or areas of interest in their field. As an editor, Research Topics will help you build your journal, as well as your community, around emerging, cutting-edge research. As research trailblazers, Research Topics attract high-quality submissions from leading experts all over the world. A thriving Research Topic can potentially evolve into a new specialty section if there is sustained interest and a growing community around it. Each Research Topic must be approved by the specialty chief editor, and they fall under the editorial oversight of our editorial boards, supported by our in-house research integrity team. The same standards and rigorous peer review processes apply to articles published as part of a Research Topic as for any other article we publish. In 2023, 80% of the Research Topics we published were edited or co-edited by our editorial board members, who are already familiar with their journal's scope, ethos, and publishing model. All other topics are guest edited by leaders in their field, each vetted and formally approved by the specialty chief editor. Publishing your article within a Research Topic with other related articles increases its discoverability and visibility, which can lead to more views, downloads, and citations. Research Topics grow dynamically as more published articles are added, causing frequent revisiting, and further visibility. As Research Topics are multidisciplinary, they are cross-listed in several fields and section journals â€“ increasing your reach even more and giving you the chance to expand your network and collaborate with researchers in different fields, all focusing on expanding knowledge around the same important topic. Our larger Research Topics are also converted into ebooks and receive social media promotion from our digital marketing team. Frontiers offers multiple article types, but it will depend on the field and section journals in which the Research Topic will be featured. The available article types for a Research Topic will appear in the drop-down menu during the submission process. Check available article types here Yes, we would love to hear your ideas for a topic. Most of our Research Topics are community-led and suggested by researchers in the field. Our in-house editorial team will contact you to talk about your idea and whether youâ€™d like to edit the topic. If youâ€™re an early-stage researcher, we will offer you the opportunity to coordinate your topic, with the support of a senior researcher as the topic editor. Suggest your topic here A team of guest editors (called topic editors) lead their Research Topic. This editorial team oversees the entire process, from the initial topic proposal to calls for participation, the peer review, and final publications. The team may also include topic coordinators, who help the topic editors send calls for participation, liaise with topic editors on abstracts, and support contributing authors. In some cases, they can also be assigned as reviewers. As a topic editor (TE), you will take the lead on all editorial decisions for the Research Topic, starting with defining its scope. This allows you to curate research around a topic that interests you, bring together different perspectives from leading researchers across different fields and shape the future of your field. You will choose your team of co-editors, curate a list of potential authors, send calls for participation and oversee the peer review process, accepting or recommending rejection for each manuscript submitted. As a topic editor, you're supported at every stage by our in-house team. You will be assigned a single point of contact to help you on both editorial and technical matters. Your topic is managed through our user-friendly online platform, and the peer review process is supported by our industry-first AI review assistant (AIRA). If youâ€™re an early-stage researcher, we will offer you the opportunity to coordinate your topic, with the support of a senior researcher as the topic editor. This provides you with valuable editorial experience, improving your ability to critically evaluate research articles and enhancing your understanding of the quality standards and requirements for scientific publishing, as well as the opportunity to discover new research in your field, and expand your professional network. Yes, certificates can be issued on request. We are happy to provide a certificate for your contribution to editing a successful Research Topic. Research Topics thrive on collaboration and their multi-disciplinary approach around emerging, cutting-edge themes, attract leading researchers from all over the world. As a topic editor, you can set the timeline for your Research Topic, and we will work with you at your pace. Typically, Research Topics are online and open for submissions within a few weeks and remain open for participation for 6 â€“ 12 months. Individual articles within a Research Topic are published as soon as they are ready. Find out moreabout our Research Topics Our fee support program ensures that all articles that pass peer review, including those published in Research Topics, can benefit from open access â€“ regardless of the author's field or funding situation. Authors and institutions with insufficient funding can apply for a discount on their publishing fees. Afee support application formis available on our website. In line with our mission to promote healthy lives on a healthy planet, we do not provide printed materials. All our articles and ebooks are available under a CC-BY license, so you can share and print copies."
https://www.geeksforgeeks.org/natural-language-processing-nlp-101-from-beginner-to-expert/,"Natural Language Processing (NLP)is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. The primary objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful. This involves a combination of linguistics, computer science, and machine learning. NLP allows machines to perform a variety of tasks such as language translation, sentiment analysis, speech recognition, and text generation. In this article, we will explore aboutNLP 101 , Basics to Advance of NLP, its Applications, Core Concepts and Techniques in NLP, Techniques, and Challenges. Table of Content Natural Language Processing (NLP)is a field of artificial intelligence (AI) focused on the interaction between computers and humans through natural language. It involves the development of algorithms and models that allow computers to understand, interpret, generate, and respond to human language in a way that is both meaningful and useful. NLP has evolved significantly since its inception in the 1950s. Early efforts in NLP were rule-based systems that required extensive hand-coding. Over the decades, advances in machine learning, especially deep learning, have revolutionized NLP, leading to the development of more sophisticated models that can handle complex language tasks with higher accuracy. To get started with NLP, you need to set up a programming environment with the necessary tools and libraries.Pythonis a popular language for NLP, and libraries like NLTK and SpaCy are widely used. Begin with basicNLPtasks such as tokenization, POS tagging, and text classification. Use code examples to understand how these tasks are implemented and how to apply them to real-world problems. There are many resources available for learning NLP, including online courses, books, and communities. Some recommended resources are: Example: Example: Example: Example: Example: Rule-Based Systemsrely on predefined linguistic rules and patterns to process and analyze text. These rules are often handcrafted by experts and can include grammatical rules, keyword searches, or regular expressions. Statistical Modelsuse mathematical techniques to analyze and predict language patterns based on probabilities derived from large corpora of text. They rely on statistical properties of language data rather than explicit rules. Machine Learning Approachesinvolve training algorithms on labeled data to learn patterns and make predictions or decisions based on new, unseen data. These methods can handle a variety of NLP tasks, such as text classification and sentiment analysis. Deep Learning in NLPleverages neural networks with multiple layers (deep neural networks) to model complex patterns in language data. These models learn hierarchical representations of text data through training on large datasets. Transformersare a type of neural network architecture that has revolutionized NLP by enabling more efficient and effective learning of language patterns. They use attention mechanisms to focus on different parts of the input text dynamically. Natural Language Processing is a rapidly evolving field with vast potential to transform how we interact with technology. From enhancing search engines to enabling seamless communication between languages, NLP applications are becoming increasingly integral to our daily lives. Despite the challenges, ongoing advancements in NLP promise to further bridge the gap between human language and machine understanding, paving the way for more intuitive and intelligent systems. N "
https://www.geeksforgeeks.org/how-to-become-an-nlp-engineer/,"In Natural Language Processing(NLP), two trends are gaining momentum, AI ethics in technology and advancements in multilingual NLP systems. As AI is integrated deeply into our daily lives, the use of NLP technologies is becoming a paramount concern. For aspiring NLP engineers learning these ethical considerations is important to master the technical aspects. In this article, We will explorethe journey of Becoming an NLP Engineer, Focusing on the essential skills, knowledge, and practices necessary for a career in the dynamic field of AI and Language processing. Table of Content Natural Language Processing (NLP)is a subfield of artificial intelligence that teaches machines to understand, interpret, and generate human language. It involves creating models and algorithms that enable computers to communicate with and handle data in natural language. Consider a chatbot that understands and responds to user queries using NLP. The chatbot can analyze input text, extract meaning, and generate contextually relevant responses, allowing it to have human-like conversations. You can also refer to our existing article -Natural Language Processing (NLP) Tutorial ANatural Language Processing (NLP)engineer is a professional who specializes in the development and implementation of technologies that allow computers to understand, interpret, and generate human language. These engineers work at the intersection of computer science, artificial intelligence, and linguistics to develop algorithms and models that help machines and humans communicate. They design NLP systems and work with speech recognition and patterns in AI. NLP engineers work in industries such as technology, healthcare, finance, and e-commerce, where language plays an important role in data analysis and user interaction. They frequently work in tandem with software engineers, data scientists, and subject matter experts to create workable solutions that take advantage of natural language generation and understanding capabilities. Some of the Key Technical Skills that are required for NLP jobs that are as follows: You can also refer to our existing article -Python Tutorial | Learn Python Programming Some of the common Softskills that are required for NLP jobs are as follows: These skills collectively empower NLP engineers to not only proficiently navigate the complexities of language but also maintain ethical integrity and contribute effectively to the ever-advancing field of natural language processing. A job interview for an Natural Language Processing engineer requires a combination of technical knowledge, problem-solving abilities, and effective communication. Here are some pointers to help you prepare for and succeed in an NLP engineer job interview: An engineer specializing in Natural Language Processing (NLP) may earn a different salary depending on their industry, location, company size, and level of experience. NLP engineers, in general, earn competitive salaries due to the specialized nature of their skills. In the United States, annual salaries for NLP engineers can range from $80,000 to well over $150,000, depending on the factors mentioned previously. Senior NLP engineers and those with extensive experience may earn even more.  In addition to base salaries, other components such as bonuses, stock options, and benefits can contribute to the overall compensation package. Tech hubs such as Silicon Valley typically pay higher salaries to tech professionals, but the cost of living in those areas is also higher. According to industry insights and compensation surveys, the average salary for NLP engineers in India ranges between INR 8,00,000 and INR 10,00,000 per year. This salary bracket, reflective of the specialized skill set and expertise demanded by the field, positions NLP engineers among the well-compensated professionals in the Indian tech landscape. As an NLP engineer, you will be in charge of creating software that can comprehend and react to natural language. To create software that can efficiently parse and process text, you will use modeling, data structures, semantic extraction algorithms, and text representation techniques. In order to create models that work well enough to be used in production, you need to be well-versed in machine learning frameworks and statistical techniques. You'll also need to know how to program in languages like Python, Java, or R. Writing testable and maintainable code is a requirement for NLP engineers. Additionally, they ought to be knowledgeable about big data frameworks like Hadoop and Spark. Writing code in multiple languages will be beneficial when working with large datasets. Yes, programming languages like Python and Java is Important. Python is particularly for NLP as it contains extensive python libraries such as NLTK, SpaCy and Tensorflow. Machine Learning is a central to NLP. IT helps to understand various machine learning models or deep learning techniques that are important for developing effective NLP systems. Yes, Internship provides valuable and practical experience that helps to understand real-world NLP applications. THey can also help as a stepping stone for full-time roles. N "
